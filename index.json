[{"content":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$\nPolicy Function $\\pi : S \\mapsto A$ maps states to actions\nExecute a policy by following the prescribed action\nValue Function The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\\pi$ $$ V^\\pi(s) = \\mathbb{E} \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots | s_0 = s,\\pi \\right] $$\nBellman Equation Given a fixed policy $\\pi$, it\u0026rsquo;s value function $V^\\pi$ satisfies the Bellman equations $$ V^\\pi(s) = R(s) + \\gamma \\sum_{s\u0026rsquo; \\in S} P_{s\\pi(s)}(s\u0026rsquo;) V^\\pi(s\u0026rsquo;) $$ Which consists of two terms:\n Immediate reward $R(s)$ Get for starting in $s$ Expected sum of future discounted rewards Can we rewritten as $E_{s\u0026rsquo;\\sim P_{s\\pi(s)}}[V^\\pi(s\u0026rsquo;)]$ The expected sum of starting in $s\u0026rsquo;$ where $s\u0026rsquo;$ is distributed according to to $P_{s\\pi(s)}$  Can be used to efficiently solve for the value function $V^\\pi$\n Write down one equation for every state Gives a set of $|S|$ linear equations in $|S|$ variables  Optimal Value Function Best possible expected sum of discounted rewards which can be attained using any policy\n$$ V^*(s) = \\max_\\pi V^\\pi (s) $$\nWhich has it\u0026rsquo;s own version of the Bellman Equation\n$$ V^{\\ast}(s) = R(s) + \\max_{a\\in A} \\gamma \\sum_{s\u0026rsquo;\\in S}{P_{sa} (s\u0026rsquo;) V^{\\ast}(s\u0026rsquo;)} $$\nThe second term is a $\\max$ over all possible actions because that is the optimal reward.\nOptimal Policy $$ \\pi^{\\ast}(s) = \\arg \\max_{a\\in A} \\sum_{s\u0026rsquo;\\in S} P_{sa}(s\u0026rsquo;) V^*(s\u0026rsquo;) $$\nWhich gives the action that attains the $\\max$ in the optimal value function\nFor every state $s$ and policy $\\pi$ $$ V^\\ast(s) = V^{\\pi*}(s) \\geq V^\\pi(s) $$ Which says that\n The value function for the optimal policy is equal to the optimal value function for every state $s$\n  The value function for the optimal policy is greater than or equal to every other policy\n  $\\pi^*$ is the optimal policy for all states  Value Iteration and Policy Iteration Two efficient algorithms for solving finite-state MDPs with known transition probabilities\nValue Iteration  For each state s, initialize $V(s) := 0$ for until convergence do For every state, update $$ V(s) := R(s) + \\max_{a\\in A} \\gamma \\sum_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) $$\n  Repeatedly update estimated value function using the Bellman equation  Can be:\n Synchronous  Compute new values for all states and update at once Can be viewed as implementing a Bellman Backup Operator, which maps the current estimate to a new estimate   Asynchronous  Loop over states and update one at a time    Both algorithms will cause $V$ to converge to $V^*$\nPolicy Iteration  Initialise $\\pi$ randomly\nfor until convergence do\n​\tLet $V := V^\\pi$\n​\tFor each state $s$ $$ \\pi(s) := \\arg \\max_{a \\in A} \\sum_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) $$\n  Repeatedly computes value function for the current policy Update policy using current value function  This is the policy that is greedy w.r.t. $\\bold V$   The value function can be updated using Bellman equations After a finite number of iterations, $V$ will converge to $V^\\ast$ and $\\pi$ to $\\pi^\\ast$  Value vs Policy Iteration  No universal agreement which is better Small MDPs : Policy iteration tends to be fast Larger MDPs : Value iteration might be faster Policy iteration can reach the exact $V^*$ Value iteration will have small non-zero error  Learning a Model for an MDP   In many realistic problems, we do not know the state transition probabilities and rewards\n  Must be estimated from data\n  Given enough trials, can derive maximum likelihood estimates for the state transition probabilities $$ P_{sa}(s\u0026rsquo;) = \\frac{num. \\text{ times } a \\text{ led to } s\u0026rsquo; } {num. \\text{ times we took } a} $$ In the case of $\\frac{0}{0}$, can set $P$ to $\\frac{1}{|S|}$ (estimate as uniform)\nEfficient Update Keep count of numerator \u0026amp; denominator\nSolving Learned MDP  Can use policy/value iteration  Continuous State MDPs  Infinite number of states Consider settings where state space is $S = \\mathbb{R}^d$  Discretization  Simplest way Discretize state space Use value/policy iteration  Downsides\n Fairly naive representation for $V^*$ Assumes the value function is constant over each discretization interval Piecewise constant representation isn\u0026rsquo;t good for many smooth functions  Little smoothing over inputs No generalization over different grid cells   Suffers from curse of dimensionality  For $S = \\mathbb{R}^d$, we will have $k^d$ states    Rule of Thumb\n Works well for 1D/2D cases Can work for 3D/4D cases Rarely works above 6D  Value Function Approximation  Approximate $V^*$ directly  Using Model/Simulator  Assume we have a model/simulator for the MDP Any black-box with  Input : State $s_t$, Action $a_t$ Output : Next-state $s_{t+1}$ sampled    Getting the Model Using physics simulation  Can use off-the-shelf physics simulation  Data collected in the MDP  Can be done with:  Random actions Executing some policy Other way of choosing actions [?] Control law?   Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$  Example : Learn linear model\nusing linear regression $$ \\arg \\min_{A,B} \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\left\\Vert s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)}) \\right\\Vert^2_2 $$ Can build either a deterministic $$ s_{t+1} = As_t + Ba_t $$ or stochastic model $$ s_{t+1} = As_t + Ba_t + \\epsilon_t $$ with $\\epsilon_t$ being a noise term usually modelled as $\\epsilon_t \\sim \\mathcal{N}(0,\\Sigma)$\nor non-linear function $$ s_{t+1} = A\\phi_s(s_t) + B\\phi_a(a_t) $$ Eg: Locally weighted linear regression\nFitted Value Iteration  Algorithm for approximating the value function of a continuous state MDP Assume state space $S = \\mathcal{R}^d$ But action space $A$ is small and discrete  Perform value iteration update $$ \\begin{aligned} V(s) :\u0026amp;= R(s) + \\gamma \\max_a \\int_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) ds\u0026rsquo; \\cr \u0026amp;= R(s) + \\gamma \\max_a E_{s\u0026rsquo; \\sim P_{sa}} [V(s\u0026rsquo;)] \\end{aligned} $$\n Main difference is the $\\int$ instead of $\\sum$ Approximately update over a finite sample of states $s^{(1)}, \\dots, s^{(n)}$ Use supervised learning algorithm, linear regression to approximate value function as linear/nonlinear function of the states  $$ V(s) = \\theta^T \\phi(s) $$\nAlgorithm\n Compute $y^{(i)}$, the approximation to $R(s) + \\gamma \\max_a E_{s\u0026rsquo; \\sim P_{sa}} [V(s\u0026rsquo;)]$ (RHS)\nUse supervised learning to get $V(s) = y^{(i)}$\nRepeat\n Convergence  Cannot be proved to converge But in practice it often does, and works well for many problems  Deterministic Simplification  Can set $k=1$ Because expectation of a deterministic distribution requires 1 sample  Policy  Outputs $V$, an approximation of $V^*$ Implicitly defines policy  When in state $s$, choose $$ \\arg \\max_a E_{s\u0026rsquo;\\sim P_{sa}} [V(s\u0026rsquo;)] $$\n Process for computing/approximating this is similar to the fitted value iteration algorithm  ","permalink":"https://andrelimzs.github.io/posts/machine-learning/cs239-reinforcement-learning/","summary":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$","title":"CS229 Reinforcement Learning"},{"content":"Deep Learning Supervised with Nonlinear Models Supervised learning is\n Predict $y$ from input $x$ Suppose model/hypothesis is $h_\\theta(x)$  Previous methods have considered\n Linear regression : $h_\\theta(x) = \\theta^Tx$ Kernel method : $h_\\theta(x) = \\theta^T \\phi(x)$  Both are linear in $\\theta$\nNow consider models that are nonlinear in both\n Parameters : $\\theta$ Inputs : $x$  The most common of which is the neural network\nCost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\\theta) = \\frac{1}{2} ( h_\\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} J^{(i)}(\\theta) $$\nOptimisers SGD or it\u0026rsquo;s variants is commonly used $$ \\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta) $$ with $\\alpha \u0026gt; 0$\nHardware parallelisation means that it\u0026rsquo;s often faster to compute the gradient of $B$ examples simultaneously\n$\\Rightarrow$ Mini-batch SGD is most commonly used\nNeural Networks  Refers to any type of nonlinear model/parameterisation $h_\\theta(x)$ that involves matrix multiplications and other entrywise nonlinear operations\n Activation Function A (1D) nonlinear function that maps $\\mathbb{R}$ to $\\mathbb{R}$\nCommon ones include\n ReLU : Rectified Linear Unit  Single Neuron $$ h_\\theta(x) = ReLU(w^Tx + b) $$\n Input : $x \\in \\mathbb{R}^d$ Weights : $w \\in \\mathbb{R}^d$ Bias : $b \\in \\mathbb{R}$  Stacking Neurons Enabels more complex networks\n[Example] Two-layer Fully-connected Network\n For a hidden layer with three neurons $$ a_1 = ReLU(w_1^T x + b_1) \\\\ a_2 = ReLU(w_2^T x + b_2) \\\\ a_3 = ReLU(w_3^T x + b_3) $$ Or equivalently $$ \\begin{aligned} \u0026amp;z_i = (w_i^{[1]})^T x + b_i^{[1]} \\quad \\forall i \\in [1,\\dots,m] \\cr \u0026amp; a_i = ReLU(z_i) \\cr \u0026amp; a = [ a_1,\\ \\dots,\\ a_m ]^T \\cr \u0026amp; h_\\theta(x) = (w^{[2]})^T a + b^{[2]} \\end{aligned} $$\nVectorisation  Simplify expression using more matrix/vector notation Important in speeding up neural networks  Takes advantage of optimised linear algebra packages such as LAPACK/BLAS Leverages GPU parallelism    Concatentae weights/inputs/outputs as $$ W^{[i]} = \\begin{bmatrix} -\\ w^{[i]\\top}_1 \\ - \\cr \\vdots \\cr -\\ w^{[i]\\top}_m \\ - \\end{bmatrix} \\in \\mathbb{R}^{m\\times d} $$ To get $$ \\underbrace{ \\begin{bmatrix} z^{[i]}_1 \\cr \\vdots \\cr z^{[i]}_m \\end{bmatrix} }_{z \\in \\mathbb{R}^{m\\times 1}} = \\underbrace{ \\begin{bmatrix} -\\ w^{[i]\\top}_1 \\ - \\cr \\vdots \\cr -\\ w^{[i]\\top}_m \\ - \\end{bmatrix} }_{W^{[1]} \\in \\mathbb{R}^{m\\times d}} \\underbrace{ \\begin{bmatrix} x_1 \\cr \\vdots \\cr x_d \\end{bmatrix} }_{x \\in \\mathbb{R}^{d\\times 1}} + \\underbrace{ \\begin{bmatrix} b^{[1]}_1 \\cr \\vdots \\cr b^{[1]}_m \\end{bmatrix} }_{b^{[1]} \\in \\mathbb{R}^{m\\times 1}} $$ Or equivalently $$ \\begin{aligned} \u0026amp;z = W^{[1]}x + b^{[1]} \\cr \u0026amp;a = ReLU(W^{[1]} + b^{[1]}) \\cr \u0026amp;h_\\theta(x) = W^{[2]} a + b^{[2]} \\end{aligned} $$\n  $\\theta$ consists of weights $W^{[i]}$ and biases $b^{[i]}$\n  Each layer consists of the pair $(W^{[i]}, b^{[i]})$\n  Multi-Layer Network  Can stack more layers to get a deeper fully-connected neural network If you let $a^{[0]} = x$ and $a^{[r]} = h_\\theta(x)$  $$ a^{[k]} = \\text{ReLU}\\left( W^{[k]} a^{[k-1]} + b^{[k]} \\right), \\quad \\forall k=1,\\dots,r-1 $$\nConnection to Kernel Method Deep learning is a method to automatically learn the best feature map (or representation) $$ h_\\theta(x) = W^{[r]} \\phi_\\beta(x) + b^{[r]} $$ With $\\beta: a^{[r-1]} = \\phi_\\beta(x)$ as the feature map\nThe neural network is therefore a linear regression over the learnt (nonlinear) feature map $\\phi_\\beta$\nBackpropagation Aka auto-differentiation\nCompute the gradient of the loss $\\nabla J^{(j)} (\\theta)$ efficiently\nTheorem (Informally)\n Suppose a differentiable circuit of size $N$ computes a real-valued function $f:\\mathbb R^l \\rightarrow \\mathbb R$. The gradient $\\nabla f$ can be computed in $O(N)$ time by a circuit of size $O(N)$\n Chain Rule For variables $\\theta_1, \\dots, \\theta_p$ and intermediate variables $g_1, \\dots, g_k$ $$ \\begin{aligned} g_j = g_j(\\theta_1, \\dots, \\theta_p) \\cr J = J(g_1, \\dots, g_k) \\end{aligned} $$ The chain rule states $$ \\frac{\\partial J}{\\partial \\theta_i} = \\sum^{k}_{j=1} \\frac{\\partial J}{\\partial g_j} \\frac{\\partial g_j}{\\partial \\theta_i} $$\n","permalink":"https://andrelimzs.github.io/posts/machine-learning/cs239-deep-learning/","summary":"Deep Learning Supervised with Nonlinear Models Supervised learning is\n Predict $y$ from input $x$ Suppose model/hypothesis is $h_\\theta(x)$  Previous methods have considered\n Linear regression : $h_\\theta(x) = \\theta^Tx$ Kernel method : $h_\\theta(x) = \\theta^T \\phi(x)$  Both are linear in $\\theta$\nNow consider models that are nonlinear in both\n Parameters : $\\theta$ Inputs : $x$  The most common of which is the neural network\nCost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\\theta) = \\frac{1}{2} ( h_\\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} J^{(i)}(\\theta) $$","title":"CS229 Deep Learning"},{"content":"Generative Learning Different approach to learning\nTry to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly\n $p(x|y)$ : Distribution of the target\u0026rsquo;s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \\frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn\u0026rsquo;t matter $$ \\arg \\max_y P(y|x) = \\arg \\max_y p(x|y) p(y) $$\nMultivariate Normal Distribution  In $d$-dimensions  Parameterised by:\n Mean Vector : $\\mu \\in \\mathbb{R}^d$ Covariance Matrix : $\\Sigma \\in \\mathbb{R}^{d\\times d}$  ​\t$\\Sigma$ symmetric and PSD $$ p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right) $$ Where $|\\Sigma|$ is the determinant of $\\Sigma$\nMean Is given by $\\mu$ $$ \\mathbb{E}[X] = \\int_x{ x p(x; \\mu,\\Sigma) dx } = \\mu $$\nCovariance Is defined as $$ Cov(X) = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^T] $$\n$$ Cov(X) = \\mathbb{E}[XX^T] - (\\mathbb{E}[X])(\\mathbb{E}[X])^T $$\nAs cov increases, the distribution becomes more spread out\nOff-diagonal terms skew the distribution\nStandard Normal Distribution Gaussian with zero mean and identity covariance\nGaussian Discriminant Analysis Assume that $p(x|y)$ is distributed according to a multivariate normal distribution $$ \\begin{aligned} y \u0026amp;\\sim \\text{Bernoulli}(\\phi) \\cr x|y=0 \u0026amp;\\sim \\mathcal{N}(\\mu_0, \\Sigma) \\cr x|y=1 \u0026amp;\\sim \\mathcal{N}(\\mu_1, \\Sigma) \\end{aligned} $$ Which expands to $$ \\begin{aligned} p(y) \u0026amp;\\sim \\phi^y (1-\\phi)^{1-y} \\cr p(x|y=0) \u0026amp;\\sim \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0) \\right) \\cr p(x|y=1) \u0026amp;\\sim \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1) \\right) \\cr \\end{aligned} $$ This model usually combines $\\Sigma_0$/$\\Sigma_1$ into $\\Sigma$\nThe log-likelihood is $$ \\begin{aligned} l(\\phi, \\mu_0, \\mu_1, \\Sigma) \u0026amp;= log \\prod_{i=1}^{n} p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma) \\cr \u0026amp;= \\log \\prod_{i=1}^{n} p(x|y; \\mu, \\mu, \\Sigma)\\ p(y;\\phi) \\end{aligned} $$ Find the maximum likelihood estimates of the parameters by maximising $l$ w.r.t the parameters $$ \\phi = \\frac{1}{n} \\sum_{i=1}^{n} 1{ y^{(i)}=1 } $$\n$$ \\mu_0 = \\frac{\\sum_{i=1}^n 1{ y^{(i)}=0 } x^{(i)}} {\\sum_{i=1}^n 1{ y^{(i)}=0 }} $$\n$$ \\mu_1 = \\frac{\\sum_{i=1}^n 1{ y^{(i)}=1 } x^{(i)}} {\\sum_{i=1}^n 1{ y^{(i)}=1 }} $$\n$$ \\Sigma = \\frac{1}{n} \\sum_{i=1}^{n} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T $$\nGraphically, the algorithm is learning the two gaussian distributions\n The decision boundary is separating hyperplane between the two distributions\nGDA and Logistic Regression Expressing $y$ as a function of $x$ $$ p(y=1|x; \\phi, \\Sigma, \\mu_0, \\mu_1) = \\frac{1}{1 + \\exp(-\\theta^Tx)} $$ Has the exact same form as logistic regression\nBut GDA and logistic regression will generally give different decision boundaries\nThis is because\n Multivariate Gaussian $\\implies$ Logistic function Logistic function $\\centernot\\implies$ Multivariate Gaussian  GDA makes stronger modelling assumptions than logistic regression\n If assumptions are correct, GDA will find better fits If $p(x|y)$ is gaussian with shared $\\Sigma$, GDA is asymptotically efficient  As $n \\rightarrow \\infty$, no algorithm is strictly better than GDA    Comparison    GDA Logistic     Stronger modelling assumptions Less sensitive to modelling errors   More data efficient More robust    [*] In practise logistic regression is used more often than GDA\nNaive Bayes Similar to GDA but with discrete-valued $x$\n  Encode set of features into a vocabulary. Dimension of $x$ equals size of vocabulary\n  But the number of parameters grows with\n  $$ \\large 2^\\text{size of vocabulary} $$\n and quickly becomes too large for an explicit representation  Naive Bayes Assumption  Assume that the $x$\u0026rsquo;s are conditially independent given $y$\n [Example]\nIf an email is spam ($y=1$) and word $x_1$ is apple and word $x_2$ is car\n Knowing that $y=1$ and $x_1 = apple$ Gives no information about $x_2 = car$  $$ p(x_2 | y) = p(x2 | y, x_1) $$\n[*] This does not say that $x_1$ and $x_2$ are independent $$ \\begin{aligned} p(\u0026amp;x_1, \\dots, x_{5000} | y) \\cr \u0026amp;= p(x_1|y) p(x1|y,x_1) \\dots p(x_5000|y,x_1\\dots x_{4999}) \\cr \\end{aligned} $$ Using the naive bayes assumption $$ \\begin{aligned} \u0026amp;= p(x_1|y) p(x1|y) \\dots p(x_5000|y) \\cr \u0026amp;= \\prod_{j=1}^d p(x_j | y) \\end{aligned} $$ The NB assumption is extremely strong\nBut the algorithm works well on many problems\nParameterisation The joint likelihood is $$ \\mathcal{L}(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^{n} p(x^{(i)}, y^{(i)}) $$ Maximise w.r.t $\\phi_y$, $\\phi_{j|y=0}$ and $\\phi_{j|y=1}$ $$ \\begin{aligned} \\phi_{j|y=1} \u0026amp;= \\frac{\\sum_{i=1}^n 1{ x_k^{(i)}=1 \\wedge y^{(i)}=1 }} {\\sum_{i=1}^n 1{ y^{(i)}=1 }} \\cr \\phi_{j|y=0} \u0026amp;= \\frac{\\sum_{i=1}^n 1{ x_k^{(i)}=1 \\wedge y^{(i)}=0 }} {\\sum_{i=1}^n 1{ y^{(i)}=0 }} \\cr \\phi_{y} \u0026amp;= \\frac{\\sum_{i=1}^n 1{ y^{(i)}=1 }}{n} \\end{aligned} $$ To make a prediction, find the highest possiblity from $$ p(y=1|x) = \\frac{p(x|y=1) p(y=1)}{p(x)} $$\nLaplace Smoothing  The first time the Naive Bayes algorithm encounters a new feature, it cannot estimate the probability  $$ \\begin{aligned} p(y=1|x) \u0026amp;= \\frac { \\prod_{j=1}^{d} p(x_j|y=1) p(y=1) } { \\prod_{j=1}^{d} p(x_j|y=1) p(y=1) + \\prod_{j=1}^{d} p(x_j|y=0) p(y=0) } \\cr p(y=1|x) \u0026amp;= \\frac{0}{0} \\end{aligned} $$\n Because $\\prod p(x_j|y)$ includes $p(x_{new}|y)$ which is $0$ and therefore it always obtains $0/0$. It is a bad idea to estimate a previously unseen event to zero.  Solution\n Introduce Laplace smoothing  $$ \\phi_j = \\frac{1 + \\sum_{i=1}^{n} 1 { z^{(i)}=j }} {k+n} $$\n Add $1$ to the numerator Add $k$ to the denominator $\\sum_{j=1}^{k} \\phi_j=1$ still holds $\\phi_j \\neq 0$ for all $j$  Under certain (strong) conditions\n Laplace smoothing gives the optimal estimator  ","permalink":"https://andrelimzs.github.io/posts/machine-learning/cs239-generative-learning/","summary":"Generative Learning Different approach to learning\nTry to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly\n $p(x|y)$ : Distribution of the target\u0026rsquo;s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \\frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn\u0026rsquo;t matter $$ \\arg \\max_y P(y|x) = \\arg \\max_y p(x|y) p(y) $$\nMultivariate Normal Distribution  In $d$-dimensions  Parameterised by:","title":"CS229 Generative Learning"},{"content":"Supervised Learning Terminology  Input (Features) : $x^{(i)}$ Output (Target) : $y^{(i)}$ Training example : $(x^{(i)}, y^{(i)})$ Training set : Set of training examples Hypothesis : $h(x)$  Types  Regression : Continuous values Classification : Discrete values  (Linear) Regression First decide the hypothesis function\nWhere the convention is to let $x_0 = 1$ $$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 $$\n$$ h(x) = \\sum_{i=0}^d \\theta_i x_i = \\theta^T x $$\nObjective Learn the parameters $\\theta$ to best predict $y$\nCost Function Define $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} ( h_\\theta (x^{(i)} - y^{(i)}))^2 $$ This is the least-squares cost function that gives ordinary least squares regression\nLMS Algorithm Least mean squares, or Widrow-Hoff learning rule\n Start with an initial guess Repeatedly perturb $\\theta$ Hope we converge  Gradient Descent $$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n Learning Rate : $\\alpha$  Single training example case $(x,y)$\nReduces to $$ \\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta (x^{(i)})) x_j^{(i)} $$ The update is proportional to the error $$ \\propto (y - h_\\theta(x)) $$\nBatch Gradient Descent Group updates into vector $x$ $$ \\theta := \\theta + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)}- h_\\theta(x^{(i)}) \\right) x^{(i)} $$ Which is equivalent to gradient descent on the original cost function $J$\nStochastic Gradient Descent $$ \\theta := \\theta + \\alpha \\left( y^{(i)}- h_\\theta(x^{(i)}) \\right) x^{(i)} $$\nAnd repeatedly run through the training set\nBatch vs SGD    Batch SGD     Slower convergence Faster convergence   Converge to minimum Oscillate about minimum    In most cases being close to the minimum is good enough, and therefore people choose SGD for the faster convergence\nThe Normal Equations Minimise explicitly by taking the derivatives w.r.t $J$ and setting to zero\nMatrix Derivatives For function $f : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}$\n$\\nabla_A f(A)$ is the Jacobian\nLeast-Squares Set $\\nabla_\\theta J(\\theta) = 0$\n$$ \\begin{aligned} \\nabla_\\theta J(\\theta) \u0026amp;= X^TX \\theta - X^T y \\cr X^TX \\theta \u0026amp;= X^T y \\end{aligned} $$ Which gives $$ \\theta = (X^TX)^{-1} X^T y $$\nProbabilistic Interpretation Assume target and inputs are related via $$ y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)} $$ where $\\epsilon^{(i)}$ are unmodelled effects/noise\nAssume $\\epsilon^{(i)}$ are Gaussian IID $$ p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( - \\frac{(\\epsilon^{(i)})^2}{2\\sigma^2} \\right) $$ which implies $$ p(y^{(i)} | x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( - \\frac{ (y^{(i)} - \\theta^T x^{(i)})^2 } { 2\\sigma^2 } \\right) $$ or $$ y^{(i)} | x^{(i)}; \\theta \\sim \\mathcal{N}(\\theta^T x ^{(i)}, \\sigma^2) $$\nLikelihood Function Given $X$ and $\\theta$, what is the distribution of $y^{(i)}$s? $$ L(\\theta) = L(\\theta; X, \\vec{y}) = p(\\vec{y}|X;\\theta) $$ Can be written as $$ \\begin{aligned} L(\\theta) \u0026amp;= \\prod_{i=1}^{n} p(y^{(i)} | x^{(i)}; \\theta) \\cr \u0026amp;= \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( - \\frac{ (y^{(i)} - \\theta^T x^{(i)})^2 } { 2\\sigma^2 } \\right) \\end{aligned} $$ The Principle of maximum likelihood says that we should choose $\\theta$ to make the probability as high as possible.\nMaximising probability is equivalent to maximising log likelihood, $l(\\theta)$. $$ l(\\theta) = n \\log \\frac{1}{\\sqrt{2\\pi\\sigma}} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} ( y^{(i)} - \\theta^T x^{(i)} )^2 $$ Which is the same as minimizing the least-squares costs $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} ( y^{(i)} - \\theta^T x^{(i)} )^2 $$ [*] This result does not depend on $\\sigma$\nLocally Weighted Linear Regression (LWR) Assuming sufficient training data, makes choice of features less important\nProcedure\n Fit $\\theta$ to minimise $\\sum_i w^{(i)} ( y^{(i)} - \\theta^T x^{(i)} )^2$ Output $\\theta^T x$  [*] Only difference is the weight $w^{(i)}$ $$ w^{(i)} = \\exp \\left( -\\frac{(x^{(i)} - x)^2}{2\\tau^2} \\right) $$ Which biases around the query point\nBandwidth : $\\tau$, controls the falloff distance\nThis is an example of a non-parametric Algorithm\n Requires data even after fitting Non-parametric means hypothesis $h$ grows linearly with size of training set  Classification Similar to regression, except $y$ only takes on a small number of discrete values\nLogistic Regression Start by ignoring the fact that $y$ is discrete\nUse linear regression with modified hypothesis function $$ h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}} $$ where $g(z) = \\frac{1}{1 + e^{-z}}$ is the logistic/sigmoid function\n$g(z)$ tends to $0$ or $1$ as $z \\rightarrow -\\infty$ or $z \\rightarrow \\infty$\nOther Functions\nOther functions can be used\nBut sigmoid has a useful function that the derivative: $$ g\u0026rsquo; = g(1 - g) $$\nProbabilistic Interpretation Assume $$ \\begin{aligned} P(y=1 | x; \\theta) \u0026amp;= h_\\theta(x) \\cr P(y=0 | x; \\theta) \u0026amp;= 1 - h_\\theta(x) \\end{aligned} $$ or equivalently $$ P(y | x; \\theta) = [h_\\theta(x)]^y [1-h_\\theta(x)]^{1-y} $$ For $n$ independent training examples the likelihood function is $$ \\begin{aligned} L(\\theta) \u0026amp;= p(y | X;\\theta) \\cr \u0026amp;= \\prod_{i=1}^{n} p(y^{(i)} | x^{(i)}; \\theta) \\cr \u0026amp;= \\prod_{i=1}^{n} \\left( h_\\theta(x^{(i)}) \\right)^{y^{(i)}} \\left(1 - h_\\theta(x^{(i)}) \\right)^{1-y^{(i)}} \\end{aligned} $$ Which can also be transformed into log likelihood $$ l(\\theta) = \\sum_{i=1}^{n} y^{(i)} \\log h(x^{(i)}) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) $$ And maximise using gradient ascent\nGradient Ascent rule The updates are $$ \\frac{\\partial}{\\partial \\theta_j} l(\\theta) = (y - h_\\theta(x)) x_j $$\n$$ \\theta_j := \\theta_j + \\alpha \\left( y^{(i)} - h_\\theta(x^{(i)}) \\right)\\ x_j^{(i)} $$\nNewton\u0026rsquo;s Method An alternative method to gradient descent is newton\u0026rsquo;s method\nTo Maximise $$ \\theta := \\theta - \\frac{l\u0026rsquo;(\\theta)}{l\u0026rsquo;\u0026rsquo;(\\theta)} $$\nVector-Valued Generalisation Newton-Raphson method $$ \\theta = \\theta - H^{-1} \\nabla_\\theta l(\\theta) $$ Where $H \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ is the Hessian $$ H_{ij} = \\frac{\\partial^2 l(\\theta)}{\\partial \\theta_i \\ \\partial \\theta_j} $$\nComparison with Gradient Descent Advantages\n Faster convergence Requires significantly fewer iterations  Disadvantages\n Each iteration can be more expensive Requires inverting the Hessian  Generalised Linear Models (GLM) Show that both linear regression and classification are special cases of Generalised Linear Models\nExponential Family The class of distributions in the exponential family is $$ p(y; \\eta) = b(y)\\ e^{\\eta^T\\ T(y)}\\ e^{-a(\\eta))} $$\n $\\eta$ : natural (canonical) parameter $T(y)$ : sufficient statistic Often let $T(y) = y$ $a(\\eta)$ : log partition function  $e^{-a(\\eta)}$ normalises the distribution and ensures it sums to $1$\nFamily/Set Defined by a (fixed) choice of $(T, a, b)$\nAnd parameterized by $\\eta$\nBernoulli With mean $\\phi$ defines a distribution over $y \\in {0,1}$\nVarying $\\phi$ gives different Bernoulli distributions\n$$ \\begin{aligned} p(y; \\phi) \u0026amp;= \\phi^y (1 - \\phi)^{1-y} \\cr \u0026amp;= \\exp \\left( y\\log\\phi + (1-y) \\log(1-\\phi) \\right) \\cr \u0026amp;= \\underbrace{1}_y \\exp ( \\underbrace{\\left(\\log \\phi/(1-\\phi) \\right)}_n \\ \\underbrace{y}_T + \\underbrace{\\log(1-\\phi)}_a ) \\end{aligned} $$\nWhich shows that is is part of the exponential family\nGaussian $$ \\begin{aligned} p(y; \\mu) \u0026amp;= \\frac{1}{\\sqrt{2\\pi}} \\left(-\\frac{1}{2} (y-\\mu)^2 \\right) \\cr \u0026amp;= \\underbrace{\\frac{1}{\\sqrt{2\\pi}} \\exp (-\\frac{1}{2} y^2)}_b \\cdot \\exp (\\underbrace{\\mu}_n \\underbrace{y}_T - \\underbrace{\\frac{1}{2} \\mu^2}_a ) \\end{aligned} $$\nOther Distributions  Multinomial Poisson Gamma Exponential Beta Dirichlet  Constructing GLMs Consider a classification/regression problem where we want to predict $y$ as a function of $x$\nMake three assumptions\n $y\\ |\\ x;\\theta \\sim \\text{ExpFamily}(\\eta)$ Given $x$ and $\\theta$, y is some exponential family distribution parameterised by $\\eta$ Predict $\\mathbb{E}[T(y)]$ given $x$ Natural parameter $\\eta$ and inputs $x$ are linear  $$ \\eta = \\theta^T x $$\nMany different/common types of distributions can be modelled as GLMs.\nCanonical Response Function Distribution\u0026rsquo;s mean as function of the natural paramter $$ g(\\eta) = \\mathbb{E}[T(y); \\eta] $$\nCanonical Link Function Inverse of the response function $$ g^{-1} $$\nOrdinary Least Squares  Target variable $y$ is continuous Model $y\\ |\\ x$ as Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$  From assumption 2 (Predict $\\mathbb{E}[T(y)]$ given $x$) $$ h_\\theta(x) = \\mathbb{E}[ y\\ |\\ x;\\theta] $$ Because it is a Gaussian $$ h_\\theta(x) = \\mu $$ From the formulation of the Gaussian as an exponential family distribution, $\\mu = \\eta$ $$ h_\\theta(x) = \\eta $$ Finally from assumption 3 $$ h_\\theta(x) = \\theta^T x $$\nLogistic Regression   $y \\in {0, 1 }$\n  Choose Bernoulli family to model $y\\ |\\ x$\n  $$ \\begin{aligned} h_\\theta(x) \u0026amp;= \\mathbb{E}[y\\ |\\ x;\\theta] \\cr \u0026amp;= \\phi \\cr \u0026amp;= 1/(1 + e^{-\\eta}) \\cr \u0026amp;= 1/1(1+e^{-\\theta^Tx}) \\end{aligned} $$\nSoftmax Regression  Classification problem with multiple classes/categories Model as a multinomial distribution  Express multinomial as an exponential family distribution\nParameterise Multinomial To parameterise over $k$ outcomes, we need to use $k-1$ parameters to prevent redundancy\n(The sum of all outcomes must be $1$)\nDefine $T(y) \\in \\mathbb{R}^{k-1}$ as $$ \\begin{aligned} T(1) \u0026amp;= { 1, 0, 0, \\dots, 0 } \\cr T(2) \u0026amp;= { 0, 1, 0, \\dots, 0 } \\cr T(k-1) \u0026amp;= { 0, 0, 0, \\dots, 1 } \\cr T(k) \u0026amp;= { 0, 0, 0, \\dots, 0 } \\end{aligned} $$ Define an indicator function: $$ \\begin{aligned} \u0026amp;1 \u0026amp;\u0026amp;\\text{if True} \\cr \u0026amp;0 \u0026amp;\u0026amp;\\text{otherwise} \\end{aligned} $$\n$$ (T(y))_i = 1 \\{ y=i \\} $$\nWhich gives $$ \\mathbb{E} [ (T(y))_i ] = P(y=i) = \\phi_i $$ Therefore the probability $$ \\begin{aligned} p(y; \\phi) \u0026amp;= \\phi_1^{1\\{y=1\\}} \\phi_2^{1\\{y=2\\}} \\dots \\phi_{k-1}^{1\\{y=k-1\\}} \\phi_k^{1-\\sum_{i=1}^{k-1}1\\{y=i\\}} \\cr \u0026amp;= \\phi_1^{(T(y))_1} \\phi_2^{(T(y))_2} \\dots \\phi_k^{1-\\sum_{i=1}^{k-1}1(T(y))_i} \\cr \u0026amp;= \\exp\\left( (T(y))_i \\log(\\phi_1) + \\dots + ((1 - \\Sigma_{i=1}^{k-1}(T(y))_i)) \\log(\\phi_k) \\right) \\end{aligned} $$\n$$ p(y;\\theta) = \\underbrace{1}_b \\exp {\\LARGE(} \\underbrace{ (T(y))_i \\log(\\frac{\\phi_1}{\\phi_k}) + \\dots + (T(y))_{k-1} \\log(\\frac{\\phi_{k-1}}{\\phi_k}) }_\\eta + \\underbrace{\\log(\\phi_k)}_a {\\LARGE)} $$\nWhich fits the exponential family form\nLink Function : $\\eta_i = \\log \\frac{\\phi_i}{\\phi_k}$\nResponse Function : $\\phi_i = \\frac{e^{n_i}}{\\sum_{j=1}^{k} e^{n_j}}$\nThis mapping from $\\eta \\rightarrow \\phi$ is also called the softmax function\nSoftmax regression is a generalisation of logistic regression\nIt ouputs estimated probabilities for every value of $i = 1, \\dots, k$\n","permalink":"https://andrelimzs.github.io/posts/machine-learning/cs239-supervised-learning/","summary":"Supervised Learning Terminology  Input (Features) : $x^{(i)}$ Output (Target) : $y^{(i)}$ Training example : $(x^{(i)}, y^{(i)})$ Training set : Set of training examples Hypothesis : $h(x)$  Types  Regression : Continuous values Classification : Discrete values  (Linear) Regression First decide the hypothesis function\nWhere the convention is to let $x_0 = 1$ $$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 $$\n$$ h(x) = \\sum_{i=0}^d \\theta_i x_i = \\theta^T x $$","title":"CS229 Supervised Learning"},{"content":"Review of Probability Theory Overview Probability is an important aspect of machine learning, and forms a useful basis for concepts. It is the study of uncertainty\nElements of Probability Defining probability on a set requires the following:\nSample Space : $\\Omega$\n The set of all possible outcomes of a random experiment Each outcome $\\omega \\in \\Omega$ is the complete description of the state at the end of the experiment  Event Space (Set of events) : $\\mathcal{F}$\n A set whose elements $A \\in \\mathcal{F}$ (events) are subsets of $\\Omega$ Eg: $A$ is the set of possible outcomes for an experiment  Probability Measure : $P : \\mathcal F \\rightarrow \\mathbb R$ which satisfies the Axioms of Probability\n $P(A) \\geq 0$ for all $A \\in \\mathcal F$ $P(\\Omega) = 1$ If $A_1, A_2, \\dots$ are disjoint events $P(\\cup_i A_i) = \\sum_i P(A_i)$  Conditional Probability For an event $B$ with non-zero probability\nThe conditional probability of $A$ given $B$ is $$ P(A|B) \\triangleq \\frac{P (A \\cap B)}{P(B)} $$\nIndependence If and only if\n $P(A \\cap B) = P(A) P(B)$ $P(A|B) = P(A)$  Random Variables A function $X : \\Omega \\rightarrow \\mathbb R$ that maps a sample space to a real-valued outcome\nCumulative Distribution Functions (CDF) Function $F_X : \\mathbb R \\rightarrow [0,1]$ that calculates the probability of an event $$ F_X(x) \\triangleq P(X \\leq x) $$ Properties\n $0 \\leq F_X(x) \\leq 1$ $\\underset{x \\rightarrow -\\infty}{\\lim} F_X(x) = 0$ $\\underset{x \\rightarrow \\infty}{\\lim} F_X(x) = 1$ $x \\leq y \\implies F_X(x) \\leq F_Y(y)$  Probability Mass Function (PMF) Function $p_X : \\Omega \\rightarrow \\mathbb R$ that directly specifies the probability of each value for a discrete RV $$ p_X(x) \\triangleq P(X=x) $$ Properties\n $0 \\leq p_X(x) \\leq 1$ $\\underset{x\\in Val(X)}{\\sum} p_X(x) = 1$ $\\underset{x\\in A}{\\sum} p_X(x) = P(X\\in A)$  Probability Density Functions (PDF) Derivative of the CDF $$ f_X(x) \\triangleq \\frac{dF_X(x)}{dx} $$ If the CDF is differentiable everywhere\nProperties\n $f_X(x) \\geq 0$ $\\int_{-\\infty}^{\\infty} f_X(x) = 1$ $\\int_{x\\in A} f_X(x) dx = P(X \\in A)$  Expectation The \u0026ldquo;weighted average\u0026rdquo; of the values that a distribution can take\nDiscrete RV $$ E[g(X)] \\triangleq \\sum_{x\\in Val(A)} g(x) p_X(x) $$ Continuous RV $$ E[g(X)] \\triangleq \\int_{-\\infty}^{\\infty} g(x) f_X(x)\\ dx $$ Properties\n $E[a] = a$ for any constant $a$ $E[a f(X)] = aE[f(X)]$ for any constant $a$ (Linearity of Expectation) $E[f(X) + g(X)] = E[f(X)] + E[g(X)]$ For discrete RV $E[1{ X=k }] = P(X = k)$  Variance Measure of how concentrated a distribution is around it\u0026rsquo;s mean $$ Var[X] \\triangleq E[(X - E(X))^2] $$ or equivalently $$ Var[x] = E[X^2] - E[X]^2 $$ Properties\n $Var[a] = 0$ for any constant $a$ $Var[a f(X)] = a^2 Var[f(X)]$ for any constant $a$  Common Random Variables Discrete Bernoulli\nEvent happens with probability $p$ $$ p(x) = \\left\\{ \\begin{aligned} \u0026amp;p \u0026amp;\u0026amp; \\text{if } p = 1 \\cr \u0026amp;1-p \u0026amp;\u0026amp; \\text{if } p = 0 \\end{aligned} \\right. $$ Binomial\nNumber of outcomes in $n$ independent events $$ p(x) = \\begin{pmatrix} n \\cr x \\end{pmatrix} p^x (1-p^{n-x}) $$ Geometric\nNumber of independent events until outcome is true $$ p(x) = p (1-p)^{x-1} $$ Poisson\nProbability distribution over nonnegative integers $$ p(x) = e^{-\\lambda} \\frac{\\lambda^x}{x!} $$\nContinuous Uniform\nEqual probability density to every value between $a$ and $b$ $$ f(x) = \\left\\{ \\begin{aligned} \u0026amp;\\frac{1}{b-a} \u0026amp;\u0026amp; \\text{if } a\\leq x \\leq b \\cr \u0026amp;0 \u0026amp;\u0026amp; \\text{otherwise} \\end{aligned} \\right. $$\nExponential\nDecaying probability density over nonnegative reals $$ f(x) = \\left\\{ \\begin{aligned} \u0026amp;\\lambda e^{-\\lambda x} \u0026amp;\u0026amp; \\text{if } x\\geq 0 \\cr \u0026amp;0 \u0026amp;\u0026amp; \\text{otherwise} \\end{aligned} \\right. $$ Normal\nGaussian distribution $$ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left( -\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right) $$\nJoint and Marginal Distributions CDF Of $X$ and $Y$ $$ F_{XY}(x,y) = P(X \\leq x, Y \\leq y) $$ The joint CDF is related to the marginal CDF $F_X(x) / F_Y(y)$ via $$ \\begin{aligned} F_X(x) \u0026amp;= \\lim_{y \\rightarrow \\infty} F_{XY}(x,y) dy \\cr F_Y(y) \u0026amp;= \\lim_{x \\rightarrow \\infty} F_{XY}(x,y) dx \\end{aligned} $$ Properties\n $0 \\leq F_{XY}(x,y) \\leq 1$ $\\lim_{x,y \\rightarrow \\infty} F_{XY}(x,y) = 1$ $\\lim_{x,y \\rightarrow -\\infty} F_{XY}(x,y) = 0$ $F_X(x) = \\lim_{y \\rightarrow \\infty} F_{XY}(x,y)$  PMF $$ p_{XY}(x,y) = P(X=x, Y=y) $$\nAnd is related to the marginal PMF via $$ \\begin{aligned} p_X(x) \u0026amp;= \\sum_y p_XY(x,y) \\cr p_Y(y) \u0026amp;= \\sum_x p_XY(x,y) \\end{aligned} $$\nPDF $$ f_{XY}(x,y) = \\frac {\\partial^2 F_{XY}(x,y)} {\\partial x \\partial y} $$\nWhere $$ \\begin{aligned} \\iint_{x\\in A} f_XY(x,y)\\ dx\\ dy = P((X,Y) \\in A) \\cr \\end{aligned} $$ And $$ \\begin{aligned} f_X(x) = \\int_{-\\infty}^{\\infty} f_{XY}(x,y) dy \\cr f_Y(y) = \\int_{-\\infty}^{\\infty} f_{XY}(x,y) dx \\end{aligned} $$\nConditional Distribution  What is the probability distribution over $Y$ given that we know $X = x$?\n Discrete $$ p_{Y|X} (y|x) = \\frac{p_{XY}(x,y)}{p_X(x)} $$ Continuous $$ f_{Y|X} (y|x) = \\frac{f_{XY}(x,y)}{f_X(x)} $$\nBayes\u0026rsquo;s Rule Used to derive conditional probability of one variable given another $$ P(A|B)\\ P(B) = P(B|A)\\ P(A) $$ And therefore $$ P(A|B) = \\frac{ P(B|A)\\ P(A) }{ P(B) } $$\nIndependence Two RVs $X$ and $Y$ are independent if $$ P(X,Y) = P(X)\\ P(Y) $$ for all values of $x$ and $y$\nExpectation and Covariance Let $X$ and $Y$ be RVs and $g : \\mathbb{R}^2 \\rightarrow \\mathbb R$\nExpectation Discrete $$ E{g(X,Y)} \\triangleq \\sum_{x \\in Val(X)} \\sum_{y \\in Val(Y)} g(x,y) p_{XY}(x,y) $$ Continuous $$ E[g(X,Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f_{XY}(x,y)\\ dx\\ dy $$\nConvariance $$ \\begin{aligned} Cov[X,Y] \u0026amp;\\triangleq E[(X-E[X])(Y-E(Y))] \\cr \u0026amp;= E[XY] - E[X] E[Y] \\end{aligned} $$\nUncorrelated : $Cov[X,Y] = 0$\nProperties\n (Linearity of Expectation) $Var[X+Y] = Var[ X ] + Var[Y] + 2 Cov[X,Y]$ If $X$ and $Y$ independent $\\implies E[f(X)g(Y)] = E[f(X)]\\ E[g(Y)]$  Multiple RV The notation for two RVs can be generalised to $n$ RVs\nProperties Chain Rule $$ f(x_1, x_2, \\dots, x_n) = f(x_1) f(x_2 | x_1) \\prod_{i=3}^{n} f(x_i | x_1, \\dots, x_{i-1}) $$\nIndependence Events $A_1, \\dots, A_k$ are mutually independent if $$ P(\\underset{i \\in S}{\\cap} A_i) = \\prod_{i\\in S} P(A_i) $$ Random variables $X_1, \\dots, X_n$ are independent if $$ f(x_1, \\dots, x_n) = f(x_1) f(x_2) \\dots f(x_n) $$\nRandom Vectors A vector of random variables is a random vector\nIt is a mapping from $\\Omega \\rightarrow \\mathbb R^n$\nIt is an alternative notation for multiple RVs $$ X = \\begin{bmatrix} X_1 \\cr \\vdots \\cr X_n \\end{bmatrix} $$\nExpectation $$ E[g(X)] = \\begin{bmatrix} E[g_1(X)] \\cr \\vdots \\cr E[g_m(X)] \\end{bmatrix} $$\nCovariance $$ \\Sigma = \\begin{bmatrix} E[X_1^2] \u0026amp; \\dots \u0026amp; E[X_1 X_n] \\cr \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\cr E[X_n X_1] \u0026amp; \\dots \u0026amp; E[X_n^2] \\end{bmatrix} $$\nor $$ \\begin{aligned} \\Sigma \u0026amp;= E[XX^T] - E[X]\\ E[X]^T \\cr \u0026amp;= E[(X-E[X])(X-E[X])^T] \\end{aligned} $$\nProperties\n Positive semidefinite : $\\Sigma \\succeq 0$ Symmetric : $\\Sigma = \\Sigma^T$  Multivariate Gaussian Distribution Particularly important example\nHas a\nMean : $\\mu \\in \\mathbb R^n$\nCovariance : $\\Sigma \\in \\mathbb S_{++}^n$ (symmetric positive definite matrices) $$ f_{X_1, \\dots, X_n} (x_1, \\dots, x_n; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{\\pi/2} |\\Sigma|^{1/2} } \\exp \\left( -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right) $$ Written as $X \\sim \\mathcal N(\\mu, \\Sigma)$\nUsefulness\nComes up extremely often due to the Central Limit Theorem (CLT) - A large number of independent RVs will tend toward a Gaussian distribution.\nUseful because it has simple closed-form solutions.\n","permalink":"https://andrelimzs.github.io/posts/machine-learning/cs239-probability-theory/","summary":"Review of Probability Theory Overview Probability is an important aspect of machine learning, and forms a useful basis for concepts. It is the study of uncertainty\nElements of Probability Defining probability on a set requires the following:\nSample Space : $\\Omega$\n The set of all possible outcomes of a random experiment Each outcome $\\omega \\in \\Omega$ is the complete description of the state at the end of the experiment  Event Space (Set of events) : $\\mathcal{F}$","title":"CS229 Probability Theory"}]