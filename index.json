[{"content":"Objective Order of growth is a simple characterization of an algorithm\u0026rsquo;s efficiency and allows for comparison between algorithms.\nAsymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.\n Asymptotic Notation Notation is defined as a function on domain $\\mathbb{N} = { 0,1,2,\\dots }$, the set of natural numbers. It is mainly used to characterise running time, but can also be used on memory/space/etc.\n$\\Theta$-notation  Encloses the function from above and below\n Useful in analyzing the average-case complexity of an algorithm. $$ \\begin{aligned} \\Theta \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c_1,\\ c_2,\\ n_0 \\text{ such that } \\\\ 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ $\\Theta(n)$ is a set, but it is usually written as $f(n) = \\Theta(n)$.\n $f(n)$ belongs to set $\\Theta(g(n))$ if $c_1 \\cdot g(n)$ and $c_2 \\cdot g(n)$ can sandwidch the function, for large $n \\geq n_0$.\nThe function $f(n)$ is equal to $g(n)$ to within a constant factor, which makes $g(n)$ an asymptotically tight bound for $f(n)$.\nThe definition of $\\Theta(n)$ requires all $f(n) \\in \\Theta(g(n))$ be asymptotically nonnegative ($f(n) \\geq 0$ as $n \\rightarrow \\infty$. This implies that $g(n)$ is also asymptotically nonnegative so that $\\Theta(g(n))$ is not empty. Therefore assume that every function in $\\Theta$-notation is asymptotically nonnegative.\n Example Show that $ \\frac{3}{4} n^2 - 5n + \\frac{3}{n} = \\Theta(n^2) $\n$c_1 n^2 \\leq \\frac{3}{4} n^2 - 5n + \\frac{3}{n} \\leq c_2 n^2$ for all $n \\geq n_0$\n$c_1 \\leq \\frac{3}{4} - \\frac{5}{n} + \\frac{3}{n^2} \\leq c_2$\n$c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 10$ satisfy the inequality\n  Example Show that $5n^3 \\neq \\Theta(n^2)$\n$c_1 \\leq 5 \\frac{n^3}{n^2} \\leq c_2$\nWhich cannot hold for arbitrarily large $n$\n This results in two observations:\n The lower-order terms can be ignored (They become of the form $1 / n^k$ which go to $0$) Coefficients can be ignored (Can be divided into $c_1, c_2$)  Constants are degree-0 polynomials, which can be expressed as $\\Theta(n^0)$ or more commonly $\\Theta(1)$.\n$O$-notation  Asymptotic upper bound\n Provides an upper bound on a function to within a constant factor. $$ \\begin{aligned} O \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c,\\ n_0 \\text{ such that } \\\\ 0 \\leq f(n) \\leq c g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ $\\Theta(g(n))$ implies $O(g(n))$ becauses $\\Theta$-notation is stronger than $O$-notation. $$ \\Theta(g(n)) \\subseteq O(g(n)) $$ $O$-notation provides an asymptotic upper bound but not a tight one. Distinguishing between both is stanard in algorithms literature, but not in informal use.\nInspection can often reveal the $O$-notation running time. For example, double nested loops are $O(n^2)$.\nWhen $O$-notation is used to bound the worst-case running time, it also bounds every input. This is different from $\\Theta$-notation, where the worse-case does not imply the running time for every input.\n$\\Omega$-notation  Asymptotic lower bound\n $$ \\begin{aligned} O \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c,\\ n_0 \\text{ such that } \\\\ 0 \\leq c g(n) \\leq f(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$\nTheorem 3.1 For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n)) and f(n) = \\Omega(g(n))$\nCommonly used to get asymptotic upper/lower bounds ( $O$ / $\\Omega$ ) from tight bounds ( $\\Theta$ )\n Example\n$ \\Omega(n^2) = an^2 + bn + c$ and $O(n^2) = an^2 + bn + c$\nimply that $ \\Theta(n^2) = an^2 + bn + c $\n Asymptotic Notation in Equations   $n = O(n^2)$ Refers to set membership $n \\in O(n^2)$\n  In a formula ( $\\dots = 2n^2 + \\Theta(n)$ ) Stand-in for some anonymous function ( $\\Theta(n) = f(n)$ )\n  The number of anonymous functions is equal to the number of times it appears Eg: $\\sum_i^n{ O(i) }$ appears once and not $O(1) + \\dots + O(n)$\n  On the left-hand side (LHS) (It usually appears on the RHS) Means that there is always a way to choose LHS and RHS to make the equation hold The RHS is a coarser level of detail than the LHS\n  Can be chained Each equation can be interpreted separately\n  o-notation $o$-notation is a bound that is not asymptotically tight. ($O$-notation may or may not be asymptotically tight.) $$ \\begin{aligned} o \\big( g(n) \\big) = \\{ f(n) :\\ \\text{for any +ve constant } c\u0026gt;0, \\text{ there exists } \\\\ \\text{a constant } n_0 \u0026gt; 0 \\text{ s.t. } 0 \\leq f(n) \u0026lt; c g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ The main difference between $o$-notation and $O$-notation is that the bound $0 \\leq f(n) \\leq cg(n)$ holds for all constants $c \u0026gt; 0$ instead of some constant.\n$\\omega$-notation Lower bound that is not asymptotically tight. $$ \\begin{aligned} o \\big( g(n) \\big) = \\{ f(n) :\\ \\text{for any +ve constant } c\u0026gt;0, \\text{ there exists } \\\\ \\text{a constant } n_0 \u0026gt; 0 \\text{ s.t. } 0 \\leq c g(n) \u0026lt; f(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$\nComparing Functions Transitivity $f(n) = \\Theta / O / \\Omega (g(n))$ and $g(n) = \\Theta / O / \\Omega (h(n))$\nImplies $f(n) = \\Theta / O / \\Omega (h(n))$\nReflexivity $f(n) = \\Theta / O / \\Omega (f(n))$\nSymmetry $f(n) = \\Theta(g(n)) \\Leftrightarrow g(n) = \\Theta(f(n))$\nTranspose Symmetry $f(n) = O(g(n)) \\Leftrightarrow g(n) = \\Omega(f(n))$\n$f(n) = o(g(n)) \\Leftrightarrow g(n) = \\omega(f(n))$\nThese properties allow for an analogy between asymptotic notation and inequalities.\n$f(n) = O(g(n)) \\quad\\leftrightarrow\\quad a \\leq b \\quad | \\quad f(n) = o(g(n)) \\quad\\leftrightarrow\\quad a \u0026lt; b$\n$f(n) = \\Omega(g(n)) \\quad\\leftrightarrow\\quad a \\geq b \\quad | \\quad f(n) = \\omega(g(n)) \\quad\\leftrightarrow\\quad a \u0026gt; b$\n$f(n) = \\Theta(g(n)) \\quad\\leftrightarrow\\quad a = b$\n Standard Notations Monotonicity $f(n)$ is monotonically increasing if $m \\leq n$ implies $f(m) \\leq f(n)$.\n$f(n)$ is strictly increasing if $m\u0026lt;n$ implies $f(m) \u0026lt; f(n)$\nFloors and Ceilings   Floor $\\lfloor x \\rfloor$ : The least integer $\\leq x$\n  Ceiling $\\lceil x \\rceil$ : The least integer $\\geq x$\n  Both are monotonically increasing\nProperties (for real $x$):\n  $x-1 \u0026lt; \\lfloor x \\rfloor \\leq x \\leq \\lceil x \\rceil \u0026lt; x+1$\n  $\\lceil n/2 \\rceil + \\lfloor n/2 \\rfloor = n$\n  $ \\lceil \\frac{1}{b} \\lceil \\frac{x}{a} \\rceil \\rceil = \\lceil \\frac{x}{ab} \\rceil$\n  $\\lceil \\frac{a}{b} \\rceil = \\frac{a + b - 1}{b}$\n  Modular Arithmetic For integer $a$ and positive integer $n$, $a \\text{ mod } n$ is the remainder of $a/n$\n$a \\mod n = a - n \\lfloor a/n \\rfloor$\nAnd therefore $0 \\leq a \\mod n \\leq n$\nEquivalent : $a \\equiv b\\ (\\text{mod } n )$ if they have the same remainder.\nThis also implies that $n$ is a divisor of $a-b$\nPolynomials $$ p(n) = \\sum^d_{i=0} {a_i n^i} $$\nFor integer $d \\geq 0$\nCoefficients: $a_i$ and $a_d \\neq 0$\nAsymptotically positive $ \\Leftrightarrow a_d \u0026gt;0$\n","permalink":"https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/","summary":"Objective Order of growth is a simple characterization of an algorithm\u0026rsquo;s efficiency and allows for comparison between algorithms.\nAsymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.\n Asymptotic Notation Notation is defined as a function on domain $\\mathbb{N} = { 0,1,2,\\dots }$, the set of natural numbers.","title":"Growth of Functions"},{"content":"Objective The main objective of CLRS is to introduce frameworks to design and analyze algorithms.\nTerminology Loop Invariant Something used to prove the correctness of an algorithm, with three properties:\n Initialization It is true prior to the first iteration of the loop Maintenance If it is true before an iteration, it remains true after that iteration Termination It helps show that the algorithm is correct after the loop terminates Commonly used in combination with the termination condition  Input Size, $n$ Depends on the problem, but commonly:\n Number of items in the input (sorting, FFT) Number of bits (multiplication)  It can also be described by more than one number, for example:\n Graph vertices and edges.  Runtime, $T(n)$  Number of primitive operations, or \u0026lsquo;steps\u0026rsquo; executed\n Assume each line requires a constant amount of time. The running time of an algorithm is now: $$ \\sum \\text{each individual line} \\times \\text{num of times executed} $$\nRandom-access Machine (RAM) Model Instructions The RAM model contains commonly found instructions:\n Arithmetic (add, subtract, multiple, divide, remainder, floor, ceiling) Data Movement Load, store copy Control (Conditional, unconditional branch, subroutine call, return)  Data Types  Integers Floating point  And assume a limit on size of each word of data.\n For inputs of size $n$, assume integers are represented by $c \\log n$ bits for some constant $c \\geq 1$.\n $c \\geq 1$ : So that each word can hold the value of $n$ to allow us to index the last element $c$ constant : So that word size cannot grow arbitrarily large   Grey Area - Instructions There are some instructions included in real computers that are not in the list above. For example, exponentiation. CLRS assumes that they are constant-time operations for small exponent values.\nMemory Hierarchy The RAM model does not model memory hierarchy, such as caches or virtual memory. This is usually sufficient, and RAM models are excellent at predicting performance of real machines.\nAnalyzing Algorithms  \u0026ldquo;Predict the resources an algorithm requires\u0026rdquo;\n This resource is most often computation time.\nThis kind of analysis requires a model of the hardware that\u0026rsquo;s being used to detail the costs of the different capabilities. Most of CLRS assumes the one-processor random-access machine (RAM) model. Instructions are executed sequentially.\nAnalysis is focused on worst-case running time, for three reasons:\n Gives an upper bound Happens fairly often (for some algorithms) Close to the \u0026ldquo;average case\u0026rdquo; for many algorithms  Later chapters will explore average-case running time via probabilistic analysis. There are also randomized algorithms that have an expected running time.\nOrder of Growth The analysis can be further simplified by reducing it to the rate of growth/order of growth. Therefore ignore:\n All lower order terms (Only the leading term is significant as $n \\rightarrow \\infty$) Constant coefficients (Less significant as $n \\rightarrow \\infty$)  This means that worst-case running time is only really relevant for very large $n$.\n","permalink":"https://andrelimzs.github.io/posts/algorithm-design/getting-started/","summary":"Objective The main objective of CLRS is to introduce frameworks to design and analyze algorithms.\nTerminology Loop Invariant Something used to prove the correctness of an algorithm, with three properties:\n Initialization It is true prior to the first iteration of the loop Maintenance If it is true before an iteration, it remains true after that iteration Termination It helps show that the algorithm is correct after the loop terminates Commonly used in combination with the termination condition  Input Size, $n$ Depends on the problem, but commonly:","title":"CLRS - Getting Started"},{"content":"title: \u0026#34;Reinforcement Learning\u0026#34; date: 2022-04-08T24:00:00+08:00 draft: false math: true Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$\nPolicy Function $\\pi : S \\mapsto A$ maps states to actions\nExecute a policy by following the prescribed action\nValue Function The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\\pi$ $$ V^\\pi(s) = \\mathbb{E} \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2)\n \\dots | s_0 = s,\\pi \\right] $$  Bellman Equation Given a fixed policy $\\pi$, it\u0026rsquo;s value function $V^\\pi$ satisfies the Bellman equations $$ V^\\pi(s) = R(s) + \\gamma \\sum_{s\u0026rsquo; \\in S} P_{s\\pi(s)}(s\u0026rsquo;) V^\\pi(s\u0026rsquo;) $$ Which consists of two terms:\n Immediate reward $R(s)$ Get for starting in $s$ Expected sum of future discounted rewards Can we rewritten as $E_{s\u0026rsquo;\\sim P_{s\\pi(s)}}[V^\\pi(s\u0026rsquo;)]$ The expected sum of starting in $s\u0026rsquo;$ where $s\u0026rsquo;$ is distributed according to to $P_{s\\pi(s)}$  Can be used to efficiently solve for the value function $V^\\pi$\n Write down one equation for every state Gives a set of $|S|$ linear equations in $|S|$ variables  Optimal Value Function Best possible expected sum of discounted rewards which can be attained using any policy $$ V^(s) = \\max_\\pi V^\\pi (s) $$ Which has it\u0026rsquo;s own version of the Bellman Equation $$ V^(s) = R(s) + \\max_{a\\in A} \\gamma \\sum_{s\u0026rsquo;\\in S} P_{sa} (s\u0026rsquo;) V^*(s\u0026rsquo;) $$ The second term is a $\\max$ over all possible actions because that is the optimal reward.\nOptimal Policy $$ \\pi^(s) = \\arg \\max_{a\\in A} \\sum_{s\u0026rsquo;\\in S} P_{sa}(s\u0026rsquo;) V^(s\u0026rsquo;) $$\nWhich gives the action that attains the $\\max$ in the optimal value function\nFor every state $s$ and policy $\\pi$ $$ V^(s) = V^{\\pi}(s) \\geq V^\\pi(s) $$ Which says that\n The value function for the optimal policy is equal to the optimal value function for every state $s$\n  The value function for the optimal policy is greater than or equal to every other policy\n  $\\pi^*$ is the optimal policy for all states  Value Iteration and Policy Iteration Two efficient algorithms for solving finite-state MDPs with known transition probabilities\nValue Iteration  For each state s, initialize $V(s) := 0$ for until convergence do For every state, update $$ V(s) := R(s) + \\max_{a\\in A} \\gamma \\sum_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) $$\n  Repeatedly update estimated value function using the Bellman equation  Can be:\n Synchronous  Compute new values for all states and update at once Can be viewed as implementing a Bellman Backup Operator, which maps the current estimate to a new estimate   Asynchronous  Loop over states and update one at a time    Both algorithms will cause $V$ to converge to $V^*$\nPolicy Iteration  Initialise $\\pi$ randomly\nfor until convergence do\n​\tLet $V := V^\\pi$\n​\tFor each state $s$ $$ \\pi(s) := \\arg \\max_{a \\in A} \\sum_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) $$\n  Repeatedly computes value function for the current policy Update policy using current value function  This is the policy that is greedy w.r.t. $V$   The value function can be updated using Bellman equations After a finite number of iterations, $V$ will converge to $V^$ and $\\pi$ to $\\pi^$  Value vs Policy Iteration  No universal agreement which is better Small MDPs : Policy iteration tends to be fast Larger MDPs : Value iteration might be faster Policy iteration can reach the exact $V^*$ Value iteration will have small non-zero error  Learning a Model for an MDP   In many realistic problems, we do not know the state transition probabilities and rewards\n  Must be estimated from data\n  Given enough trials, can derive maximum likelihood estimates for the state transition probabilities $$ P_{sa}(s\u0026rsquo;) = \\frac{# \\text{ times } a \\text{ led to } s\u0026rsquo; } {# \\text{ times we took } a} $$ In the case of $\\frac{0}{0}$, can set $P$ to $\\frac{1}{|S|}$ (estimate as uniform)\nEfficient Update Keep count of numerator \u0026amp; denominator\nSolving Learned MDP  Can use policy/value iteration  Continuous State MDPs  Infinite number of states Consider settings where state space is $S = \\mathbb{R}^d$  Discretization  Simplest way Discretize state space Use value/policy iteration  Downsides\n Fairly naive representation for $V^*$ Assumes the value function is constant over each discretization interval Piecewise constant representation isn\u0026rsquo;t good for many smooth functions  Little smoothing over inputs No generalization over different grid cells   Suffers from curse of dimensionality  For $S = \\mathbb{R}^d$, we will have $k^d$ states    Rule of Thumb\n Works well for 1D/2D cases Can work for 3D/4D cases Rarely works above 6D  Value Function Approximation  Approximate $V^*$ directly  Using Model/Simulator  Assume we have a model/simulator for the MDP Any black-box with  Input : State $s_t$, Action $a_t$ Output : Next-state $s_{t+1}$ sampled    Getting the Model Using physics simulation  Can use off-the-shelf physics simulation  Data collected in the MDP  Can be done with:  Random actions Executing some policy Other way of choosing actions [?] Control law?   Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$  Example : Learn linear model\nusing linear regression $$ \\arg \\min_{A,B} \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\left\\Vert s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)}) \\right\\Vert^2_2 $$ Can build either a deterministic $$ s_{t+1} = As_t + Ba_t $$ or stochastic model $$ s_{t+1} = As_t + Ba_t + \\epsilon_t $$ with $\\epsilon_t$ being a noise term usually modelled as $\\epsilon_t \\sim \\mathcal{N}(0,\\Sigma)$\nor non-linear function $$ s_{t+1} = A\\phi_s(s_t) + B\\phi_a(a_t) $$ Eg: Locally weighted linear regression\nFitted Value Iteration  Algorithm for approximating the value function of a continuous state MDP Assume state space $S = \\mathcal{R}^d$ But action space $A$ is small and discrete  Perform value iteration update $$ \\begin{aligned} V(s) :\u0026amp;= R(s) + \\gamma \\max_a \\int_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) ds\u0026rsquo; \\ \u0026amp;= R(s) + \\gamma \\max_a E_{s\u0026rsquo; \\sim P_{sa}} [V(s\u0026rsquo;)] \\end{aligned} $$\n Main difference is the $\\int$ instead of $\\sum$ Approximately update over a finite sample of states $s^{(1)}, \\dots, s^{(n)}$ Use supervised learning algorithm, linear regression to approximate value function as linear/nonlinear function of the states  $$ V(s) = \\theta^T \\phi(s) $$\nAlgorithm\n Compute $y^{(i)}$, the approximation to $R(s) + \\gamma \\max_a E_{s\u0026rsquo; \\sim P_{sa}} [V(s\u0026rsquo;)]$ (RHS)\nUse supervised learning to get $V(s) = y^{(i)}$\nRepeat\n Convergence  Cannot be proved to converge But in practice it often does, and works well for many problems  Deterministic Simplification  Can set $k=1$ Because expectation of a deterministic distribution requires 1 sample  Policy  Outputs $V$, an approximation of $V^*$ Implicitly defines policy  When in state $s$, choose $$ \\arg \\max_a E_{s\u0026rsquo;\\sim P_{sa}} [V(s\u0026rsquo;)] $$\n Process for computing/approximating this is similar to the fitted value iteration algorithm  ","permalink":"https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/","summary":"title: \u0026#34;Reinforcement Learning\u0026#34; date: 2022-04-08T24:00:00+08:00 draft: false math: true Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$","title":""}]