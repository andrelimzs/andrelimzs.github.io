[{"content":"Objective Order of growth is a simple characterization of an algorithm\u0026rsquo;s efficiency and allows for comparison between algorithms.\nAsymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.\nAsymptotic Notation Notation is defined as a function on domain $\\mathbb{N} = { 0,1,2,\\dots }$, the set of natural numbers. It is mainly used to characterise running time, but can also be used on memory/space/etc.\n$\\Theta$-notation  Encloses the function from above and below\n Useful in analyzing the average-case complexity of an algorithm. $$ \\begin{aligned} \\Theta \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c_1,\\ c_2,\\ n_0 \\text{ such that } \\\\ 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ $\\Theta(n)$ is a set, but it is usually written as $f(n) = \\Theta(n)$.\n $f(n)$ belongs to set $\\Theta(g(n))$ if $c_1 \\cdot g(n)$ and $c_2 \\cdot g(n)$ can sandwidch the function, for large $n \\geq n_0$.\nThe function $f(n)$ is equal to $g(n)$ to within a constant factor, which makes $g(n)$ an asymptotically tight bound for $f(n)$.\nThe definition of $\\Theta(n)$ requires all $f(n) \\in \\Theta(g(n))$ be asymptotically nonnegative ($f(n) \\geq 0$ as $n \\rightarrow \\infty$. This implies that $g(n)$ is also asymptotically nonnegative so that $\\Theta(g(n))$ is not empty. Therefore assume that every function in $\\Theta$-notation is asymptotically nonnegative.\n Example Show that $ \\frac{3}{4} n^2 - 5n + \\frac{3}{n} = \\Theta(n^2) $\n$c_1 n^2 \\leq \\frac{3}{4} n^2 - 5n + \\frac{3}{n} \\leq c_2 n^2$ for all $n \\geq n_0$\n$c_1 \\leq \\frac{3}{4} - \\frac{5}{n} + \\frac{3}{n^2} \\leq c_2$\n$c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 10$ satisfy the inequality\n  Example Show that $5n^3 \\neq \\Theta(n^2)$\n$c_1 \\leq 5 \\frac{n^3}{n^2} \\leq c_2$\nWhich cannot hold for arbitrarily large $n$\n This results in two observations:\n The lower-order terms can be ignored (They become of the form $1 / n^k$ which go to $0$) Coefficients can be ignored (Can be divided into $c_1, c_2$)  Constants are degree-0 polynomials, which can be expressed as $\\Theta(n^0)$ or more commonly $\\Theta(1)$.\n$O$-notation  Asymptotic upper bound\n Provides an upper bound on a function to within a constant factor. $$ \\begin{aligned} O \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c,\\ n_0 \\text{ such that } \\\\ 0 \\leq f(n) \\leq c g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ $\\Theta(g(n))$ implies $O(g(n))$ becauses $\\Theta$-notation is stronger than $O$-notation. $$ \\Theta(g(n)) \\subseteq O(g(n)) $$ $O$-notation provides an asymptotic upper bound but not a tight one. Distinguishing between both is stanard in algorithms literature, but not in informal use.\nInspection can often reveal the $O$-notation running time. For example, double nested loops are $O(n^2)$.\nWhen $O$-notation is used to bound the worst-case running time, it also bounds every input. This is different from $\\Theta$-notation, where the worse-case does not imply the running time for every input.\n$\\Omega$-notation  Asymptotic lower bound\n $$ \\begin{aligned} O \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c,\\ n_0 \\text{ such that } \\\\ 0 \\leq c g(n) \\leq f(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$\nTheorem 3.1 For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n)) and f(n) = \\Omega(g(n))$\nCommonly used to get asymptotic upper/lower bounds ( $O$ / $\\Omega$ ) from tight bounds ( $\\Theta$ )\n Example\n$ \\Omega(n^2) = an^2 + bn + c$ and $O(n^2) = an^2 + bn + c$\nimply that $ \\Theta(n^2) = an^2 + bn + c $\n ","permalink":"https://andrelimzs.github.io/posts/growth-of-functions/","summary":"Objective Order of growth is a simple characterization of an algorithm\u0026rsquo;s efficiency and allows for comparison between algorithms.\nAsymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.\nAsymptotic Notation Notation is defined as a function on domain $\\mathbb{N} = { 0,1,2,\\dots }$, the set of natural numbers.","title":"Growth of Functions"},{"content":"Objective The main objective of CLRS is to introduce frameworks to design and analyze algorithms.\nTerminology Loop Invariant Something used to prove the correctness of an algorithm, with three properties:\n Initialization It is true prior to the first iteration of the loop Maintenance If it is true before an iteration, it remains true after that iteration Termination It helps show that the algorithm is correct after the loop terminates Commonly used in combination with the termination condition  Input Size, $n$ Depends on the problem, but commonly:\n Number of items in the input (sorting, FFT) Number of bits (multiplication)  It can also be described by more than one number, for example:\n Graph vertices and edges.  Runtime, $T(n)$  Number of primitive operations, or \u0026lsquo;steps\u0026rsquo; executed\n Assume each line requires a constant amount of time. The running time of an algorithm is now: $$ \\sum \\text{each individual line} \\times \\text{num of times executed} $$\nRandom-access Machine (RAM) Model Instructions The RAM model contains commonly found instructions:\n Arithmetic (add, subtract, multiple, divide, remainder, floor, ceiling) Data Movement Load, store copy Control (Conditional, unconditional branch, subroutine call, return)  Data Types  Integers Floating point  And assume a limit on size of each word of data.\n For inputs of size $n$, assume integers are represented by $c \\log n$ bits for some constant $c \\geq 1$.\n $c \\geq 1$ : So that each word can hold the value of $n$ to allow us to index the last element $c$ constant : So that word size cannot grow arbitrarily large   Grey Area - Instructions There are some instructions included in real computers that are not in the list above. For example, exponentiation. CLRS assumes that they are constant-time operations for small exponent values.\nMemory Hierarchy The RAM model does not model memory hierarchy, such as caches or virtual memory. This is usually sufficient, and RAM models are excellent at predicting performance of real machines.\nAnalyzing Algorithms  \u0026ldquo;Predict the resources an algorithm requires\u0026rdquo;\n This resource is most often computation time.\nThis kind of analysis requires a model of the hardware that\u0026rsquo;s being used to detail the costs of the different capabilities. Most of CLRS assumes the one-processor random-access machine (RAM) model. Instructions are executed sequentially.\nAnalysis is focused on worst-case running time, for three reasons:\n Gives an upper bound Happens fairly often (for some algorithms) Close to the \u0026ldquo;average case\u0026rdquo; for many algorithms  Later chapters will explore average-case running time via probabilistic analysis. There are also randomized algorithms that have an expected running time.\nOrder of Growth The analysis can be further simplified by reducing it to the rate of growth/order of growth. Therefore ignore:\n All lower order terms (Only the leading term is significant as $n \\rightarrow \\infty$) Constant coefficients (Less significant as $n \\rightarrow \\infty$)  This means that worst-case running time is only really relevant for very large $n$.\n","permalink":"https://andrelimzs.github.io/posts/getting-started/","summary":"Objective The main objective of CLRS is to introduce frameworks to design and analyze algorithms.\nTerminology Loop Invariant Something used to prove the correctness of an algorithm, with three properties:\n Initialization It is true prior to the first iteration of the loop Maintenance If it is true before an iteration, it remains true after that iteration Termination It helps show that the algorithm is correct after the loop terminates Commonly used in combination with the termination condition  Input Size, $n$ Depends on the problem, but commonly:","title":"CLRS - Getting Started"}]