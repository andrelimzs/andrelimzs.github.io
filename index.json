[{"content":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$\nPolicy Function $\\pi : S \\mapsto A$ maps states to actions\nExecute a policy by following the prescribed action\nValue Function The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\\pi$ $$ V^\\pi(s) = \\mathbb{E} \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots | s_0 = s,\\pi \\right] $$\nBellman Equation Given a fixed policy $\\pi$, it\u0026rsquo;s value function $V^\\pi$ satisfies the Bellman equations $$ V^\\pi(s) = R(s) + \\gamma \\sum_{s\u0026rsquo; \\in S} P_{s\\pi(s)}(s\u0026rsquo;) V^\\pi(s\u0026rsquo;) $$ Which consists of two terms:\n Immediate reward $R(s)$ Get for starting in $s$ Expected sum of future discounted rewards Can we rewritten as $E_{s\u0026rsquo;\\sim P_{s\\pi(s)}}[V^\\pi(s\u0026rsquo;)]$ The expected sum of starting in $s\u0026rsquo;$ where $s\u0026rsquo;$ is distributed according to to $P_{s\\pi(s)}$  Can be used to efficiently solve for the value function $V^\\pi$\n Write down one equation for every state Gives a set of $|S|$ linear equations in $|S|$ variables  Optimal Value Function Best possible expected sum of discounted rewards which can be attained using any policy\n$$ V^*(s) = \\max_\\pi V^\\pi (s) $$\nWhich has it\u0026rsquo;s own version of the Bellman Equation\n$$ V^{\\ast}(s) = R(s) + \\max_{a\\in A} \\gamma \\sum_{s\u0026rsquo;\\in S}{P_{sa} (s\u0026rsquo;) V^{\\ast}(s\u0026rsquo;)} $$\nThe second term is a $\\max$ over all possible actions because that is the optimal reward.\nOptimal Policy $$ \\pi^{\\ast}(s) = \\arg \\max_{a\\in A} \\sum_{s\u0026rsquo;\\in S} P_{sa}(s\u0026rsquo;) V^*(s\u0026rsquo;) $$\nWhich gives the action that attains the $\\max$ in the optimal value function\nFor every state $s$ and policy $\\pi$ $$ V^\\ast(s) = V^{\\pi*}(s) \\geq V^\\pi(s) $$ Which says that\n The value function for the optimal policy is equal to the optimal value function for every state $s$\n  The value function for the optimal policy is greater than or equal to every other policy\n  $\\pi^*$ is the optimal policy for all states  Value Iteration and Policy Iteration Two efficient algorithms for solving finite-state MDPs with known transition probabilities\nValue Iteration  For each state s, initialize $V(s) := 0$ for until convergence do For every state, update $$ V(s) := R(s) + \\max_{a\\in A} \\gamma \\sum_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) $$\n  Repeatedly update estimated value function using the Bellman equation  Can be:\n Synchronous  Compute new values for all states and update at once Can be viewed as implementing a Bellman Backup Operator, which maps the current estimate to a new estimate   Asynchronous  Loop over states and update one at a time    Both algorithms will cause $V$ to converge to $V^*$\nPolicy Iteration  Initialise $\\pi$ randomly\nfor until convergence do\n​\tLet $V := V^\\pi$\n​\tFor each state $s$ $$ \\pi(s) := \\arg \\max_{a \\in A} \\sum_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) $$\n  Repeatedly computes value function for the current policy Update policy using current value function  This is the policy that is greedy w.r.t. $\\bold V$   The value function can be updated using Bellman equations After a finite number of iterations, $V$ will converge to $V^\\ast$ and $\\pi$ to $\\pi^\\ast$  Value vs Policy Iteration  No universal agreement which is better Small MDPs : Policy iteration tends to be fast Larger MDPs : Value iteration might be faster Policy iteration can reach the exact $V^*$ Value iteration will have small non-zero error  Learning a Model for an MDP   In many realistic problems, we do not know the state transition probabilities and rewards\n  Must be estimated from data\n  Given enough trials, can derive maximum likelihood estimates for the state transition probabilities $$ P_{sa}(s\u0026rsquo;) = \\frac{num. \\text{ times } a \\text{ led to } s\u0026rsquo; } {num. \\text{ times we took } a} $$ In the case of $\\frac{0}{0}$, can set $P$ to $\\frac{1}{|S|}$ (estimate as uniform)\nEfficient Update Keep count of numerator \u0026amp; denominator\nSolving Learned MDP  Can use policy/value iteration  Continuous State MDPs  Infinite number of states Consider settings where state space is $S = \\mathbb{R}^d$  Discretization  Simplest way Discretize state space Use value/policy iteration  Downsides\n Fairly naive representation for $V^*$ Assumes the value function is constant over each discretization interval Piecewise constant representation isn\u0026rsquo;t good for many smooth functions  Little smoothing over inputs No generalization over different grid cells   Suffers from curse of dimensionality  For $S = \\mathbb{R}^d$, we will have $k^d$ states    Rule of Thumb\n Works well for 1D/2D cases Can work for 3D/4D cases Rarely works above 6D  Value Function Approximation  Approximate $V^*$ directly  Using Model/Simulator  Assume we have a model/simulator for the MDP Any black-box with  Input : State $s_t$, Action $a_t$ Output : Next-state $s_{t+1}$ sampled    Getting the Model Using physics simulation  Can use off-the-shelf physics simulation  Data collected in the MDP  Can be done with:  Random actions Executing some policy Other way of choosing actions [?] Control law?   Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$  Example : Learn linear model\nusing linear regression $$ \\arg \\min_{A,B} \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\left\\Vert s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)}) \\right\\Vert^2_2 $$ Can build either a deterministic $$ s_{t+1} = As_t + Ba_t $$ or stochastic model $$ s_{t+1} = As_t + Ba_t + \\epsilon_t $$ with $\\epsilon_t$ being a noise term usually modelled as $\\epsilon_t \\sim \\mathcal{N}(0,\\Sigma)$\nor non-linear function $$ s_{t+1} = A\\phi_s(s_t) + B\\phi_a(a_t) $$ Eg: Locally weighted linear regression\nFitted Value Iteration  Algorithm for approximating the value function of a continuous state MDP Assume state space $S = \\mathcal{R}^d$ But action space $A$ is small and discrete  Perform value iteration update $$ \\begin{aligned} V(s) :\u0026amp;= R(s) + \\gamma \\max_a \\int_{s\u0026rsquo;} P_{sa}(s\u0026rsquo;) V(s\u0026rsquo;) ds\u0026rsquo; \\cr \u0026amp;= R(s) + \\gamma \\max_a E_{s\u0026rsquo; \\sim P_{sa}} [V(s\u0026rsquo;)] \\end{aligned} $$\n Main difference is the $\\int$ instead of $\\sum$ Approximately update over a finite sample of states $s^{(1)}, \\dots, s^{(n)}$ Use supervised learning algorithm, linear regression to approximate value function as linear/nonlinear function of the states  $$ V(s) = \\theta^T \\phi(s) $$\nAlgorithm\n Compute $y^{(i)}$, the approximation to $R(s) + \\gamma \\max_a E_{s\u0026rsquo; \\sim P_{sa}} [V(s\u0026rsquo;)]$ (RHS)\nUse supervised learning to get $V(s) = y^{(i)}$\nRepeat\n Convergence  Cannot be proved to converge But in practice it often does, and works well for many problems  Deterministic Simplification  Can set $k=1$ Because expectation of a deterministic distribution requires 1 sample  Policy  Outputs $V$, an approximation of $V^*$ Implicitly defines policy  When in state $s$, choose $$ \\arg \\max_a E_{s\u0026rsquo;\\sim P_{sa}} [V(s\u0026rsquo;)] $$\n Process for computing/approximating this is similar to the fitted value iteration algorithm  ","permalink":"https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/","summary":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$","title":"Reinforcement Learning"},{"content":"Supervised Learning Terminology  Input (Features) : $x^{(i)}$ Output (Target) : $y^{(i)}$ Training example : $(x^{(i)}, y^{(i)})$ Training set : Set of training examples Hypothesis : $h(x)$  Types  Regression : Continuous values Classification : Discrete values  (Linear) Regression First decide the hypothesis function\nWhere the convention is to let $x_0 = 1$ $$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 $$\n$$ h(x) = \\sum_{i=0}^d \\theta_i x_i = \\theta^T x $$\nObjective Learn the parameters $\\theta$ to best predict $y$\nCost Function Define $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} ( h_\\theta (x^{(i)} - y^{(i)}))^2 $$ This is the least-squares cost function that gives ordinary least squares regression\nLMS Algorithm Least mean squares, or Widrow-Hoff learning rule\n Start with an initial guess Repeatedly perturb $\\theta$ Hope we converge  Gradient Descent $$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n Learning Rate : $\\alpha$  Single training example case $(x,y)$\nReduces to $$ \\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta (x^{(i)})) x_j^{(i)} $$ The update is proportional to the error $$ \\propto (y - h_\\theta(x)) $$\nBatch Gradient Descent Group updates into vector $x$ $$ \\theta := \\theta + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)}- h_\\theta(x^{(i)}) \\right) x^{(i)} $$ Which is equivalent to gradient descent on the original cost function $J$\nStochastic Gradient Descent $$ \\theta := \\theta + \\alpha \\left( y^{(i)}- h_\\theta(x^{(i)}) \\right) x^{(i)} $$\nAnd repeatedly run through the training set\nBatch vs SGD    Batch SGD     Slower convergence Faster convergence   Converge to minimum Oscillate about minimum    In most cases being close to the minimum is good enough, and therefore people choose SGD for the faster convergence\nThe Normal Equations Minimise explicitly by taking the derivatives w.r.t $J$ and setting to zero\nMatrix Derivatives For function $f : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}$\n$\\nabla_A f(A)$ is the Jacobian\nLeast-Squares Set $\\nabla_\\theta J(\\theta) = 0$\n$$ \\begin{aligned} \\nabla_\\theta J(\\theta) \u0026amp;= X^TX \\theta - X^T y \\cr X^TX \\theta \u0026amp;= X^T y \\end{aligned} $$ Which gives $$ \\theta = (X^TX)^{-1} X^T y $$\nProbabilistic Interpretation Assume target and inputs are related via $$ y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)} $$ where $\\epsilon^{(i)}$ are unmodelled effects/noise\nAssume $\\epsilon^{(i)}$ are Gaussian IID $$ p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( - \\frac{(\\epsilon^{(i)})^2}{2\\sigma^2} \\right) $$ which implies $$ p(y^{(i)} | x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( - \\frac{ (y^{(i)} - \\theta^T x^{(i)})^2 } { 2\\sigma^2 } \\right) $$ or $$ y^{(i)} | x^{(i)}; \\theta \\sim \\mathcal{N}(\\theta^T x ^{(i)}, \\sigma^2) $$\nLikelihood Function Given $X$ and $\\theta$, what is the distribution of $y^{(i)}$s? $$ L(\\theta) = L(\\theta; X, \\vec{y}) = p(\\vec{y}|X;\\theta) $$ Can be written as $$ \\begin{aligned} L(\\theta) \u0026amp;= \\prod_{i=1}^{n} p(y^{(i)} | x^{(i)}; \\theta) \\cr \u0026amp;= \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( - \\frac{ (y^{(i)} - \\theta^T x^{(i)})^2 } { 2\\sigma^2 } \\right) \\end{aligned} $$ The Principle of maximum likelihood says that we should choose $\\theta$ to make the probability as high as possible.\nMaximising probability is equivalent to maximising log likelihood, $l(\\theta)$. $$ l(\\theta) = n \\log \\frac{1}{\\sqrt{2\\pi\\sigma}} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} ( y^{(i)} - \\theta^T x^{(i)} )^2 $$ Which is the same as minimizing the least-squares costs $$ J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} ( y^{(i)} - \\theta^T x^{(i)} )^2 $$ [*] This result does not depend on $\\sigma$\nLocally Weighted Linear Regression (LWR) Assuming sufficient training data, makes choice of features less important\nProcedure\n Fit $\\theta$ to minimise $\\sum_i w^{(i)} ( y^{(i)} - \\theta^T x^{(i)} )^2$ Output $\\theta^T x$  [*] Only difference is the weight $w^{(i)}$ $$ w^{(i)} = \\exp \\left( -\\frac{(x^{(i)} - x)^2}{2\\tau^2} \\right) $$ Which biases around the query point\nBandwidth : $\\tau$, controls the falloff distance\nThis is an example of a non-parametric Algorithm\n Requires data even after fitting Non-parametric means hypothesis $h$ grows linearly with size of training set  Classification Similar to regression, except $y$ only takes on a small number of discrete values\nLogistic Regression Start by ignoring the fact that $y$ is discrete\nUse linear regression with modified hypothesis function $$ h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}} $$ where $g(z) = \\frac{1}{1 + e^{-z}}$ is the logistic/sigmoid function\n$g(z)$ tends to $0$ or $1$ as $z \\rightarrow -\\infty$ or $z \\rightarrow \\infty$\nOther Functions\nOther functions can be used\nBut sigmoid has a useful function that the derivative: $$ g\u0026rsquo; = g(1 - g) $$\nProbabilistic Interpretation Assume $$ \\begin{aligned} P(y=1 | x; \\theta) \u0026amp;= h_\\theta(x) \\cr P(y=0 | x; \\theta) \u0026amp;= 1 - h_\\theta(x) \\end{aligned} $$ or equivalently $$ P(y | x; \\theta) = [h_\\theta(x)]^y [1-h_\\theta(x)]^{1-y} $$\n","permalink":"https://andrelimzs.github.io/posts/machine-learning/supervised-learning/","summary":"Supervised Learning Terminology  Input (Features) : $x^{(i)}$ Output (Target) : $y^{(i)}$ Training example : $(x^{(i)}, y^{(i)})$ Training set : Set of training examples Hypothesis : $h(x)$  Types  Regression : Continuous values Classification : Discrete values  (Linear) Regression First decide the hypothesis function\nWhere the convention is to let $x_0 = 1$ $$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 $$\n$$ h(x) = \\sum_{i=0}^d \\theta_i x_i = \\theta^T x $$","title":"Supervised Learning"}]