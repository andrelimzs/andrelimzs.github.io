<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta name=robots content="index, follow"><title>Growth of Functions | AndreLimZS</title><meta name=keywords content><meta name=description content="Objective Order of growth is a simple characterization of an algorithm&rsquo;s efficiency and allows for comparison between algorithms.
Asymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.
 Asymptotic Notation Notation is defined as a function on domain $\mathbb{N} = { 0,1,2,\dots }$, the set of natural numbers."><meta name=author content><link rel=canonical href=https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://andrelimzs.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrelimzs.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrelimzs.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrelimzs.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrelimzs.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.96.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Growth of Functions"><meta property="og:description" content="Objective Order of growth is a simple characterization of an algorithm&rsquo;s efficiency and allows for comparison between algorithms.
Asymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.
 Asymptotic Notation Notation is defined as a function on domain $\mathbb{N} = { 0,1,2,\dots }$, the set of natural numbers."><meta property="og:type" content="article"><meta property="og:url" content="https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-17T23:05:22+08:00"><meta property="article:modified_time" content="2022-01-17T23:05:22+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Growth of Functions"><meta name=twitter:description content="Objective Order of growth is a simple characterization of an algorithm&rsquo;s efficiency and allows for comparison between algorithms.
Asymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.
 Asymptotic Notation Notation is defined as a function on domain $\mathbb{N} = { 0,1,2,\dots }$, the set of natural numbers."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://andrelimzs.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Growth of Functions","item":"https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Growth of Functions","name":"Growth of Functions","description":"Objective Order of growth is a simple characterization of an algorithm\u0026rsquo;s efficiency and allows for comparison between algorithms.\nAsymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.\n Asymptotic Notation Notation is defined as a function on domain $\\mathbb{N} = { 0,1,2,\\dots }$, the set of natural numbers.","keywords":[],"articleBody":"Objective Order of growth is a simple characterization of an algorithmâ€™s efficiency and allows for comparison between algorithms.\nAsymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.\n Asymptotic Notation Notation is defined as a function on domain $\\mathbb{N} = { 0,1,2,\\dots }$, the set of natural numbers. It is mainly used to characterise running time, but can also be used on memory/space/etc.\n$\\Theta$-notation  Encloses the function from above and below\n Useful in analyzing the average-case complexity of an algorithm. $$ \\begin{aligned} \\Theta \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c_1,\\ c_2,\\ n_0 \\text{ such that } \\\\ 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ $\\Theta(n)$ is a set, but it is usually written as $f(n) = \\Theta(n)$.\n $f(n)$ belongs to set $\\Theta(g(n))$ if $c_1 \\cdot g(n)$ and $c_2 \\cdot g(n)$ can sandwidch the function, for large $n \\geq n_0$.\nThe function $f(n)$ is equal to $g(n)$ to within a constant factor, which makes $g(n)$ an asymptotically tight bound for $f(n)$.\nThe definition of $\\Theta(n)$ requires all $f(n) \\in \\Theta(g(n))$ be asymptotically nonnegative ($f(n) \\geq 0$ as $n \\rightarrow \\infty$. This implies that $g(n)$ is also asymptotically nonnegative so that $\\Theta(g(n))$ is not empty. Therefore assume that every function in $\\Theta$-notation is asymptotically nonnegative.\n Example Show that $ \\frac{3}{4} n^2 - 5n + \\frac{3}{n} = \\Theta(n^2) $\n$c_1 n^2 \\leq \\frac{3}{4} n^2 - 5n + \\frac{3}{n} \\leq c_2 n^2$ for all $n \\geq n_0$\n$c_1 \\leq \\frac{3}{4} - \\frac{5}{n} + \\frac{3}{n^2} \\leq c_2$\n$c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 10$ satisfy the inequality\n  Example Show that $5n^3 \\neq \\Theta(n^2)$\n$c_1 \\leq 5 \\frac{n^3}{n^2} \\leq c_2$\nWhich cannot hold for arbitrarily large $n$\n This results in two observations:\n The lower-order terms can be ignored (They become of the form $1 / n^k$ which go to $0$) Coefficients can be ignored (Can be divided into $c_1, c_2$)  Constants are degree-0 polynomials, which can be expressed as $\\Theta(n^0)$ or more commonly $\\Theta(1)$.\n$O$-notation  Asymptotic upper bound\n Provides an upper bound on a function to within a constant factor. $$ \\begin{aligned} O \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c,\\ n_0 \\text{ such that } \\\\ 0 \\leq f(n) \\leq c g(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$ $\\Theta(g(n))$ implies $O(g(n))$ becauses $\\Theta$-notation is stronger than $O$-notation. $$ \\Theta(g(n)) \\subseteq O(g(n)) $$ $O$-notation provides an asymptotic upper bound but not a tight one. Distinguishing between both is stanard in algorithms literature, but not in informal use.\nInspection can often reveal the $O$-notation running time. For example, double nested loops are $O(n^2)$.\nWhen $O$-notation is used to bound the worst-case running time, it also bounds every input. This is different from $\\Theta$-notation, where the worse-case does not imply the running time for every input.\n$\\Omega$-notation  Asymptotic lower bound\n $$ \\begin{aligned} O \\big( g(n) \\big) = \\{ f(n) :\\ \\text{there exists } c,\\ n_0 \\text{ such that } \\\\ 0 \\leq c g(n) \\leq f(n) \\enspace \\forall n \\geq n_0 \\} \\end{aligned} $$\nTheorem 3.1 For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n)) and f(n) = \\Omega(g(n))$\nCommonly used to get asymptotic upper/lower bounds ( $O$ / $\\Omega$ ) from tight bounds ( $\\Theta$ )\n Example\n$ \\Omega(n^2) = an^2 + bn + c$ and $O(n^2) = an^2 + bn + c$\nimply that $ \\Theta(n^2) = an^2 + bn + c $\n Asymptotic Notation in Equations   $n = O(n^2)$ Refers to set membership $n \\in O(n^2)$\n  In a formula ( $\\dots = 2n^2 + \\Theta(n)$ ) Stand-in for some anonymous function ( $\\Theta(n) = f(n)$ )\n  The number of anonymous functions is equal to the number of times it appears Eg: $\\sum_i^n{ O(i) }$ appears once and not $O(1) + \\dots + O(n)$\n  On the left-hand side (LHS) (It usually appears on the RHS) Means that there is always a way to choose LHS and RHS to make the equation hold The RHS is a coarser level of detail than the LHS\n  Can be chained Each equation can be interpreted separately\n  o-notation $o$-notation is a bound that is not asymptotically tight. ($O$-notation may or may not be asymptotically tight.) $$ \\begin{aligned} o \\big( g(n) \\big) = \\{ f(n) :\\ \\text{for any +ve constant } c0, \\text{ there exists } \\\\ \\text{a constant } n_0  0 \\text{ s.t. } 0 \\leq f(n) 0$ instead of some constant.\n$\\omega$-notation Lower bound that is not asymptotically tight. $$ \\begin{aligned} o \\big( g(n) \\big) = \\{ f(n) :\\ \\text{for any +ve constant } c0, \\text{ there exists } \\\\ \\text{a constant } n_0  0 \\text{ s.t. } 0 \\leq c g(n) Comparing Functions Transitivity $f(n) = \\Theta / O / \\Omega (g(n))$ and $g(n) = \\Theta / O / \\Omega (h(n))$\nImplies $f(n) = \\Theta / O / \\Omega (h(n))$\nReflexivity $f(n) = \\Theta / O / \\Omega (f(n))$\nSymmetry $f(n) = \\Theta(g(n)) \\Leftrightarrow g(n) = \\Theta(f(n))$\nTranspose Symmetry $f(n) = O(g(n)) \\Leftrightarrow g(n) = \\Omega(f(n))$\n$f(n) = o(g(n)) \\Leftrightarrow g(n) = \\omega(f(n))$\nThese properties allow for an analogy between asymptotic notation and inequalities.\n$f(n) = O(g(n)) \\quad\\leftrightarrow\\quad a \\leq b \\quad | \\quad f(n) = o(g(n)) \\quad\\leftrightarrow\\quad a $f(n) = \\Omega(g(n)) \\quad\\leftrightarrow\\quad a \\geq b \\quad | \\quad f(n) = \\omega(g(n)) \\quad\\leftrightarrow\\quad a  b$\n$f(n) = \\Theta(g(n)) \\quad\\leftrightarrow\\quad a = b$\n Standard Notations Monotonicity $f(n)$ is monotonically increasing if $m \\leq n$ implies $f(m) \\leq f(n)$.\n$f(n)$ is strictly increasing if $mFloors and Ceilings   Floor $\\lfloor x \\rfloor$ : The least integer $\\leq x$\n  Ceiling $\\lceil x \\rceil$ : The least integer $\\geq x$\n  Both are monotonically increasing\nProperties (for real $x$):\n  $x-1  $\\lceil n/2 \\rceil + \\lfloor n/2 \\rfloor = n$\n  $ \\lceil \\frac{1}{b} \\lceil \\frac{x}{a} \\rceil \\rceil = \\lceil \\frac{x}{ab} \\rceil$\n  $\\lceil \\frac{a}{b} \\rceil = \\frac{a + b - 1}{b}$\n  Modular Arithmetic For integer $a$ and positive integer $n$, $a \\text{ mod } n$ is the remainder of $a/n$\n$a \\mod n = a - n \\lfloor a/n \\rfloor$\nAnd therefore $0 \\leq a \\mod n \\leq n$\nEquivalent : $a \\equiv b\\ (\\text{mod } n )$ if they have the same remainder.\nThis also implies that $n$ is a divisor of $a-b$\nPolynomials $$ p(n) = \\sum^d_{i=0} {a_i n^i} $$\nFor integer $d \\geq 0$\nCoefficients: $a_i$ and $a_d \\neq 0$\nAsymptotically positive $ \\Leftrightarrow a_d 0$\n","wordCount":"1152","inLanguage":"en","datePublished":"2022-01-17T23:05:22+08:00","dateModified":"2022-01-17T23:05:22+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/"},"publisher":{"@type":"Organization","name":"AndreLimZS","logo":{"@type":"ImageObject","url":"https://andrelimzs.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrelimzs.github.io accesskey=h title="AndreLimZS (Alt + H)">AndreLimZS</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Growth of Functions</h1><div class=post-meta><span title="2022-01-17 23:05:22 +0800 +0800">January 17, 2022</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#objective aria-label=Objective>Objective</a><ul><li><a href=#asymptotic-efficiency aria-label="Asymptotic Efficiency">Asymptotic Efficiency</a></li></ul></li><li><a href=#asymptotic-notation aria-label="Asymptotic Notation">Asymptotic Notation</a><ul><li><a href=#theta-notation aria-label=$\Theta$-notation>$\Theta$-notation</a></li><li><a href=#o-notation aria-label=$O$-notation>$O$-notation</a></li><li><a href=#omega-notation aria-label=$\Omega$-notation>$\Omega$-notation</a></li><li><a href=#theorem-31 aria-label="Theorem 3.1">Theorem 3.1</a></li><li><a href=#asymptotic-notation-in-equations aria-label="Asymptotic Notation in Equations">Asymptotic Notation in Equations</a></li><li><a href=#o-notation-1 aria-label=o-notation>o-notation</a></li><li><a href=#omega-notation-1 aria-label=$\omega$-notation>$\omega$-notation</a></li></ul></li><li><a href=#comparing-functions aria-label="Comparing Functions">Comparing Functions</a><ul><li><a href=#transitivity aria-label=Transitivity>Transitivity</a></li><li><a href=#reflexivity aria-label=Reflexivity>Reflexivity</a></li><li><a href=#symmetry aria-label=Symmetry>Symmetry</a></li><li><a href=#transpose-symmetry aria-label="Transpose Symmetry">Transpose Symmetry</a></li></ul></li><li><a href=#standard-notations aria-label="Standard Notations">Standard Notations</a><ul><li><a href=#monotonicity aria-label=Monotonicity>Monotonicity</a></li><li><a href=#floors-and-ceilings aria-label="Floors and Ceilings">Floors and Ceilings</a></li><li><a href=#modular-arithmetic aria-label="Modular Arithmetic">Modular Arithmetic</a></li><li><a href=#polynomials aria-label=Polynomials>Polynomials</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=objective>Objective<a hidden class=anchor aria-hidden=true href=#objective>#</a></h2><p>Order of growth is a simple characterization of an algorithm&rsquo;s efficiency and allows for comparison between algorithms.</p><h3 id=asymptotic-efficiency>Asymptotic Efficiency<a hidden class=anchor aria-hidden=true href=#asymptotic-efficiency>#</a></h3><p>When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time <em>in the limit</em> as input increases without bound.</p><hr><h2 id=asymptotic-notation>Asymptotic Notation<a hidden class=anchor aria-hidden=true href=#asymptotic-notation>#</a></h2><p>Notation is defined as a function on domain $\mathbb{N} = { 0,1,2,\dots }$, the set of natural numbers. It is mainly used to characterise running time, but can also be used on memory/space/etc.</p><h3 id=theta-notation>$\Theta$-notation<a hidden class=anchor aria-hidden=true href=#theta-notation>#</a></h3><blockquote><p>Encloses the function from above and below</p></blockquote><p>Useful in analyzing the average-case complexity of an algorithm.
$$
\begin{aligned}
\Theta \big( g(n) \big)
= \{ f(n) :\ \text{there exists } c_1,\ c_2,\ n_0 \text{ such that } \\
0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \enspace \forall n \geq n_0
\}
\end{aligned}
$$
$\Theta(n)$ is a set, but it is usually written as $f(n) = \Theta(n)$.</p><figure class=align-center><img loading=lazy src=theta-notation.JPG#center width=360></figure><p>$f(n)$ belongs to set $\Theta(g(n))$ if $c_1 \cdot g(n)$ and $c_2 \cdot g(n)$ can sandwidch the function, for large $n \geq n_0$.</p><p>The function $f(n)$ is equal to $g(n)$ to within a constant factor, which makes $g(n)$ an <strong>asymptotically tight bound</strong> for $f(n)$.</p><p>The definition of $\Theta(n)$ requires all $f(n) \in \Theta(g(n))$ be <strong>asymptotically nonnegative</strong> ($f(n) \geq 0$ as $n \rightarrow \infty$. This implies that $g(n)$ is also asymptotically nonnegative so that $\Theta(g(n))$ is not empty. Therefore assume that every function in $\Theta$-notation is asymptotically nonnegative.</p><blockquote><p><strong>Example</strong> Show that $ \frac{3}{4} n^2 - 5n + \frac{3}{n} = \Theta(n^2) $</p><p>$c_1 n^2 \leq \frac{3}{4} n^2 - 5n + \frac{3}{n} \leq c_2 n^2$ for all $n \geq n_0$</p><p>$c_1 \leq \frac{3}{4} - \frac{5}{n} + \frac{3}{n^2} \leq c_2$</p><p>$c_1 = \frac{1}{2}$, $c_2 = 1$ and $n_0 = 10$ satisfy the inequality</p></blockquote><blockquote><p><strong>Example</strong> Show that $5n^3 \neq \Theta(n^2)$</p><p>$c_1 \leq 5 \frac{n^3}{n^2} \leq c_2$</p><p>Which cannot hold for arbitrarily large $n$</p></blockquote><p>This results in two observations:</p><ul><li>The <strong>lower-order terms</strong> can be ignored
(They become of the form $1 / n^k$ which go to $0$)</li><li>Coefficients can be ignored
(Can be divided into $c_1, c_2$)</li></ul><p><strong>Constants</strong> are degree-0 polynomials, which can be expressed as $\Theta(n^0)$ or more commonly $\Theta(1)$.</p><h3 id=o-notation>$O$-notation<a hidden class=anchor aria-hidden=true href=#o-notation>#</a></h3><blockquote><p>Asymptotic upper bound</p></blockquote><p>Provides an upper bound on a function to within a constant factor.
$$
\begin{aligned}
O \big( g(n) \big)
= \{ f(n) :\ \text{there exists } c,\ n_0 \text{ such that } \\
0 \leq f(n) \leq c g(n) \enspace \forall n \geq n_0
\}
\end{aligned}
$$
$\Theta(g(n))$ implies $O(g(n))$ becauses $\Theta$-notation is stronger than $O$-notation.
$$
\Theta(g(n)) \subseteq O(g(n))
$$
$O$-notation provides an asymptotic upper bound but not a <strong>tight</strong> one. Distinguishing between both is stanard in algorithms literature, but not in informal use.</p><p>Inspection can often reveal the $O$-notation running time. For example, double nested loops are $O(n^2)$.</p><p>When $O$-notation is used to bound the worst-case running time, it also bounds <em>every input</em>. This is different from $\Theta$-notation, where the worse-case does not imply the running time for <em>every</em> input.</p><h3 id=omega-notation>$\Omega$-notation<a hidden class=anchor aria-hidden=true href=#omega-notation>#</a></h3><blockquote><p>Asymptotic lower bound</p></blockquote><p>$$
\begin{aligned}
O \big( g(n) \big)
= \{ f(n) :\ \text{there exists } c,\ n_0 \text{ such that } \\
0 \leq c g(n) \leq f(n) \enspace \forall n \geq n_0
\}
\end{aligned}
$$</p><h3 id=theorem-31>Theorem 3.1<a hidden class=anchor aria-hidden=true href=#theorem-31>#</a></h3><p>For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n)) and f(n) = \Omega(g(n))$</p><p>Commonly used to get asymptotic upper/lower bounds ( $O$ / $\Omega$ ) from tight bounds ( $\Theta$ )</p><blockquote><p><strong>Example</strong></p><p>$ \Omega(n^2) = an^2 + bn + c$ and $O(n^2) = an^2 + bn + c$</p><p>imply that $ \Theta(n^2) = an^2 + bn + c $</p></blockquote><h3 id=asymptotic-notation-in-equations>Asymptotic Notation in Equations<a hidden class=anchor aria-hidden=true href=#asymptotic-notation-in-equations>#</a></h3><ul><li><p>$n = O(n^2)$<br>Refers to <strong>set membership</strong> $n \in O(n^2)$</p></li><li><p>In a formula ( $\dots = 2n^2 + \Theta(n)$ )<br>Stand-in for <strong>some anonymous function</strong>
( $\Theta(n) = f(n)$ )</p></li><li><p>The number of anonymous functions is equal to the number of times it appears<br>Eg: $\sum_i^n{ O(i) }$ appears once and not $O(1) + \dots + O(n)$</p></li><li><p>On the left-hand side (LHS) (It usually appears on the RHS)<br>Means that there is always a way to choose LHS and RHS to make the equation hold<br>The RHS is a <strong>coarser</strong> level of detail than the LHS</p></li><li><p>Can be <strong>chained</strong><br>Each equation can be interpreted separately</p></li></ul><h3 id=o-notation-1>o-notation<a hidden class=anchor aria-hidden=true href=#o-notation-1>#</a></h3><p>$o$-notation is a bound that is <strong>not</strong> asymptotically tight. ($O$-notation may or may not be asymptotically tight.)
$$
\begin{aligned}
o \big( g(n) \big)
= \{ f(n) :\ \text{for any +ve constant } c>0, \text{ there exists } \\
\text{a constant } n_0 > 0 \text{ s.t. } 0 \leq f(n) &lt; c g(n) \enspace \forall n \geq n_0
\}
\end{aligned}
$$
The main difference between $o$-notation and $O$-notation is that the bound $0 \leq f(n) \leq cg(n)$ holds for all constants $c > 0$ instead of <strong>some</strong> constant.</p><h3 id=omega-notation-1>$\omega$-notation<a hidden class=anchor aria-hidden=true href=#omega-notation-1>#</a></h3><p>Lower bound that is <strong>not</strong> asymptotically tight.
$$
\begin{aligned}
o \big( g(n) \big)
= \{ f(n) :\ \text{for any +ve constant } c>0, \text{ there exists } \\
\text{a constant } n_0 > 0 \text{ s.t. } 0 \leq c g(n) &lt; f(n) \enspace \forall n \geq n_0
\}
\end{aligned}
$$</p><h2 id=comparing-functions>Comparing Functions<a hidden class=anchor aria-hidden=true href=#comparing-functions>#</a></h2><h3 id=transitivity>Transitivity<a hidden class=anchor aria-hidden=true href=#transitivity>#</a></h3><p>$f(n) = \Theta / O / \Omega (g(n))$ and $g(n) = \Theta / O / \Omega (h(n))$</p><p>Implies $f(n) = \Theta / O / \Omega (h(n))$</p><h3 id=reflexivity>Reflexivity<a hidden class=anchor aria-hidden=true href=#reflexivity>#</a></h3><p>$f(n) = \Theta / O / \Omega (f(n))$</p><h3 id=symmetry>Symmetry<a hidden class=anchor aria-hidden=true href=#symmetry>#</a></h3><p>$f(n) = \Theta(g(n)) \Leftrightarrow g(n) = \Theta(f(n))$</p><h3 id=transpose-symmetry>Transpose Symmetry<a hidden class=anchor aria-hidden=true href=#transpose-symmetry>#</a></h3><p>$f(n) = O(g(n)) \Leftrightarrow g(n) = \Omega(f(n))$</p><p>$f(n) = o(g(n)) \Leftrightarrow g(n) = \omega(f(n))$</p><p>These properties allow for an analogy between asymptotic notation and inequalities.</p><p>$f(n) = O(g(n)) \quad\leftrightarrow\quad a \leq b \quad | \quad f(n) = o(g(n)) \quad\leftrightarrow\quad a &lt; b$</p><p>$f(n) = \Omega(g(n)) \quad\leftrightarrow\quad a \geq b \quad | \quad f(n) = \omega(g(n)) \quad\leftrightarrow\quad a > b$</p><p>$f(n) = \Theta(g(n)) \quad\leftrightarrow\quad a = b$</p><hr><h2 id=standard-notations>Standard Notations<a hidden class=anchor aria-hidden=true href=#standard-notations>#</a></h2><h3 id=monotonicity>Monotonicity<a hidden class=anchor aria-hidden=true href=#monotonicity>#</a></h3><p>$f(n)$ is <strong>monotonically increasing</strong> if $m \leq n$ implies $f(m) \leq f(n)$.</p><p>$f(n)$ is <strong>strictly increasing</strong> if $m&lt;n$ implies $f(m) &lt; f(n)$</p><h3 id=floors-and-ceilings>Floors and Ceilings<a hidden class=anchor aria-hidden=true href=#floors-and-ceilings>#</a></h3><ul><li><p>Floor $\lfloor x \rfloor$ : The least integer $\leq x$</p></li><li><p>Ceiling $\lceil x \rceil$ : The least integer $\geq x$</p></li></ul><p>Both are <strong>monotonically increasing</strong></p><p>Properties (for real $x$):</p><ul><li><p>$x-1 &lt; \lfloor x \rfloor \leq x \leq \lceil x \rceil &lt; x+1$</p></li><li><p>$\lceil n/2 \rceil + \lfloor n/2 \rfloor = n$</p></li><li><p>$ \lceil \frac{1}{b} \lceil \frac{x}{a} \rceil \rceil = \lceil \frac{x}{ab} \rceil$</p></li><li><p>$\lceil \frac{a}{b} \rceil = \frac{a + b - 1}{b}$</p></li></ul><h3 id=modular-arithmetic>Modular Arithmetic<a hidden class=anchor aria-hidden=true href=#modular-arithmetic>#</a></h3><p>For integer $a$ and positive integer $n$, $a \text{ mod } n$ is the remainder of $a/n$</p><p>$a \mod n = a - n \lfloor a/n \rfloor$</p><p>And therefore $0 \leq a \mod n \leq n$</p><p><strong>Equivalent</strong> : $a \equiv b\ (\text{mod } n )$ if they have the same remainder.</p><p>This also implies that $n$ is a divisor of $a-b$</p><h3 id=polynomials>Polynomials<a hidden class=anchor aria-hidden=true href=#polynomials>#</a></h3><p>$$
p(n) = \sum^d_{i=0} {a_i n^i}
$$</p><p>For integer $d \geq 0$</p><p>Coefficients: $a_i$ and $a_d \neq 0$</p><p>Asymptotically positive $ \Leftrightarrow a_d >0$</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://andrelimzs.github.io>AndreLimZS</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>