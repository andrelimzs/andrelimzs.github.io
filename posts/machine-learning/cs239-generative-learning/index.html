<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta name=robots content="index, follow"><title>CS229 Generative Learning | AndreLimZS</title><meta name=keywords content><meta name=description content="Generative Learning Different approach to learning
Try to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly
 $p(x|y)$ : Distribution of the target&rsquo;s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn&rsquo;t matter $$ \arg \max_y P(y|x) = \arg \max_y p(x|y) p(y) $$
Multivariate Normal Distribution  In $d$-dimensions  Parameterised by:"><meta name=author content><link rel=canonical href=https://andrelimzs.github.io/posts/machine-learning/cs239-generative-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://andrelimzs.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrelimzs.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrelimzs.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrelimzs.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrelimzs.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.97.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="CS229 Generative Learning"><meta property="og:description" content="Generative Learning Different approach to learning
Try to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly
 $p(x|y)$ : Distribution of the target&rsquo;s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn&rsquo;t matter $$ \arg \max_y P(y|x) = \arg \max_y p(x|y) p(y) $$
Multivariate Normal Distribution  In $d$-dimensions  Parameterised by:"><meta property="og:type" content="article"><meta property="og:url" content="https://andrelimzs.github.io/posts/machine-learning/cs239-generative-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-07T22:00:00+08:00"><meta property="article:modified_time" content="2022-04-07T22:00:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CS229 Generative Learning"><meta name=twitter:description content="Generative Learning Different approach to learning
Try to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly
 $p(x|y)$ : Distribution of the target&rsquo;s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn&rsquo;t matter $$ \arg \max_y P(y|x) = \arg \max_y p(x|y) p(y) $$
Multivariate Normal Distribution  In $d$-dimensions  Parameterised by:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://andrelimzs.github.io/posts/"},{"@type":"ListItem","position":3,"name":"CS229 Generative Learning","item":"https://andrelimzs.github.io/posts/machine-learning/cs239-generative-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CS229 Generative Learning","name":"CS229 Generative Learning","description":"Generative Learning Different approach to learning\nTry to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly\n $p(x|y)$ : Distribution of the target\u0026rsquo;s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \\frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn\u0026rsquo;t matter $$ \\arg \\max_y P(y|x) = \\arg \\max_y p(x|y) p(y) $$\nMultivariate Normal Distribution  In $d$-dimensions  Parameterised by:","keywords":[],"articleBody":"Generative Learning Different approach to learning\nTry to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly\n $p(x|y)$ : Distribution of the target’s features $p(y)$ : Class priors  Use Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \\frac{p(x|y) p(y)}{p(x)} $$ Can be simplified when using for prediction because denominator doesn’t matter $$ \\arg \\max_y P(y|x) = \\arg \\max_y p(x|y) p(y) $$\nMultivariate Normal Distribution  In $d$-dimensions  Parameterised by:\n Mean Vector : $\\mu \\in \\mathbb{R}^d$ Covariance Matrix : $\\Sigma \\in \\mathbb{R}^{d\\times d}$  ​\t$\\Sigma$ symmetric and PSD $$ p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right) $$ Where $|\\Sigma|$ is the determinant of $\\Sigma$\nMean Is given by $\\mu$ $$ \\mathbb{E}[X] = \\int_x{ x p(x; \\mu,\\Sigma) dx } = \\mu $$\nCovariance Is defined as $$ Cov(X) = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^T] $$\n$$ Cov(X) = \\mathbb{E}[XX^T] - (\\mathbb{E}[X])(\\mathbb{E}[X])^T $$\nAs cov increases, the distribution becomes more spread out\nOff-diagonal terms skew the distribution\nStandard Normal Distribution Gaussian with zero mean and identity covariance\nGaussian Discriminant Analysis Assume that $p(x|y)$ is distributed according to a multivariate normal distribution $$ \\begin{aligned} y \u0026\\sim \\text{Bernoulli}(\\phi) \\cr x|y=0 \u0026\\sim \\mathcal{N}(\\mu_0, \\Sigma) \\cr x|y=1 \u0026\\sim \\mathcal{N}(\\mu_1, \\Sigma) \\end{aligned} $$ Which expands to $$ \\begin{aligned} p(y) \u0026\\sim \\phi^y (1-\\phi)^{1-y} \\cr p(x|y=0) \u0026\\sim \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0) \\right) \\cr p(x|y=1) \u0026\\sim \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1) \\right) \\cr \\end{aligned} $$ This model usually combines $\\Sigma_0$/$\\Sigma_1$ into $\\Sigma$\nThe log-likelihood is $$ \\begin{aligned} l(\\phi, \\mu_0, \\mu_1, \\Sigma) \u0026= log \\prod_{i=1}^{n} p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma) \\cr \u0026= \\log \\prod_{i=1}^{n} p(x|y; \\mu, \\mu, \\Sigma)\\ p(y;\\phi) \\end{aligned} $$ Find the maximum likelihood estimates of the parameters by maximising $l$ w.r.t the parameters $$ \\phi = \\frac{1}{n} \\sum_{i=1}^{n} 1{ y^{(i)}=1 } $$\n$$ \\mu_0 = \\frac{\\sum_{i=1}^n 1{ y^{(i)}=0 } x^{(i)}} {\\sum_{i=1}^n 1{ y^{(i)}=0 }} $$\n$$ \\mu_1 = \\frac{\\sum_{i=1}^n 1{ y^{(i)}=1 } x^{(i)}} {\\sum_{i=1}^n 1{ y^{(i)}=1 }} $$\n$$ \\Sigma = \\frac{1}{n} \\sum_{i=1}^{n} (x^{(i)} - \\mu_{y^{(i)}}) (x^{(i)} - \\mu_{y^{(i)}})^T $$\nGraphically, the algorithm is learning the two gaussian distributions\n The decision boundary is separating hyperplane between the two distributions\nGDA and Logistic Regression Expressing $y$ as a function of $x$ $$ p(y=1|x; \\phi, \\Sigma, \\mu_0, \\mu_1) = \\frac{1}{1 + \\exp(-\\theta^Tx)} $$ Has the exact same form as logistic regression\nBut GDA and logistic regression will generally give different decision boundaries\nThis is because\n Multivariate Gaussian $\\implies$ Logistic function Logistic function $\\centernot\\implies$ Multivariate Gaussian  GDA makes stronger modelling assumptions than logistic regression\n If assumptions are correct, GDA will find better fits If $p(x|y)$ is gaussian with shared $\\Sigma$, GDA is asymptotically efficient  As $n \\rightarrow \\infty$, no algorithm is strictly better than GDA    Comparison    GDA Logistic     Stronger modelling assumptions Less sensitive to modelling errors   More data efficient More robust    [*] In practise logistic regression is used more often than GDA\nNaive Bayes Similar to GDA but with discrete-valued $x$\n  Encode set of features into a vocabulary. Dimension of $x$ equals size of vocabulary\n  But the number of parameters grows with\n  $$ \\large 2^\\text{size of vocabulary} $$\n and quickly becomes too large for an explicit representation  Naive Bayes Assumption  Assume that the $x$’s are conditially independent given $y$\n [Example]\nIf an email is spam ($y=1$) and word $x_1$ is apple and word $x_2$ is car\n Knowing that $y=1$ and $x_1 = apple$ Gives no information about $x_2 = car$  $$ p(x_2 | y) = p(x2 | y, x_1) $$\n[*] This does not say that $x_1$ and $x_2$ are independent $$ \\begin{aligned} p(\u0026x_1, \\dots, x_{5000} | y) \\cr \u0026= p(x_1|y) p(x1|y,x_1) \\dots p(x_5000|y,x_1\\dots x_{4999}) \\cr \\end{aligned} $$ Using the naive bayes assumption $$ \\begin{aligned} \u0026= p(x_1|y) p(x1|y) \\dots p(x_5000|y) \\cr \u0026= \\prod_{j=1}^d p(x_j | y) \\end{aligned} $$ The NB assumption is extremely strong\nBut the algorithm works well on many problems\nParameterisation The joint likelihood is $$ \\mathcal{L}(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^{n} p(x^{(i)}, y^{(i)}) $$ Maximise w.r.t $\\phi_y$, $\\phi_{j|y=0}$ and $\\phi_{j|y=1}$ $$ \\begin{aligned} \\phi_{j|y=1} \u0026= \\frac{\\sum_{i=1}^n 1{ x_k^{(i)}=1 \\wedge y^{(i)}=1 }} {\\sum_{i=1}^n 1{ y^{(i)}=1 }} \\cr \\phi_{j|y=0} \u0026= \\frac{\\sum_{i=1}^n 1{ x_k^{(i)}=1 \\wedge y^{(i)}=0 }} {\\sum_{i=1}^n 1{ y^{(i)}=0 }} \\cr \\phi_{y} \u0026= \\frac{\\sum_{i=1}^n 1{ y^{(i)}=1 }}{n} \\end{aligned} $$ To make a prediction, find the highest possiblity from $$ p(y=1|x) = \\frac{p(x|y=1) p(y=1)}{p(x)} $$\nLaplace Smoothing  The first time the Naive Bayes algorithm encounters a new feature, it cannot estimate the probability  $$ \\begin{aligned} p(y=1|x) \u0026= \\frac { \\prod_{j=1}^{d} p(x_j|y=1) p(y=1) } { \\prod_{j=1}^{d} p(x_j|y=1) p(y=1) + \\prod_{j=1}^{d} p(x_j|y=0) p(y=0) } \\cr p(y=1|x) \u0026= \\frac{0}{0} \\end{aligned} $$\n Because $\\prod p(x_j|y)$ includes $p(x_{new}|y)$ which is $0$ and therefore it always obtains $0/0$. It is a bad idea to estimate a previously unseen event to zero.  Solution\n Introduce Laplace smoothing  $$ \\phi_j = \\frac{1 + \\sum_{i=1}^{n} 1 { z^{(i)}=j }} {k+n} $$\n Add $1$ to the numerator Add $k$ to the denominator $\\sum_{j=1}^{k} \\phi_j=1$ still holds $\\phi_j \\neq 0$ for all $j$  Under certain (strong) conditions\n Laplace smoothing gives the optimal estimator  ","wordCount":"814","inLanguage":"en","datePublished":"2022-04-07T22:00:00+08:00","dateModified":"2022-04-07T22:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrelimzs.github.io/posts/machine-learning/cs239-generative-learning/"},"publisher":{"@type":"Organization","name":"AndreLimZS","logo":{"@type":"ImageObject","url":"https://andrelimzs.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrelimzs.github.io accesskey=h title="AndreLimZS (Alt + H)">AndreLimZS</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>CS229 Generative Learning</h1><div class=post-meta><span title="2022-04-07 22:00:00 +0800 +0800">April 7, 2022</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#generative-learning aria-label="Generative Learning">Generative Learning</a><ul><li><a href=#multivariate-normal-distribution aria-label="Multivariate Normal Distribution">Multivariate Normal Distribution</a><ul><li><a href=#mean aria-label=Mean>Mean</a></li><li><a href=#covariance aria-label=Covariance>Covariance</a></li><li><a href=#standard-normal-distribution aria-label="Standard Normal Distribution">Standard Normal Distribution</a></li></ul></li><li><a href=#gaussian-discriminant-analysis aria-label="Gaussian Discriminant Analysis">Gaussian Discriminant Analysis</a><ul><li><a href=#gda-and-logistic-regression aria-label="GDA and Logistic Regression">GDA and Logistic Regression</a></li><li><a href=#comparison aria-label=Comparison>Comparison</a></li></ul></li><li><a href=#naive-bayes aria-label="Naive Bayes">Naive Bayes</a><ul><li><a href=#naive-bayes-assumption aria-label="Naive Bayes Assumption">Naive Bayes Assumption</a></li><li><a href=#parameterisation aria-label=Parameterisation>Parameterisation</a></li></ul></li><li><a href=#laplace-smoothing aria-label="Laplace Smoothing">Laplace Smoothing</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=generative-learning>Generative Learning<a hidden class=anchor aria-hidden=true href=#generative-learning>#</a></h1><p>Different approach to learning</p><p>Try to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly</p><ul><li>$p(x|y)$ : Distribution of the target&rsquo;s features</li><li>$p(y)$ : Class priors</li></ul><p>Use <em>Bayes rule</em> to calculate posterior distribution $p(y|x)$
$$
p(y|x) = \frac{p(x|y) p(y)}{p(x)}
$$
Can be simplified when using for prediction because denominator doesn&rsquo;t matter
$$
\arg \max_y P(y|x) = \arg \max_y p(x|y) p(y)
$$</p><h2 id=multivariate-normal-distribution>Multivariate Normal Distribution<a hidden class=anchor aria-hidden=true href=#multivariate-normal-distribution>#</a></h2><ul><li>In $d$-dimensions</li></ul><p>Parameterised by:</p><ul><li><strong>Mean Vector</strong> : $\mu \in \mathbb{R}^d$</li><li><strong>Covariance Matrix</strong> : $\Sigma \in \mathbb{R}^{d\times d}$</li></ul><p>​ $\Sigma$ symmetric and PSD
$$
p(x; \mu, \Sigma) =
\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}
\exp \left( -\frac{1}{2} (x-\mu)^T
\Sigma^{-1} (x-\mu)
\right)
$$
Where $|\Sigma|$ is the determinant of $\Sigma$</p><h3 id=mean>Mean<a hidden class=anchor aria-hidden=true href=#mean>#</a></h3><p>Is given by $\mu$
$$
\mathbb{E}[X] = \int_x{ x p(x; \mu,\Sigma) dx } = \mu
$$</p><h3 id=covariance>Covariance<a hidden class=anchor aria-hidden=true href=#covariance>#</a></h3><p>Is defined as
$$
Cov(X) = \mathbb{E}[(X-\mathbb{E}[X])(X-\mathbb{E}[X])^T]
$$</p><p>$$
Cov(X) = \mathbb{E}[XX^T] - (\mathbb{E}[X])(\mathbb{E}[X])^T
$$</p><p>As cov increases, the distribution becomes more spread out</p><p>Off-diagonal terms skew the distribution</p><h3 id=standard-normal-distribution>Standard Normal Distribution<a hidden class=anchor aria-hidden=true href=#standard-normal-distribution>#</a></h3><p>Gaussian with zero mean and identity covariance</p><h2 id=gaussian-discriminant-analysis>Gaussian Discriminant Analysis<a hidden class=anchor aria-hidden=true href=#gaussian-discriminant-analysis>#</a></h2><p>Assume that $p(x|y)$ is distributed according to a multivariate normal distribution
$$
\begin{aligned}
y &\sim \text{Bernoulli}(\phi) \cr
x|y=0 &\sim \mathcal{N}(\mu_0, \Sigma) \cr
x|y=1 &\sim \mathcal{N}(\mu_1, \Sigma)
\end{aligned}
$$
Which expands to
$$
\begin{aligned}
p(y) &\sim \phi^y (1-\phi)^{1-y} \cr
p(x|y=0) &\sim
\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}
\exp \left(
-\frac{1}{2} (x-\mu_0)^T \Sigma^{-1} (x-\mu_0)
\right) \cr
p(x|y=1) &\sim
\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}
\exp \left(
-\frac{1}{2} (x-\mu_1)^T \Sigma^{-1} (x-\mu_1)
\right) \cr
\end{aligned}
$$
This model usually combines $\Sigma_0$/$\Sigma_1$ into $\Sigma$</p><p>The log-likelihood is
$$
\begin{aligned}
l(\phi, \mu_0, \mu_1, \Sigma) &=
log \prod_{i=1}^{n} p(x^{(i)}, y^{(i)};
\phi, \mu_0, \mu_1, \Sigma) \cr
&= \log \prod_{i=1}^{n} p(x|y; \mu, \mu, \Sigma)\ p(y;\phi)
\end{aligned}
$$
Find the maximum likelihood estimates of the parameters by maximising $l$ w.r.t the parameters
$$
\phi = \frac{1}{n} \sum_{i=1}^{n} 1{ y^{(i)}=1 }
$$</p><p>$$
\mu_0 = \frac{\sum_{i=1}^n 1{ y^{(i)}=0 } x^{(i)}}
{\sum_{i=1}^n 1{ y^{(i)}=0 }}
$$</p><p>$$
\mu_1 = \frac{\sum_{i=1}^n 1{ y^{(i)}=1 } x^{(i)}}
{\sum_{i=1}^n 1{ y^{(i)}=1 }}
$$</p><p>$$
\Sigma = \frac{1}{n} \sum_{i=1}^{n}
(x^{(i)} - \mu_{y^{(i)}})
(x^{(i)} - \mu_{y^{(i)}})^T
$$</p><p>Graphically, the algorithm is learning the two gaussian distributions</p><figure class=align-center><img loading=lazy src=GDA01.jpg#center width=360></figure><p>The decision boundary is separating hyperplane between the two distributions</p><h3 id=gda-and-logistic-regression>GDA and Logistic Regression<a hidden class=anchor aria-hidden=true href=#gda-and-logistic-regression>#</a></h3><p>Expressing $y$ as a function of $x$
$$
p(y=1|x; \phi, \Sigma, \mu_0, \mu_1) =
\frac{1}{1 + \exp(-\theta^Tx)}
$$
Has the exact same form as <strong>logistic regression</strong></p><p>But GDA and logistic regression will generally give different decision boundaries</p><p>This is because</p><ul><li>Multivariate Gaussian $\implies$ Logistic function</li><li>Logistic function $\centernot\implies$ Multivariate Gaussian</li></ul><p>GDA makes <strong>stronger modelling assumptions</strong> than logistic regression</p><ul><li>If assumptions are correct, GDA will find better fits</li><li>If $p(x|y)$ is gaussian with shared $\Sigma$, GDA is <strong>asymptotically efficient</strong><ul><li>As $n \rightarrow \infty$, no algorithm is strictly better than GDA</li></ul></li></ul><h3 id=comparison>Comparison<a hidden class=anchor aria-hidden=true href=#comparison>#</a></h3><table><thead><tr><th>GDA</th><th>Logistic</th></tr></thead><tbody><tr><td>Stronger modelling assumptions</td><td>Less sensitive to modelling errors</td></tr><tr><td>More data efficient</td><td>More robust</td></tr></tbody></table><p>[*] In practise logistic regression is used more often than GDA</p><h2 id=naive-bayes>Naive Bayes<a hidden class=anchor aria-hidden=true href=#naive-bayes>#</a></h2><p>Similar to GDA but with discrete-valued $x$</p><ul><li><p>Encode set of features into a <strong>vocabulary</strong>. Dimension of $x$ equals size of vocabulary</p></li><li><p>But the number of parameters grows with</p></li></ul><p>$$
\large 2^\text{size of vocabulary}
$$</p><ul><li>and quickly becomes too large for an explicit representation</li></ul><h3 id=naive-bayes-assumption>Naive Bayes Assumption<a hidden class=anchor aria-hidden=true href=#naive-bayes-assumption>#</a></h3><blockquote><p>Assume that the $x$&rsquo;s are <strong>conditially independent</strong> given $y$</p></blockquote><p><strong>[Example]</strong></p><p>If an email is spam ($y=1$) and word $x_1$ is apple and word $x_2$ is car</p><ul><li>Knowing that $y=1$ and $x_1 = apple$</li><li>Gives no information about $x_2 = car$</li></ul><p>$$
p(x_2 | y) = p(x2 | y, x_1)
$$</p><p>[*] This does <strong>not</strong> say that $x_1$ and $x_2$ are independent
$$
\begin{aligned}
p(&x_1, \dots, x_{5000} | y) \cr
&= p(x_1|y) p(x1|y,x_1) \dots p(x_5000|y,x_1\dots x_{4999}) \cr
\end{aligned}
$$
Using the naive bayes assumption
$$
\begin{aligned}
&= p(x_1|y) p(x1|y) \dots p(x_5000|y) \cr
&= \prod_{j=1}^d p(x_j | y)
\end{aligned}
$$
The NB assumption is extremely strong</p><p>But the algorithm works well on many problems</p><h3 id=parameterisation>Parameterisation<a hidden class=anchor aria-hidden=true href=#parameterisation>#</a></h3><p>The joint likelihood is
$$
\mathcal{L}(\phi_y, \phi_{j|y=0}, \phi_{j|y=1}) =
\prod_{i=1}^{n} p(x^{(i)}, y^{(i)})
$$
Maximise w.r.t $\phi_y$, $\phi_{j|y=0}$ and $\phi_{j|y=1}$
$$
\begin{aligned}
\phi_{j|y=1} &=
\frac{\sum_{i=1}^n 1{ x_k^{(i)}=1 \wedge y^{(i)}=1 }}
{\sum_{i=1}^n 1{ y^{(i)}=1 }} \cr
\phi_{j|y=0} &=
\frac{\sum_{i=1}^n 1{ x_k^{(i)}=1 \wedge y^{(i)}=0 }}
{\sum_{i=1}^n 1{ y^{(i)}=0 }} \cr
\phi_{y} &=
\frac{\sum_{i=1}^n 1{ y^{(i)}=1 }}{n}
\end{aligned}
$$
To make a prediction, find the highest possiblity from
$$
p(y=1|x) = \frac{p(x|y=1) p(y=1)}{p(x)}
$$</p><h2 id=laplace-smoothing>Laplace Smoothing<a hidden class=anchor aria-hidden=true href=#laplace-smoothing>#</a></h2><ul><li>The first time the Naive Bayes algorithm encounters a new feature, it cannot estimate the probability</li></ul><p>$$
\begin{aligned}
p(y=1|x) &= \frac
{ \prod_{j=1}^{d} p(x_j|y=1) p(y=1) }
{ \prod_{j=1}^{d} p(x_j|y=1) p(y=1) + \prod_{j=1}^{d} p(x_j|y=0) p(y=0) } \cr
p(y=1|x) &= \frac{0}{0}
\end{aligned}
$$</p><ul><li>Because $\prod p(x_j|y)$ includes $p(x_{new}|y)$ which is $0$ and therefore it always obtains $0/0$.</li><li>It is a bad idea to estimate a previously unseen event to zero.</li></ul><p><strong>Solution</strong></p><ul><li>Introduce <strong>Laplace smoothing</strong></li></ul><p>$$
\phi_j = \frac{1 + \sum_{i=1}^{n} 1 { z^{(i)}=j }}
{k+n}
$$</p><ul><li>Add $1$ to the numerator</li><li>Add $k$ to the denominator</li><li>$\sum_{j=1}^{k} \phi_j=1$ still holds</li><li>$\phi_j \neq 0$ for all $j$</li></ul><p>Under certain (strong) conditions</p><ul><li>Laplace smoothing gives the optimal estimator</li></ul></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://andrelimzs.github.io>AndreLimZS</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>