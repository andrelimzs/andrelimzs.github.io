<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta name=robots content="index, follow"><title>Reinforcement Learning | AndreLimZS</title><meta name=keywords content><meta name=description content="Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$"><meta name=author content><link rel=canonical href=https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://andrelimzs.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrelimzs.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrelimzs.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrelimzs.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrelimzs.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.96.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$"><meta property="og:type" content="article"><meta property="og:url" content="https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-07T23:59:00+08:00"><meta property="article:modified_time" content="2022-04-07T23:59:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://andrelimzs.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Reinforcement Learning","item":"https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning","name":"Reinforcement Learning","description":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$","keywords":[],"articleBody":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology  States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$  Dynamics of MDPs  Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat  Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$\nPolicy Function $\\pi : S \\mapsto A$ maps states to actions\nExecute a policy by following the prescribed action\nValue Function The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\\pi$ $$ V^\\pi(s) = \\mathbb{E} \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2)\n \\dots | s_0 = s,\\pi \\right] $$  Bellman Equation Given a fixed policy $\\pi$, it’s value function $V^\\pi$ satisfies the Bellman equations $$ V^\\pi(s) = R(s) + \\gamma \\sum_{s’ \\in S} P_{s\\pi(s)}(s’) V^\\pi(s’) $$ Which consists of two terms:\n Immediate reward $R(s)$ Get for starting in $s$ Expected sum of future discounted rewards Can we rewritten as $E_{s’\\sim P_{s\\pi(s)}}[V^\\pi(s’)]$ The expected sum of starting in $s’$ where $s’$ is distributed according to to $P_{s\\pi(s)}$  Can be used to efficiently solve for the value function $V^\\pi$\n Write down one equation for every state Gives a set of $|S|$ linear equations in $|S|$ variables  Optimal Value Function Best possible expected sum of discounted rewards which can be attained using any policy $$ V^(s) = \\max_\\pi V^\\pi (s) $$ Which has it’s own version of the Bellman Equation $$ V^(s) = R(s) + \\max_{a\\in A} \\gamma \\sum_{s’\\in S} P_{sa} (s’) V^*(s’) $$ The second term is a $\\max$ over all possible actions because that is the optimal reward.\nOptimal Policy $$ \\pi^(s) = \\arg \\max_{a\\in A} \\sum_{s’\\in S} P_{sa}(s’) V^(s’) $$\nWhich gives the action that attains the $\\max$ in the optimal value function\nFor every state $s$ and policy $\\pi$ $$ V^(s) = V^{\\pi}(s) \\geq V^\\pi(s) $$ Which says that\n The value function for the optimal policy is equal to the optimal value function for every state $s$\n  The value function for the optimal policy is greater than or equal to every other policy\n  $\\pi^*$ is the optimal policy for all states  Value Iteration and Policy Iteration Two efficient algorithms for solving finite-state MDPs with known transition probabilities\nValue Iteration  For each state s, initialize $V(s) := 0$ for until convergence do For every state, update $$ V(s) := R(s) + \\max_{a\\in A} \\gamma \\sum_{s’} P_{sa}(s’) V(s’) $$\n  Repeatedly update estimated value function using the Bellman equation  Can be:\n Synchronous  Compute new values for all states and update at once Can be viewed as implementing a Bellman Backup Operator, which maps the current estimate to a new estimate   Asynchronous  Loop over states and update one at a time    Both algorithms will cause $V$ to converge to $V^*$\nPolicy Iteration  Initialise $\\pi$ randomly\nfor until convergence do\n​\tLet $V := V^\\pi$\n​\tFor each state $s$ $$ \\pi(s) := \\arg \\max_{a \\in A} \\sum_{s’} P_{sa}(s’) V(s’) $$\n  Repeatedly computes value function for the current policy Update policy using current value function  This is the policy that is greedy w.r.t. $V$   The value function can be updated using Bellman equations After a finite number of iterations, $V$ will converge to $V^$ and $\\pi$ to $\\pi^$  Value vs Policy Iteration  No universal agreement which is better Small MDPs : Policy iteration tends to be fast Larger MDPs : Value iteration might be faster Policy iteration can reach the exact $V^*$ Value iteration will have small non-zero error  Learning a Model for an MDP   In many realistic problems, we do not know the state transition probabilities and rewards\n  Must be estimated from data\n  Given enough trials, can derive maximum likelihood estimates for the state transition probabilities $$ P_{sa}(s’) = \\frac{# \\text{ times } a \\text{ led to } s’ } {# \\text{ times we took } a} $$ In the case of $\\frac{0}{0}$, can set $P$ to $\\frac{1}{|S|}$ (estimate as uniform)\nEfficient Update Keep count of numerator \u0026 denominator\nSolving Learned MDP  Can use policy/value iteration  Continuous State MDPs  Infinite number of states Consider settings where state space is $S = \\mathbb{R}^d$  Discretization  Simplest way Discretize state space Use value/policy iteration  Downsides\n Fairly naive representation for $V^*$ Assumes the value function is constant over each discretization interval Piecewise constant representation isn’t good for many smooth functions  Little smoothing over inputs No generalization over different grid cells   Suffers from curse of dimensionality  For $S = \\mathbb{R}^d$, we will have $k^d$ states    Rule of Thumb\n Works well for 1D/2D cases Can work for 3D/4D cases Rarely works above 6D  Value Function Approximation  Approximate $V^*$ directly  Using Model/Simulator  Assume we have a model/simulator for the MDP Any black-box with  Input : State $s_t$, Action $a_t$ Output : Next-state $s_{t+1}$ sampled    Getting the Model Using physics simulation  Can use off-the-shelf physics simulation  Data collected in the MDP  Can be done with:  Random actions Executing some policy Other way of choosing actions [?] Control law?   Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$  Example : Learn linear model\nusing linear regression $$ \\arg \\min_{A,B} \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\left\\Vert s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)}) \\right\\Vert^2_2 $$ Can build either a deterministic $$ s_{t+1} = As_t + Ba_t $$ or stochastic model $$ s_{t+1} = As_t + Ba_t + \\epsilon_t $$ with $\\epsilon_t$ being a noise term usually modelled as $\\epsilon_t \\sim \\mathcal{N}(0,\\Sigma)$\nor non-linear function $$ s_{t+1} = A\\phi_s(s_t) + B\\phi_a(a_t) $$ Eg: Locally weighted linear regression\nFitted Value Iteration  Algorithm for approximating the value function of a continuous state MDP Assume state space $S = \\mathcal{R}^d$ But action space $A$ is small and discrete  Perform value iteration update $$ \\begin{aligned} V(s) :\u0026= R(s) + \\gamma \\max_a \\int_{s’} P_{sa}(s’) V(s’) ds’ \\ \u0026= R(s) + \\gamma \\max_a E_{s’ \\sim P_{sa}} [V(s’)] \\end{aligned} $$\n Main difference is the $\\int$ instead of $\\sum$ Approximately update over a finite sample of states $s^{(1)}, \\dots, s^{(n)}$ Use supervised learning algorithm, linear regression to approximate value function as linear/nonlinear function of the states  $$ V(s) = \\theta^T \\phi(s) $$\nAlgorithm\n Compute $y^{(i)}$, the approximation to $R(s) + \\gamma \\max_a E_{s’ \\sim P_{sa}} [V(s’)]$ (RHS)\nUse supervised learning to get $V(s) = y^{(i)}$\nRepeat\n Convergence  Cannot be proved to converge But in practice it often does, and works well for many problems  Deterministic Simplification  Can set $k=1$ Because expectation of a deterministic distribution requires 1 sample  Policy  Outputs $V$, an approximation of $V^*$ Implicitly defines policy  When in state $s$, choose $$ \\arg \\max_a E_{s’\\sim P_{sa}} [V(s’)] $$\n Process for computing/approximating this is similar to the fitted value iteration algorithm  ","wordCount":"1199","inLanguage":"en","datePublished":"2022-04-07T23:59:00+08:00","dateModified":"2022-04-07T23:59:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/"},"publisher":{"@type":"Organization","name":"AndreLimZS","logo":{"@type":"ImageObject","url":"https://andrelimzs.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrelimzs.github.io accesskey=h title="AndreLimZS (Alt + H)">AndreLimZS</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Reinforcement Learning</h1><div class=post-meta><span title="2022-04-07 23:59:00 +0800 +0800">April 7, 2022</span></div></header><div class=post-content><h2 id=reinforcement-learning>Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#reinforcement-learning>#</a></h2><p>Many sequential decision making and control problems are hard to provide explicit supervision for.</p><p>Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.</p><p>Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc</p><h2 id=markov-decision-processes-mdp>Markov Decision Processes (MDP)<a hidden class=anchor aria-hidden=true href=#markov-decision-processes-mdp>#</a></h2><p>Provide formalism for many RL problems</p><h3 id=terminology>Terminology<a hidden class=anchor aria-hidden=true href=#terminology>#</a></h3><ul><li><strong>States</strong> : $s$</li><li><strong>Actions</strong> : $a$</li><li><strong>State Transition Probabilities</strong> : $P_{sa}$</li><li><strong>Discount Factor</strong> : $\gamma \in [0,1)$</li><li><strong>Reward Function</strong> : $R : S \times A \mapsto \mathbb{R}$</li></ul><h3 id=dynamics-of-mdps>Dynamics of MDPs<a hidden class=anchor aria-hidden=true href=#dynamics-of-mdps>#</a></h3><ol><li>Start in some state $s_0$</li><li>Choose some action $a_0 \in A$</li><li>MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$</li><li>Pick another action</li><li>Repeat</li></ol><p>Represent as
$$
s_0
\overset{a_0}{\rightarrow} s_1
\overset{a_1}{\rightarrow} s_2
\overset{a_2}{\rightarrow} s_3
\overset{a_3}{\rightarrow} \dots
$$
<strong>Total Payoff</strong>
$$
R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots
$$
<strong>Goal</strong> : Maximise expected value of total payoff
$$
E \left[
R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots
\right]
$$</p><h3 id=policy>Policy<a hidden class=anchor aria-hidden=true href=#policy>#</a></h3><p>Function $\pi : S \mapsto A$ maps states to actions</p><p><strong>Execute</strong> a policy by following the prescribed action</p><h3 id=value-function>Value Function<a hidden class=anchor aria-hidden=true href=#value-function>#</a></h3><p>The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\pi$
$$
V^\pi(s) = \mathbb{E}
\left[
R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2)</p><ul><li>\dots | s_0 = s,\pi
\right]
$$</li></ul><h3 id=bellman-equation>Bellman Equation<a hidden class=anchor aria-hidden=true href=#bellman-equation>#</a></h3><p>Given a fixed policy $\pi$, it&rsquo;s value function $V^\pi$ satisfies the Bellman equations
$$
V^\pi(s) = R(s) + \gamma \sum_{s&rsquo; \in S} P_{s\pi(s)}(s&rsquo;) V^\pi(s&rsquo;)
$$
Which consists of two terms:</p><ul><li>Immediate reward $R(s)$
Get for starting in $s$</li><li>Expected sum of future discounted rewards
Can we rewritten as $E_{s&rsquo;\sim P_{s\pi(s)}}[V^\pi(s&rsquo;)]$
The expected sum of starting in $s&rsquo;$ where $s&rsquo;$ is distributed according to to $P_{s\pi(s)}$</li></ul><p>Can be used to efficiently solve for the value function $V^\pi$</p><ul><li>Write down one equation for every state</li><li>Gives a set of $|S|$ linear equations in $|S|$ variables</li></ul><h3 id=optimal-value-function>Optimal Value Function<a hidden class=anchor aria-hidden=true href=#optimal-value-function>#</a></h3><p>Best possible expected sum of discounted rewards which can be attained using any policy
$$
V^<em>(s) = \max_\pi V^\pi (s)
$$
Which has it&rsquo;s own version of the <strong>Bellman Equation</strong>
$$
V^</em>(s) = R(s) + \max_{a\in A} \gamma \sum_{s&rsquo;\in S} P_{sa} (s&rsquo;) V^*(s&rsquo;)
$$
The second term is a $\max$ over all possible actions because that is the optimal reward.</p><h3 id=optimal-policy>Optimal Policy<a hidden class=anchor aria-hidden=true href=#optimal-policy>#</a></h3><p>$$
\pi^<em>(s) = \arg \max_{a\in A} \sum_{s&rsquo;\in S} P_{sa}(s&rsquo;) V^</em>(s&rsquo;)
$$</p><p>Which gives the action that attains the $\max$ in the optimal value function</p><p>For every state $s$ and policy $\pi$
$$
V^<em>(s) = V^{\pi</em>}(s) \geq V^\pi(s)
$$
Which says that</p><blockquote><p>The value function for the optimal policy is equal to the optimal value function for every state $s$</p></blockquote><blockquote><p>The value function for the optimal policy is greater than or equal to every other policy</p></blockquote><ul><li>$\pi^*$ is the optimal policy for <strong>all</strong> states</li></ul><h2 id=value-iteration-and-policy-iteration>Value Iteration and Policy Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration-and-policy-iteration>#</a></h2><p>Two efficient algorithms for solving finite-state MDPs with known transition probabilities</p><h3 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h3><blockquote><p>For each state s, initialize $V(s) := 0$
for until convergence do
For every state, update
$$
V(s) := R(s) + \max_{a\in A} \gamma
\sum_{s&rsquo;} P_{sa}(s&rsquo;) V(s&rsquo;)
$$</p></blockquote><ul><li>Repeatedly update estimated value function using the Bellman equation</li></ul><p>Can be:</p><ul><li>Synchronous<ul><li>Compute new values for <strong>all</strong> states and update at once</li><li>Can be viewed as implementing a <em>Bellman Backup Operator</em>, which maps the current estimate to a new estimate</li></ul></li><li>Asynchronous<ul><li>Loop over states and update one at a time</li></ul></li></ul><p>Both algorithms will cause $V$ to converge to $V^*$</p><h3 id=policy-iteration>Policy Iteration<a hidden class=anchor aria-hidden=true href=#policy-iteration>#</a></h3><blockquote><p>Initialise $\pi$ randomly</p><p>for until convergence do</p><p>​ Let $V := V^\pi$</p><p>​ For each state $s$
$$
\pi(s) := \arg \max_{a \in A} \sum_{s&rsquo;} P_{sa}(s&rsquo;) V(s&rsquo;)
$$</p></blockquote><ul><li>Repeatedly computes value function for the current policy</li><li>Update policy using current value function<ul><li>This is the policy that is <strong>greedy w.r.t. $V$</strong></li></ul></li><li>The value function can be updated using Bellman equations</li><li>After a finite number of iterations, $V$ will converge to $V^<em>$ and $\pi$ to $\pi^</em>$</li></ul><h3 id=value-vs-policy-iteration>Value vs Policy Iteration<a hidden class=anchor aria-hidden=true href=#value-vs-policy-iteration>#</a></h3><ul><li>No universal agreement which is better</li><li>Small MDPs : Policy iteration tends to be fast</li><li>Larger MDPs : Value iteration might be faster</li><li>Policy iteration can reach the exact $V^*$</li><li>Value iteration will have small non-zero error</li></ul><h2 id=learning-a-model-for-an-mdp>Learning a Model for an MDP<a hidden class=anchor aria-hidden=true href=#learning-a-model-for-an-mdp>#</a></h2><ul><li><p>In many realistic problems, we do not know the state transition probabilities and rewards</p></li><li><p>Must be estimated from data</p></li></ul><p>Given enough trials, can derive <strong>maximum likelihood estimates</strong> for the state transition probabilities
$$
P_{sa}(s&rsquo;) =
\frac{# \text{ times } a \text{ led to } s&rsquo; }
{# \text{ times we took } a}
$$
In the case of $\frac{0}{0}$, can set $P$ to $\frac{1}{|S|}$ (estimate as uniform)</p><h3 id=efficient-update>Efficient Update<a hidden class=anchor aria-hidden=true href=#efficient-update>#</a></h3><p>Keep count of numerator & denominator</p><h3 id=solving-learned-mdp>Solving Learned MDP<a hidden class=anchor aria-hidden=true href=#solving-learned-mdp>#</a></h3><ul><li>Can use <strong>policy</strong>/<strong>value</strong> iteration</li></ul><h2 id=continuous-state-mdps>Continuous State MDPs<a hidden class=anchor aria-hidden=true href=#continuous-state-mdps>#</a></h2><ul><li>Infinite number of states</li><li>Consider settings where state space is $S = \mathbb{R}^d$</li></ul><h3 id=discretization>Discretization<a hidden class=anchor aria-hidden=true href=#discretization>#</a></h3><ul><li>Simplest way</li><li>Discretize state space</li><li>Use value/policy iteration</li></ul><p><strong>Downsides</strong></p><ul><li>Fairly naive representation for $V^*$</li><li>Assumes the value function is constant over each discretization interval</li><li>Piecewise constant representation isn&rsquo;t good for many smooth functions<ul><li>Little smoothing over inputs</li><li>No generalization over different grid cells</li></ul></li><li>Suffers from <strong>curse of dimensionality</strong><ul><li>For $S = \mathbb{R}^d$, we will have $k^d$ states</li></ul></li></ul><p><strong>Rule of Thumb</strong></p><ul><li>Works well for 1D/2D cases</li><li>Can work for 3D/4D cases</li><li>Rarely works above 6D</li></ul><h3 id=value-function-approximation>Value Function Approximation<a hidden class=anchor aria-hidden=true href=#value-function-approximation>#</a></h3><ul><li>Approximate $V^*$ directly</li></ul><h3 id=using-modelsimulator>Using Model/Simulator<a hidden class=anchor aria-hidden=true href=#using-modelsimulator>#</a></h3><ul><li>Assume we have a model/simulator for the MDP</li><li>Any black-box with<ul><li>Input : State $s_t$, Action $a_t$</li><li>Output : Next-state $s_{t+1}$ sampled</li></ul></li></ul><h3 id=getting-the-model>Getting the Model<a hidden class=anchor aria-hidden=true href=#getting-the-model>#</a></h3><h4 id=using-physics-simulation>Using physics simulation<a hidden class=anchor aria-hidden=true href=#using-physics-simulation>#</a></h4><ul><li>Can use off-the-shelf physics simulation</li></ul><h4 id=data-collected-in-the-mdp>Data collected in the MDP<a hidden class=anchor aria-hidden=true href=#data-collected-in-the-mdp>#</a></h4><ul><li>Can be done with:<ul><li>Random actions</li><li>Executing some policy</li><li>Other way of choosing actions
[?] Control law?</li></ul></li><li>Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$</li></ul><p><strong>Example</strong> : Learn linear model</p><p>using linear regression
$$
\arg \min_{A,B} \sum_{i=1}^{n} \sum_{t=0}^{T-1}
\left\Vert
s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)})
\right\Vert^2_2
$$
Can build either a <em>deterministic</em>
$$
s_{t+1} = As_t + Ba_t
$$
or <em>stochastic</em> model
$$
s_{t+1} = As_t + Ba_t + \epsilon_t
$$
with $\epsilon_t$ being a noise term usually modelled as $\epsilon_t \sim \mathcal{N}(0,\Sigma)$</p><p>or non-linear function
$$
s_{t+1} = A\phi_s(s_t) + B\phi_a(a_t)
$$
Eg: Locally weighted linear regression</p><h3 id=fitted-value-iteration>Fitted Value Iteration<a hidden class=anchor aria-hidden=true href=#fitted-value-iteration>#</a></h3><ul><li>Algorithm for approximating the value function of a continuous state MDP</li><li>Assume state space $S = \mathcal{R}^d$</li><li>But action space $A$ is <strong>small</strong> and <strong>discrete</strong></li></ul><p>Perform value iteration update
$$
\begin{aligned}
V(s) :&= R(s) + \gamma \max_a
\int_{s&rsquo;} P_{sa}(s&rsquo;) V(s&rsquo;) ds&rsquo; \
&= R(s) + \gamma \max_a E_{s&rsquo; \sim P_{sa}} [V(s&rsquo;)]
\end{aligned}
$$</p><ul><li>Main difference is the $\int$ instead of $\sum$</li><li>Approximately update over a finite sample of states $s^{(1)}, \dots, s^{(n)}$</li><li>Use supervised learning algorithm, <strong>linear regression</strong> to approximate value function as linear/nonlinear function of the states</li></ul><p>$$
V(s) = \theta^T \phi(s)
$$</p><p><strong>Algorithm</strong></p><blockquote><p>Compute $y^{(i)}$, the approximation to $R(s) + \gamma \max_a E_{s&rsquo; \sim P_{sa}} [V(s&rsquo;)]$ (RHS)</p><p>Use supervised learning to get $V(s) = y^{(i)}$</p><p>Repeat</p></blockquote><h4 id=convergence>Convergence<a hidden class=anchor aria-hidden=true href=#convergence>#</a></h4><ul><li>Cannot be proved to converge</li><li>But in practice it often does, and works well for many problems</li></ul><h4 id=deterministic-simplification>Deterministic Simplification<a hidden class=anchor aria-hidden=true href=#deterministic-simplification>#</a></h4><ul><li>Can set $k=1$</li><li>Because expectation of a deterministic distribution requires 1 sample</li></ul><h4 id=policy-1>Policy<a hidden class=anchor aria-hidden=true href=#policy-1>#</a></h4><ul><li>Outputs $V$, an approximation of $V^*$</li><li>Implicitly defines policy</li></ul><p>When in state $s$, choose
$$
\arg \max_a E_{s&rsquo;\sim P_{sa}} [V(s&rsquo;)]
$$</p><ul><li>Process for computing/approximating this is similar to the fitted value iteration algorithm</li></ul></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://andrelimzs.github.io>AndreLimZS</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>