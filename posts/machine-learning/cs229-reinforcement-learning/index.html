<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta name=robots content="index, follow"><title>Reinforcement Learning | AndreLimZS</title><meta name=keywords content><meta name=description content="Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$"><meta name=author content="Andre Lim"><link rel=canonical href=https://andrelimzs.github.io/posts/machine-learning/cs229-reinforcement-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://andrelimzs.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrelimzs.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrelimzs.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrelimzs.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrelimzs.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.101.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$"><meta property="og:type" content="article"><meta property="og:url" content="https://andrelimzs.github.io/posts/machine-learning/cs229-reinforcement-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-11T23:59:00+08:00"><meta property="article:modified_time" content="2022-04-11T23:59:00+08:00"><meta property="og:site_name" content="AndreLimZS"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://andrelimzs.github.io/posts/"},{"@type":"ListItem","position":4,"name":"Reinforcement Learning","item":"https://andrelimzs.github.io/posts/machine-learning/cs229-reinforcement-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning","name":"Reinforcement Learning","description":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$","keywords":[],"articleBody":"Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.\nInstead provide a reward function and let the learning algorithm figure out how to choose actions over time.\nSuccessful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc\nMarkov Decision Processes (MDP) Provide formalism for many RL problems\nTerminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\\gamma \\in [0,1)$ Reward Function : $R : S \\times A \\mapsto \\mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \\in A$ MDP randomly transitions to successor state $s_1 \\sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \\overset{a_0}{\\rightarrow} s_1 \\overset{a_1}{\\rightarrow} s_2 \\overset{a_2}{\\rightarrow} s_3 \\overset{a_3}{\\rightarrow} \\dots $$ Total Payoff $$ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\dots $$ Goal : Maximise expected value of total payoff $$ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots \\right] $$\nPolicy Function $\\pi : S \\mapsto A$ maps states to actions\nExecute a policy by following the prescribed action\nValue Function The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\\pi$ $$ V^\\pi(s) = \\mathbb{E} \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots | s_0 = s,\\pi \\right] $$\nBellman Equation Given a fixed policy $\\pi$, it’s value function $V^\\pi$ satisfies the Bellman equations $$ V^\\pi(s) = R(s) + \\gamma \\sum_{s’ \\in S} P_{s\\pi(s)}(s’) V^\\pi(s’) $$ Which consists of two terms:\nImmediate reward $R(s)$ Get for starting in $s$ Expected sum of future discounted rewards Can we rewritten as $E_{s’\\sim P_{s\\pi(s)}}[V^\\pi(s’)]$ The expected sum of starting in $s’$ where $s’$ is distributed according to to $P_{s\\pi(s)}$ Can be used to efficiently solve for the value function $V^\\pi$\nWrite down one equation for every state Gives a set of $|S|$ linear equations in $|S|$ variables Optimal Value Function Best possible expected sum of discounted rewards which can be attained using any policy\n$$ V^*(s) = \\max_\\pi V^\\pi (s) $$\nWhich has it’s own version of the Bellman Equation\n$$ V^{\\ast}(s) = R(s) + \\max_{a\\in A} \\gamma \\sum_{s’\\in S}{P_{sa} (s’) V^{\\ast}(s’)} $$\nThe second term is a $\\max$ over all possible actions because that is the optimal reward.\nOptimal Policy $$ \\pi^{\\ast}(s) = \\arg \\max_{a\\in A} \\sum_{s’\\in S} P_{sa}(s’) V^*(s’) $$\nWhich gives the action that attains the $\\max$ in the optimal value function\nFor every state $s$ and policy $\\pi$ $$ V^\\ast(s) = V^{\\pi*}(s) \\geq V^\\pi(s) $$ Which says that\nThe value function for the optimal policy is equal to the optimal value function for every state $s$\nThe value function for the optimal policy is greater than or equal to every other policy\n$\\pi^*$ is the optimal policy for all states Value Iteration and Policy Iteration Two efficient algorithms for solving finite-state MDPs with known transition probabilities\nValue Iteration For each state s, initialize $V(s) := 0$ for until convergence do For every state, update $$ V(s) := R(s) + \\max_{a\\in A} \\gamma \\sum_{s’} P_{sa}(s’) V(s’) $$\nRepeatedly update estimated value function using the Bellman equation Can be:\nSynchronous Compute new values for all states and update at once Can be viewed as implementing a Bellman Backup Operator, which maps the current estimate to a new estimate Asynchronous Loop over states and update one at a time Both algorithms will cause $V$ to converge to $V^*$\nPolicy Iteration Initialise $\\pi$ randomly\nfor until convergence do\n​\tLet $V := V^\\pi$\n​\tFor each state $s$ $$ \\pi(s) := \\arg \\max_{a \\in A} \\sum_{s’} P_{sa}(s’) V(s’) $$\nRepeatedly computes value function for the current policy Update policy using current value function This is the policy that is greedy w.r.t. $\\bold V$ The value function can be updated using Bellman equations After a finite number of iterations, $V$ will converge to $V^\\ast$ and $\\pi$ to $\\pi^\\ast$ Value vs Policy Iteration No universal agreement which is better Small MDPs : Policy iteration tends to be fast Larger MDPs : Value iteration might be faster Policy iteration can reach the exact $V^*$ Value iteration will have small non-zero error Learning a Model for an MDP In many realistic problems, we do not know the state transition probabilities and rewards\nMust be estimated from data\nGiven enough trials, can derive maximum likelihood estimates for the state transition probabilities $$ P_{sa}(s’) = \\frac{num. \\text{ times } a \\text{ led to } s’ } {num. \\text{ times we took } a} $$ In the case of $\\frac{0}{0}$, can set $P$ to $\\frac{1}{|S|}$ (estimate as uniform)\nEfficient Update Keep count of numerator \u0026 denominator\nSolving Learned MDP Can use policy/value iteration Continuous State MDPs Infinite number of states Consider settings where state space is $S = \\mathbb{R}^d$ Discretization Simplest way Discretize state space Use value/policy iteration Downsides\nFairly naive representation for $V^*$ Assumes the value function is constant over each discretization interval Piecewise constant representation isn’t good for many smooth functions Little smoothing over inputs No generalization over different grid cells Suffers from curse of dimensionality For $S = \\mathbb{R}^d$, we will have $k^d$ states Rule of Thumb\nWorks well for 1D/2D cases Can work for 3D/4D cases Rarely works above 6D Value Function Approximation Approximate $V^*$ directly Using Model/Simulator Assume we have a model/simulator for the MDP Any black-box with Input : State $s_t$, Action $a_t$ Output : Next-state $s_{t+1}$ sampled Getting the Model Using physics simulation Can use off-the-shelf physics simulation Data collected in the MDP Can be done with: Random actions Executing some policy Other way of choosing actions [?] Control law? Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$ Example : Learn linear model\nusing linear regression $$ \\arg \\min_{A,B} \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\left\\Vert s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)}) \\right\\Vert^2_2 $$ Can build either a deterministic $$ s_{t+1} = As_t + Ba_t $$ or stochastic model $$ s_{t+1} = As_t + Ba_t + \\epsilon_t $$ with $\\epsilon_t$ being a noise term usually modelled as $\\epsilon_t \\sim \\mathcal{N}(0,\\Sigma)$\nor non-linear function $$ s_{t+1} = A\\phi_s(s_t) + B\\phi_a(a_t) $$ Eg: Locally weighted linear regression\nFitted Value Iteration Algorithm for approximating the value function of a continuous state MDP Assume state space $S = \\mathcal{R}^d$ But action space $A$ is small and discrete Perform value iteration update $$ \\begin{aligned} V(s) :\u0026= R(s) + \\gamma \\max_a \\int_{s’} P_{sa}(s’) V(s’) ds’ \\cr \u0026= R(s) + \\gamma \\max_a E_{s’ \\sim P_{sa}} [V(s’)] \\end{aligned} $$\nMain difference is the $\\int$ instead of $\\sum$ Approximately update over a finite sample of states $s^{(1)}, \\dots, s^{(n)}$ Use supervised learning algorithm, linear regression to approximate value function as linear/nonlinear function of the states $$ V(s) = \\theta^T \\phi(s) $$\nAlgorithm\nCompute $y^{(i)}$, the approximation to $R(s) + \\gamma \\max_a E_{s’ \\sim P_{sa}} [V(s’)]$ (RHS)\nUse supervised learning to get $V(s) = y^{(i)}$\nRepeat\nConvergence Cannot be proved to converge But in practice it often does, and works well for many problems Deterministic Simplification Can set $k=1$ Because expectation of a deterministic distribution requires 1 sample Policy Outputs $V$, an approximation of $V^*$ Implicitly defines policy When in state $s$, choose $$ \\arg \\max_a E_{s’\\sim P_{sa}} [V(s’)] $$\nProcess for computing/approximating this is similar to the fitted value iteration algorithm ","wordCount":"1200","inLanguage":"en","datePublished":"2022-04-11T23:59:00+08:00","dateModified":"2022-04-11T23:59:00+08:00","author":{"@type":"Person","name":"Andre Lim"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://andrelimzs.github.io/posts/machine-learning/cs229-reinforcement-learning/"},"publisher":{"@type":"Organization","name":"AndreLimZS","logo":{"@type":"ImageObject","url":"https://andrelimzs.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://andrelimzs.github.io accesskey=h title="AndreLimZS (Alt + H)">AndreLimZS</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://andrelimzs.github.io/posts/machine-learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://andrelimzs.github.io/posts/cnc-mill/ title="CNC Mill"><span>CNC Mill</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://andrelimzs.github.io>Home</a>&nbsp;»&nbsp;<a href=https://andrelimzs.github.io/posts/>Posts</a></div><h1 class=post-title>Reinforcement Learning</h1><div class=post-meta><span title='2022-04-11 23:59:00 +0800 +0800'>April 11, 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Andre Lim</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#reinforcement-learning aria-label="Reinforcement Learning">Reinforcement Learning</a></li><li><a href=#markov-decision-processes-mdp aria-label="Markov Decision Processes (MDP)">Markov Decision Processes (MDP)</a><ul><li><a href=#terminology aria-label=Terminology>Terminology</a></li><li><a href=#dynamics-of-mdps aria-label="Dynamics of MDPs">Dynamics of MDPs</a></li><li><a href=#policy aria-label=Policy>Policy</a></li><li><a href=#value-function aria-label="Value Function">Value Function</a></li><li><a href=#bellman-equation aria-label="Bellman Equation">Bellman Equation</a></li><li><a href=#optimal-value-function aria-label="Optimal Value Function">Optimal Value Function</a></li><li><a href=#optimal-policy aria-label="Optimal Policy">Optimal Policy</a></li></ul></li><li><a href=#value-iteration-and-policy-iteration aria-label="Value Iteration and Policy Iteration">Value Iteration and Policy Iteration</a><ul><li><a href=#value-iteration aria-label="Value Iteration">Value Iteration</a></li><li><a href=#policy-iteration aria-label="Policy Iteration">Policy Iteration</a></li><li><a href=#value-vs-policy-iteration aria-label="Value vs Policy Iteration">Value vs Policy Iteration</a></li></ul></li><li><a href=#learning-a-model-for-an-mdp aria-label="Learning a Model for an MDP">Learning a Model for an MDP</a><ul><li><a href=#efficient-update aria-label="Efficient Update">Efficient Update</a></li><li><a href=#solving-learned-mdp aria-label="Solving Learned MDP">Solving Learned MDP</a></li></ul></li><li><a href=#continuous-state-mdps aria-label="Continuous State MDPs">Continuous State MDPs</a><ul><li><a href=#discretization aria-label=Discretization>Discretization</a></li><li><a href=#value-function-approximation aria-label="Value Function Approximation">Value Function Approximation</a></li><li><a href=#using-modelsimulator aria-label="Using Model/Simulator">Using Model/Simulator</a></li><li><a href=#getting-the-model aria-label="Getting the Model">Getting the Model</a><ul><li><a href=#using-physics-simulation aria-label="Using physics simulation">Using physics simulation</a></li><li><a href=#data-collected-in-the-mdp aria-label="Data collected in the MDP">Data collected in the MDP</a></li></ul></li><li><a href=#fitted-value-iteration aria-label="Fitted Value Iteration">Fitted Value Iteration</a><ul><li><a href=#convergence aria-label=Convergence>Convergence</a></li><li><a href=#deterministic-simplification aria-label="Deterministic Simplification">Deterministic Simplification</a></li><li><a href=#policy-1 aria-label=Policy>Policy</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h2 id=reinforcement-learning>Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#reinforcement-learning>#</a></h2><p>Many sequential decision making and control problems are hard to provide explicit supervision for.</p><p>Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.</p><p>Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc</p><h2 id=markov-decision-processes-mdp>Markov Decision Processes (MDP)<a hidden class=anchor aria-hidden=true href=#markov-decision-processes-mdp>#</a></h2><p>Provide formalism for many RL problems</p><h3 id=terminology>Terminology<a hidden class=anchor aria-hidden=true href=#terminology>#</a></h3><ul><li><strong>States</strong> : $s$</li><li><strong>Actions</strong> : $a$</li><li><strong>State Transition Probabilities</strong> : $P_{sa}$</li><li><strong>Discount Factor</strong> : $\gamma \in [0,1)$</li><li><strong>Reward Function</strong> : $R : S \times A \mapsto \mathbb{R}$</li></ul><h3 id=dynamics-of-mdps>Dynamics of MDPs<a hidden class=anchor aria-hidden=true href=#dynamics-of-mdps>#</a></h3><ol><li>Start in some state $s_0$</li><li>Choose some action $a_0 \in A$</li><li>MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$</li><li>Pick another action</li><li>Repeat</li></ol><p>Represent as
$$
s_0
\overset{a_0}{\rightarrow} s_1
\overset{a_1}{\rightarrow} s_2
\overset{a_2}{\rightarrow} s_3
\overset{a_3}{\rightarrow} \dots
$$
<strong>Total Payoff</strong>
$$
R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots
$$
<strong>Goal</strong> : Maximise expected value of total payoff
$$
E \left[
R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots
\right]
$$</p><h3 id=policy>Policy<a hidden class=anchor aria-hidden=true href=#policy>#</a></h3><p>Function $\pi : S \mapsto A$ maps states to actions</p><p><strong>Execute</strong> a policy by following the prescribed action</p><h3 id=value-function>Value Function<a hidden class=anchor aria-hidden=true href=#value-function>#</a></h3><p>The expected sum of discounted rewards upon starting in $s$ and taking actions according to $\pi$
$$
V^\pi(s) = \mathbb{E}
\left[
R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) +
\dots | s_0 = s,\pi
\right]
$$</p><h3 id=bellman-equation>Bellman Equation<a hidden class=anchor aria-hidden=true href=#bellman-equation>#</a></h3><p>Given a fixed policy $\pi$, it&rsquo;s value function $V^\pi$ satisfies the Bellman equations
$$
V^\pi(s) = R(s) + \gamma \sum_{s&rsquo; \in S} P_{s\pi(s)}(s&rsquo;) V^\pi(s&rsquo;)
$$
Which consists of two terms:</p><ul><li>Immediate reward $R(s)$
Get for starting in $s$</li><li>Expected sum of future discounted rewards
Can we rewritten as $E_{s&rsquo;\sim P_{s\pi(s)}}[V^\pi(s&rsquo;)]$
The expected sum of starting in $s&rsquo;$ where $s&rsquo;$ is distributed according to to $P_{s\pi(s)}$</li></ul><p>Can be used to efficiently solve for the value function $V^\pi$</p><ul><li>Write down one equation for every state</li><li>Gives a set of $|S|$ linear equations in $|S|$ variables</li></ul><h3 id=optimal-value-function>Optimal Value Function<a hidden class=anchor aria-hidden=true href=#optimal-value-function>#</a></h3><p>Best possible expected sum of discounted rewards which can be attained using any policy</p><p>$$
V^*(s) = \max_\pi V^\pi (s)
$$</p><p>Which has it&rsquo;s own version of the <strong>Bellman Equation</strong></p><p>$$
V^{\ast}(s) = R(s) + \max_{a\in A} \gamma
\sum_{s&rsquo;\in S}{P_{sa} (s&rsquo;) V^{\ast}(s&rsquo;)}
$$</p><p>The second term is a $\max$ over all possible actions because that is the optimal reward.</p><h3 id=optimal-policy>Optimal Policy<a hidden class=anchor aria-hidden=true href=#optimal-policy>#</a></h3><p>$$
\pi^{\ast}(s) = \arg \max_{a\in A} \sum_{s&rsquo;\in S} P_{sa}(s&rsquo;) V^*(s&rsquo;)
$$</p><p>Which gives the action that attains the $\max$ in the optimal value function</p><p>For every state $s$ and policy $\pi$
$$
V^\ast(s) = V^{\pi*}(s) \geq V^\pi(s)
$$
Which says that</p><blockquote><p>The value function for the optimal policy is equal to the optimal value function for every state $s$</p></blockquote><blockquote><p>The value function for the optimal policy is greater than or equal to every other policy</p></blockquote><ul><li>$\pi^*$ is the optimal policy for <strong>all</strong> states</li></ul><h2 id=value-iteration-and-policy-iteration>Value Iteration and Policy Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration-and-policy-iteration>#</a></h2><p>Two efficient algorithms for solving finite-state MDPs with known transition probabilities</p><h3 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h3><blockquote><p>For each state s, initialize $V(s) := 0$
for until convergence do
For every state, update
$$
V(s) := R(s) + \max_{a\in A} \gamma
\sum_{s&rsquo;} P_{sa}(s&rsquo;) V(s&rsquo;)
$$</p></blockquote><ul><li>Repeatedly update estimated value function using the Bellman equation</li></ul><p>Can be:</p><ul><li>Synchronous<ul><li>Compute new values for <strong>all</strong> states and update at once</li><li>Can be viewed as implementing a <em>Bellman Backup Operator</em>, which maps the current estimate to a new estimate</li></ul></li><li>Asynchronous<ul><li>Loop over states and update one at a time</li></ul></li></ul><p>Both algorithms will cause $V$ to converge to $V^*$</p><h3 id=policy-iteration>Policy Iteration<a hidden class=anchor aria-hidden=true href=#policy-iteration>#</a></h3><blockquote><p>Initialise $\pi$ randomly</p><p>for until convergence do</p><p>​ Let $V := V^\pi$</p><p>​ For each state $s$
$$
\pi(s) := \arg \max_{a \in A} \sum_{s&rsquo;} P_{sa}(s&rsquo;) V(s&rsquo;)
$$</p></blockquote><ul><li>Repeatedly computes value function for the current policy</li><li>Update policy using current value function<ul><li>This is the policy that is <strong>greedy w.r.t. $\bold V$</strong></li></ul></li><li>The value function can be updated using Bellman equations</li><li>After a finite number of iterations, $V$ will converge to $V^\ast$ and $\pi$ to $\pi^\ast$</li></ul><h3 id=value-vs-policy-iteration>Value vs Policy Iteration<a hidden class=anchor aria-hidden=true href=#value-vs-policy-iteration>#</a></h3><ul><li>No universal agreement which is better</li><li>Small MDPs : Policy iteration tends to be fast</li><li>Larger MDPs : Value iteration might be faster</li><li>Policy iteration can reach the exact $V^*$</li><li>Value iteration will have small non-zero error</li></ul><h2 id=learning-a-model-for-an-mdp>Learning a Model for an MDP<a hidden class=anchor aria-hidden=true href=#learning-a-model-for-an-mdp>#</a></h2><ul><li><p>In many realistic problems, we do not know the state transition probabilities and rewards</p></li><li><p>Must be estimated from data</p></li></ul><p>Given enough trials, can derive <strong>maximum likelihood estimates</strong> for the state transition probabilities
$$
P_{sa}(s&rsquo;) =
\frac{num. \text{ times } a \text{ led to } s&rsquo; }
{num. \text{ times we took } a}
$$
In the case of $\frac{0}{0}$, can set $P$ to $\frac{1}{|S|}$ (estimate as uniform)</p><h3 id=efficient-update>Efficient Update<a hidden class=anchor aria-hidden=true href=#efficient-update>#</a></h3><p>Keep count of numerator & denominator</p><h3 id=solving-learned-mdp>Solving Learned MDP<a hidden class=anchor aria-hidden=true href=#solving-learned-mdp>#</a></h3><ul><li>Can use <strong>policy</strong>/<strong>value</strong> iteration</li></ul><h2 id=continuous-state-mdps>Continuous State MDPs<a hidden class=anchor aria-hidden=true href=#continuous-state-mdps>#</a></h2><ul><li>Infinite number of states</li><li>Consider settings where state space is $S = \mathbb{R}^d$</li></ul><h3 id=discretization>Discretization<a hidden class=anchor aria-hidden=true href=#discretization>#</a></h3><ul><li>Simplest way</li><li>Discretize state space</li><li>Use value/policy iteration</li></ul><p><strong>Downsides</strong></p><ul><li>Fairly naive representation for $V^*$</li><li>Assumes the value function is constant over each discretization interval</li><li>Piecewise constant representation isn&rsquo;t good for many smooth functions<ul><li>Little smoothing over inputs</li><li>No generalization over different grid cells</li></ul></li><li>Suffers from <strong>curse of dimensionality</strong><ul><li>For $S = \mathbb{R}^d$, we will have $k^d$ states</li></ul></li></ul><p><strong>Rule of Thumb</strong></p><ul><li>Works well for 1D/2D cases</li><li>Can work for 3D/4D cases</li><li>Rarely works above 6D</li></ul><h3 id=value-function-approximation>Value Function Approximation<a hidden class=anchor aria-hidden=true href=#value-function-approximation>#</a></h3><ul><li>Approximate $V^*$ directly</li></ul><h3 id=using-modelsimulator>Using Model/Simulator<a hidden class=anchor aria-hidden=true href=#using-modelsimulator>#</a></h3><ul><li>Assume we have a model/simulator for the MDP</li><li>Any black-box with<ul><li>Input : State $s_t$, Action $a_t$</li><li>Output : Next-state $s_{t+1}$ sampled</li></ul></li></ul><h3 id=getting-the-model>Getting the Model<a hidden class=anchor aria-hidden=true href=#getting-the-model>#</a></h3><h4 id=using-physics-simulation>Using physics simulation<a hidden class=anchor aria-hidden=true href=#using-physics-simulation>#</a></h4><ul><li>Can use off-the-shelf physics simulation</li></ul><h4 id=data-collected-in-the-mdp>Data collected in the MDP<a hidden class=anchor aria-hidden=true href=#data-collected-in-the-mdp>#</a></h4><ul><li>Can be done with:<ul><li>Random actions</li><li>Executing some policy</li><li>Other way of choosing actions<br>[?] Control law?</li></ul></li><li>Apply learning algorithm to predict $s_{t+1}$ as function of $s_t$</li></ul><p><strong>Example</strong> : Learn linear model</p><p>using linear regression
$$
\arg \min_{A,B} \sum_{i=1}^{n} \sum_{t=0}^{T-1}
\left\Vert
s_{t+1}^{(i)} - (A s_t^{(i)} + Ba_t^{(i)})
\right\Vert^2_2
$$
Can build either a <em>deterministic</em>
$$
s_{t+1} = As_t + Ba_t
$$
or <em>stochastic</em> model
$$
s_{t+1} = As_t + Ba_t + \epsilon_t
$$
with $\epsilon_t$ being a noise term usually modelled as $\epsilon_t \sim \mathcal{N}(0,\Sigma)$</p><p>or non-linear function
$$
s_{t+1} = A\phi_s(s_t) + B\phi_a(a_t)
$$
Eg: Locally weighted linear regression</p><h3 id=fitted-value-iteration>Fitted Value Iteration<a hidden class=anchor aria-hidden=true href=#fitted-value-iteration>#</a></h3><ul><li>Algorithm for approximating the value function of a continuous state MDP</li><li>Assume state space $S = \mathcal{R}^d$</li><li>But action space $A$ is <strong>small</strong> and <strong>discrete</strong></li></ul><p>Perform value iteration update
$$
\begin{aligned}
V(s) :&= R(s) + \gamma \max_a
\int_{s&rsquo;} P_{sa}(s&rsquo;) V(s&rsquo;) ds&rsquo; \cr
&= R(s) + \gamma \max_a E_{s&rsquo; \sim P_{sa}} [V(s&rsquo;)]
\end{aligned}
$$</p><ul><li>Main difference is the $\int$ instead of $\sum$</li><li>Approximately update over a finite sample of states $s^{(1)}, \dots, s^{(n)}$</li><li>Use supervised learning algorithm, <strong>linear regression</strong> to approximate value function as linear/nonlinear function of the states</li></ul><p>$$
V(s) = \theta^T \phi(s)
$$</p><p><strong>Algorithm</strong></p><blockquote><p>Compute $y^{(i)}$, the approximation to $R(s) + \gamma \max_a E_{s&rsquo; \sim P_{sa}} [V(s&rsquo;)]$ (RHS)</p><p>Use supervised learning to get $V(s) = y^{(i)}$</p><p>Repeat</p></blockquote><h4 id=convergence>Convergence<a hidden class=anchor aria-hidden=true href=#convergence>#</a></h4><ul><li>Cannot be proved to converge</li><li>But in practice it often does, and works well for many problems</li></ul><h4 id=deterministic-simplification>Deterministic Simplification<a hidden class=anchor aria-hidden=true href=#deterministic-simplification>#</a></h4><ul><li>Can set $k=1$</li><li>Because expectation of a deterministic distribution requires 1 sample</li></ul><h4 id=policy-1>Policy<a hidden class=anchor aria-hidden=true href=#policy-1>#</a></h4><ul><li>Outputs $V$, an approximation of $V^*$</li><li>Implicitly defines policy</li></ul><p>When in state $s$, choose
$$
\arg \max_a E_{s&rsquo;\sim P_{sa}} [V(s&rsquo;)]
$$</p><ul><li>Process for computing/approximating this is similar to the fitted value iteration algorithm</li></ul></div><footer class=post-footer><nav class=paginav><a class=prev href=https://andrelimzs.github.io/posts/machine-learning/cs229-generalization/><span class=title>« Prev Page</span><br><span>Generalization</span></a>
<a class=next href=https://andrelimzs.github.io/posts/machine-learning/cs229-deep-learning/><span class=title>Next Page »</span><br><span>Deep Learning</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on twitter" href="https://twitter.com/intent/tweet/?text=Reinforcement%20Learning&url=https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f&title=Reinforcement%20Learning&summary=Reinforcement%20Learning&source=https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f&title=Reinforcement%20Learning"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on whatsapp" href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%20-%20https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on telegram" href="https://telegram.me/share/url?text=Reinforcement%20Learning&url=https%3a%2f%2fandrelimzs.github.io%2fposts%2fmachine-learning%2fcs229-reinforcement-learning%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://andrelimzs.github.io>AndreLimZS</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>