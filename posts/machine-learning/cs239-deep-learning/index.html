<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta name=robots content="index, follow"><title>CS229 Deep Learning | AndreLimZS</title><meta name=keywords content><meta name=description content="Deep Learning Supervised with Nonlinear Models Supervised learning is
Predict $y$ from input $x$ Suppose model/hypothesis is $h_\theta(x)$ Previous methods have considered
Linear regression : $h_\theta(x) = \theta^Tx$ Kernel method : $h_\theta(x) = \theta^T \phi(x)$ Both are linear in $\theta$
Now consider models that are nonlinear in both
Parameters : $\theta$ Inputs : $x$ The most common of which is the neural network
Cost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\theta) = \frac{1}{2} ( h_\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\theta) = \frac{1}{n} \sum_{i=1}^{n} J^{(i)}(\theta) $$"><meta name=author content><link rel=canonical href=https://andrelimzs.github.io/posts/machine-learning/cs239-deep-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://andrelimzs.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://andrelimzs.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://andrelimzs.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://andrelimzs.github.io/apple-touch-icon.png><link rel=mask-icon href=https://andrelimzs.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.100.2"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="CS229 Deep Learning"><meta property="og:description" content="Deep Learning Supervised with Nonlinear Models Supervised learning is
Predict $y$ from input $x$ Suppose model/hypothesis is $h_\theta(x)$ Previous methods have considered
Linear regression : $h_\theta(x) = \theta^Tx$ Kernel method : $h_\theta(x) = \theta^T \phi(x)$ Both are linear in $\theta$
Now consider models that are nonlinear in both
Parameters : $\theta$ Inputs : $x$ The most common of which is the neural network
Cost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\theta) = \frac{1}{2} ( h_\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\theta) = \frac{1}{n} \sum_{i=1}^{n} J^{(i)}(\theta) $$"><meta property="og:type" content="article"><meta property="og:url" content="https://andrelimzs.github.io/posts/machine-learning/cs239-deep-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-08T22:00:00+08:00"><meta property="article:modified_time" content="2022-04-08T22:00:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CS229 Deep Learning"><meta name=twitter:description content="Deep Learning Supervised with Nonlinear Models Supervised learning is
Predict $y$ from input $x$ Suppose model/hypothesis is $h_\theta(x)$ Previous methods have considered
Linear regression : $h_\theta(x) = \theta^Tx$ Kernel method : $h_\theta(x) = \theta^T \phi(x)$ Both are linear in $\theta$
Now consider models that are nonlinear in both
Parameters : $\theta$ Inputs : $x$ The most common of which is the neural network
Cost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\theta) = \frac{1}{2} ( h_\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\theta) = \frac{1}{n} \sum_{i=1}^{n} J^{(i)}(\theta) $$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://andrelimzs.github.io/posts/"},{"@type":"ListItem","position":4,"name":"CS229 Deep Learning","item":"https://andrelimzs.github.io/posts/machine-learning/cs239-deep-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CS229 Deep Learning","name":"CS229 Deep Learning","description":"Deep Learning Supervised with Nonlinear Models Supervised learning is\nPredict $y$ from input $x$ Suppose model/hypothesis is $h_\\theta(x)$ Previous methods have considered\nLinear regression : $h_\\theta(x) = \\theta^Tx$ Kernel method : $h_\\theta(x) = \\theta^T \\phi(x)$ Both are linear in $\\theta$\nNow consider models that are nonlinear in both\nParameters : $\\theta$ Inputs : $x$ The most common of which is the neural network\nCost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\\theta) = \\frac{1}{2} ( h_\\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} J^{(i)}(\\theta) $$","keywords":[],"articleBody":"Deep Learning Supervised with Nonlinear Models Supervised learning is\nPredict $y$ from input $x$ Suppose model/hypothesis is $h_\\theta(x)$ Previous methods have considered\nLinear regression : $h_\\theta(x) = \\theta^Tx$ Kernel method : $h_\\theta(x) = \\theta^T \\phi(x)$ Both are linear in $\\theta$\nNow consider models that are nonlinear in both\nParameters : $\\theta$ Inputs : $x$ The most common of which is the neural network\nCost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\\theta) = \\frac{1}{2} ( h_\\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} J^{(i)}(\\theta) $$\nOptimisers SGD or it’s variants is commonly used $$ \\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta) $$ with $\\alpha \u003e 0$\nHardware parallelisation means that it’s often faster to compute the gradient of $B$ examples simultaneously\n$\\Rightarrow$ Mini-batch SGD is most commonly used\nNeural Networks Refers to any type of nonlinear model/parameterisation $h_\\theta(x)$ that involves matrix multiplications and other entrywise nonlinear operations\nActivation Function A (1D) nonlinear function that maps $\\mathbb{R}$ to $\\mathbb{R}$\nCommon ones include\nReLU : Rectified Linear Unit Single Neuron $$ h_\\theta(x) = ReLU(w^Tx + b) $$\nInput : $x \\in \\mathbb{R}^d$ Weights : $w \\in \\mathbb{R}^d$ Bias : $b \\in \\mathbb{R}$ Stacking Neurons Enabels more complex networks\n[Example] Two-layer Fully-connected Network\nFor a hidden layer with three neurons $$ a_1 = ReLU(w_1^T x + b_1) \\\\ a_2 = ReLU(w_2^T x + b_2) \\\\ a_3 = ReLU(w_3^T x + b_3) $$ Or equivalently $$ \\begin{aligned} \u0026z_i = (w_i^{[1]})^T x + b_i^{[1]} \\quad \\forall i \\in [1,\\dots,m] \\cr \u0026 a_i = ReLU(z_i) \\cr \u0026 a = [ a_1,\\ \\dots,\\ a_m ]^T \\cr \u0026 h_\\theta(x) = (w^{[2]})^T a + b^{[2]} \\end{aligned} $$\nVectorisation Simplify expression using more matrix/vector notation Important in speeding up neural networks Takes advantage of optimised linear algebra packages such as LAPACK/BLAS Leverages GPU parallelism Concatentae weights/inputs/outputs as $$ W^{[i]} = \\begin{bmatrix} -\\ w^{[i]\\top}_1 \\ - \\cr \\vdots \\cr -\\ w^{[i]\\top}_m \\ - \\end{bmatrix} \\in \\mathbb{R}^{m\\times d} $$ To get $$ \\underbrace{ \\begin{bmatrix} z^{[i]}_1 \\cr \\vdots \\cr z^{[i]}_m \\end{bmatrix} }_{z \\in \\mathbb{R}^{m\\times 1}} = \\underbrace{ \\begin{bmatrix} -\\ w^{[i]\\top}_1 \\ - \\cr \\vdots \\cr -\\ w^{[i]\\top}_m \\ - \\end{bmatrix} }_{W^{[1]} \\in \\mathbb{R}^{m\\times d}} \\underbrace{ \\begin{bmatrix} x_1 \\cr \\vdots \\cr x_d \\end{bmatrix} }_{x \\in \\mathbb{R}^{d\\times 1}} + \\underbrace{ \\begin{bmatrix} b^{[1]}_1 \\cr \\vdots \\cr b^{[1]}_m \\end{bmatrix} }_{b^{[1]} \\in \\mathbb{R}^{m\\times 1}} $$ Or equivalently $$ \\begin{aligned} \u0026z = W^{[1]}x + b^{[1]} \\cr \u0026a = ReLU(W^{[1]} + b^{[1]}) \\cr \u0026h_\\theta(x) = W^{[2]} a + b^{[2]} \\end{aligned} $$\n$\\theta$ consists of weights $W^{[i]}$ and biases $b^{[i]}$\nEach layer consists of the pair $(W^{[i]}, b^{[i]})$\nMulti-Layer Network Can stack more layers to get a deeper fully-connected neural network If you let $a^{[0]} = x$ and $a^{[r]} = h_\\theta(x)$ $$ a^{[k]} = \\text{ReLU}\\left( W^{[k]} a^{[k-1]} + b^{[k]} \\right), \\quad \\forall k=1,\\dots,r-1 $$\nConnection to Kernel Method Deep learning is a method to automatically learn the best feature map (or representation) $$ h_\\theta(x) = W^{[r]} \\phi_\\beta(x) + b^{[r]} $$ With $\\beta: a^{[r-1]} = \\phi_\\beta(x)$ as the feature map\nThe neural network is therefore a linear regression over the learnt (nonlinear) feature map $\\phi_\\beta$\nBackpropagation Aka auto-differentiation\nCompute the gradient of the loss $\\nabla J^{(j)} (\\theta)$ efficiently\nTheorem (Informally)\nSuppose a differentiable circuit of size $N$ computes a real-valued function $f:\\mathbb R^l \\rightarrow \\mathbb R$. The gradient $\\nabla f$ can be computed in $O(N)$ time by a circuit of size $O(N)$\nChain Rule For variables $\\theta_1, \\dots, \\theta_p$ and intermediate variables $g_1, \\dots, g_k$ $$ \\begin{aligned} g_j = g_j(\\theta_1, \\dots, \\theta_p) \\cr J = J(g_1, \\dots, g_k) \\end{aligned} $$ The chain rule states $$ \\frac{\\partial J}{\\partial \\theta_i} = \\sum^{k}_{j=1} \\frac{\\partial J}{\\partial g_j} \\frac{\\partial g_j}{\\partial \\theta_i} $$\n","wordCount":"603","inLanguage":"en","datePublished":"2022-04-08T22:00:00+08:00","dateModified":"2022-04-08T22:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://andrelimzs.github.io/posts/machine-learning/cs239-deep-learning/"},"publisher":{"@type":"Organization","name":"AndreLimZS","logo":{"@type":"ImageObject","url":"https://andrelimzs.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><header class=header><nav class=nav><div class=logo><a href=https://andrelimzs.github.io accesskey=h title="AndreLimZS (Alt + H)">AndreLimZS</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>CS229 Deep Learning</h1><div class=post-meta><span title='2022-04-08 22:00:00 +0800 +0800'>April 8, 2022</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#deep-learning aria-label="Deep Learning">Deep Learning</a><ul><li><a href=#supervised-with-nonlinear-models aria-label="Supervised with Nonlinear Models">Supervised with Nonlinear Models</a><ul><li><a href=#costloss-function aria-label="Cost/Loss Function">Cost/Loss Function</a></li><li><a href=#optimisers aria-label=Optimisers>Optimisers</a></li></ul></li><li><a href=#neural-networks aria-label="Neural Networks">Neural Networks</a><ul><li><a href=#activation-function aria-label="Activation Function">Activation Function</a></li><li><a href=#single-neuron aria-label="Single Neuron">Single Neuron</a></li><li><a href=#stacking-neurons aria-label="Stacking Neurons">Stacking Neurons</a></li><li><a href=#vectorisation aria-label=Vectorisation>Vectorisation</a></li><li><a href=#multi-layer-network aria-label="Multi-Layer Network">Multi-Layer Network</a></li><li><a href=#connection-to-kernel-method aria-label="Connection to Kernel Method">Connection to Kernel Method</a></li></ul></li><li><a href=#backpropagation aria-label=Backpropagation>Backpropagation</a><ul><li><a href=#chain-rule aria-label="Chain Rule">Chain Rule</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=deep-learning>Deep Learning<a hidden class=anchor aria-hidden=true href=#deep-learning>#</a></h1><h2 id=supervised-with-nonlinear-models>Supervised with Nonlinear Models<a hidden class=anchor aria-hidden=true href=#supervised-with-nonlinear-models>#</a></h2><p>Supervised learning is</p><ul><li>Predict $y$ from input $x$</li><li>Suppose model/hypothesis is $h_\theta(x)$</li></ul><p>Previous methods have considered</p><ul><li>Linear regression : $h_\theta(x) = \theta^Tx$</li><li>Kernel method : $h_\theta(x) = \theta^T \phi(x)$</li></ul><p>Both are linear in $\theta$</p><p>Now consider models that are nonlinear in both</p><ul><li>Parameters : $\theta$</li><li>Inputs : $x$</li></ul><p>The most common of which is the <strong>neural network</strong></p><h3 id=costloss-function>Cost/Loss Function<a hidden class=anchor aria-hidden=true href=#costloss-function>#</a></h3><p>Define least-squares cost for one sample
$$
J^{(i)}(\theta) = \frac{1}{2} ( h_\theta(x^{(i)}) - y^{(i)})^2
$$
and mean-square cost for the dataset
$$
J(\theta) = \frac{1}{n} \sum_{i=1}^{n} J^{(i)}(\theta)
$$</p><h3 id=optimisers>Optimisers<a hidden class=anchor aria-hidden=true href=#optimisers>#</a></h3><p>SGD or it&rsquo;s variants is commonly used
$$
\theta := \theta - \alpha \nabla_\theta J(\theta)
$$
with $\alpha > 0$</p><p><strong>Hardware parallelisation</strong> means that it&rsquo;s often faster to compute the gradient of $B$ examples simultaneously</p><p>$\Rightarrow$ Mini-batch SGD is most commonly used</p><h2 id=neural-networks>Neural Networks<a hidden class=anchor aria-hidden=true href=#neural-networks>#</a></h2><blockquote><p>Refers to any type of nonlinear model/parameterisation $h_\theta(x)$ that involves matrix multiplications and other entrywise nonlinear operations</p></blockquote><h3 id=activation-function>Activation Function<a hidden class=anchor aria-hidden=true href=#activation-function>#</a></h3><p>A (1D) nonlinear function that maps $\mathbb{R}$ to $\mathbb{R}$</p><p>Common ones include</p><ul><li>ReLU : Rectified Linear Unit</li></ul><h3 id=single-neuron>Single Neuron<a hidden class=anchor aria-hidden=true href=#single-neuron>#</a></h3><p>$$
h_\theta(x) = ReLU(w^Tx + b)
$$</p><ul><li><strong>Input</strong> : $x \in \mathbb{R}^d$</li><li><strong>Weights</strong> : $w \in \mathbb{R}^d$</li><li><strong>Bias</strong> : $b \in \mathbb{R}$</li></ul><h3 id=stacking-neurons>Stacking Neurons<a hidden class=anchor aria-hidden=true href=#stacking-neurons>#</a></h3><p>Enabels more complex networks</p><p><strong>[Example] Two-layer Fully-connected Network</strong></p><figure class=align-center><img loading=lazy src=fcn_two_layer.jpg#center width=360></figure><p>For a hidden layer with three neurons
$$
a_1 = ReLU(w_1^T x + b_1) \\
a_2 = ReLU(w_2^T x + b_2) \\
a_3 = ReLU(w_3^T x + b_3)
$$
Or equivalently
$$
\begin{aligned}
&z_i = (w_i^{[1]})^T x + b_i^{[1]}
\quad \forall i \in [1,\dots,m] \cr
& a_i = ReLU(z_i) \cr
& a = [ a_1,\ \dots,\ a_m ]^T \cr
& h_\theta(x) = (w^{[2]})^T a + b^{[2]}
\end{aligned}
$$</p><h3 id=vectorisation>Vectorisation<a hidden class=anchor aria-hidden=true href=#vectorisation>#</a></h3><ul><li>Simplify expression using more matrix/vector notation</li><li>Important in speeding up neural networks<ul><li>Takes advantage of optimised linear algebra packages such as LAPACK/BLAS</li><li>Leverages GPU parallelism</li></ul></li></ul><p>Concatentae weights/inputs/outputs as
$$
W^{[i]} = \begin{bmatrix}
-\ w^{[i]\top}_1 \ - \cr
\vdots \cr
-\ w^{[i]\top}_m \ -
\end{bmatrix} \in \mathbb{R}^{m\times d}
$$
To get
$$
\underbrace{
\begin{bmatrix}
z^{[i]}_1 \cr
\vdots \cr
z^{[i]}_m
\end{bmatrix}
}_{z \in \mathbb{R}^{m\times 1}} =
\underbrace{
\begin{bmatrix}
-\ w^{[i]\top}_1 \ - \cr
\vdots \cr
-\ w^{[i]\top}_m \ -
\end{bmatrix}
}_{W^{[1]} \in \mathbb{R}^{m\times d}}
\underbrace{
\begin{bmatrix} x_1 \cr \vdots \cr x_d \end{bmatrix}
}_{x \in \mathbb{R}^{d\times 1}} +
\underbrace{
\begin{bmatrix}
b^{[1]}_1 \cr
\vdots \cr
b^{[1]}_m
\end{bmatrix}
}_{b^{[1]} \in \mathbb{R}^{m\times 1}}
$$
Or equivalently
$$
\begin{aligned}
&z = W^{[1]}x + b^{[1]} \cr
&a = ReLU(W^{[1]} + b^{[1]}) \cr
&h_\theta(x) = W^{[2]} a + b^{[2]}
\end{aligned}
$$</p><ul><li><p>$\theta$ consists of weights $W^{[i]}$ and biases $b^{[i]}$</p></li><li><p>Each layer consists of the pair $(W^{[i]}, b^{[i]})$</p></li></ul><h3 id=multi-layer-network>Multi-Layer Network<a hidden class=anchor aria-hidden=true href=#multi-layer-network>#</a></h3><ul><li>Can stack more layers to get a deeper fully-connected neural network</li><li>If you let $a^{[0]} = x$ and $a^{[r]} = h_\theta(x)$</li></ul><p>$$
a^{[k]} = \text{ReLU}\left(
W^{[k]} a^{[k-1]} + b^{[k]}
\right), \quad \forall k=1,\dots,r-1
$$</p><h3 id=connection-to-kernel-method>Connection to Kernel Method<a hidden class=anchor aria-hidden=true href=#connection-to-kernel-method>#</a></h3><p>Deep learning is a method to automatically learn the best feature map (or representation)
$$
h_\theta(x) = W^{[r]} \phi_\beta(x) + b^{[r]}
$$
With $\beta: a^{[r-1]} = \phi_\beta(x)$ as the feature map</p><p>The neural network is therefore a linear regression over the learnt (nonlinear) feature map $\phi_\beta$</p><h2 id=backpropagation>Backpropagation<a hidden class=anchor aria-hidden=true href=#backpropagation>#</a></h2><p>Aka auto-differentiation</p><p>Compute the gradient of the loss $\nabla J^{(j)} (\theta)$ efficiently</p><p><strong>Theorem</strong> (Informally)</p><blockquote><p>Suppose a differentiable circuit of size $N$ computes a real-valued function $f:\mathbb R^l \rightarrow \mathbb R$. The gradient $\nabla f$ can be computed in $O(N)$ time by a circuit of size $O(N)$</p></blockquote><h3 id=chain-rule>Chain Rule<a hidden class=anchor aria-hidden=true href=#chain-rule>#</a></h3><p>For variables $\theta_1, \dots, \theta_p$ and intermediate variables $g_1, \dots, g_k$
$$
\begin{aligned}
g_j = g_j(\theta_1, \dots, \theta_p) \cr
J = J(g_1, \dots, g_k)
\end{aligned}
$$
The chain rule states
$$
\frac{\partial J}{\partial \theta_i} = \sum^{k}_{j=1}
\frac{\partial J}{\partial g_j}
\frac{\partial g_j}{\partial \theta_i}
$$</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://andrelimzs.github.io>AndreLimZS</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>