<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AndreLimZS</title><link>https://andrelimzs.github.io/</link><description>Recent content on AndreLimZS</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 17 Jan 2022 23:05:22 +0800</lastBuildDate><atom:link href="https://andrelimzs.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Growth of Functions</title><link>https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/</link><pubDate>Mon, 17 Jan 2022 23:05:22 +0800</pubDate><guid>https://andrelimzs.github.io/posts/algorithm-design/growth-of-functions/</guid><description>Objective Order of growth is a simple characterization of an algorithm&amp;rsquo;s efficiency and allows for comparison between algorithms.
Asymptotic Efficiency When we look at input size $n$ large enough that only the order of growth matters. We are concerned with the running time in the limit as input increases without bound.
Asymptotic Notation Notation is defined as a function on domain $\mathbb{N} = { 0,1,2,\dots }$, the set of natural numbers.</description></item><item><title>CLRS - Getting Started</title><link>https://andrelimzs.github.io/posts/algorithm-design/getting-started/</link><pubDate>Sun, 16 Jan 2022 17:43:50 +0800</pubDate><guid>https://andrelimzs.github.io/posts/algorithm-design/getting-started/</guid><description>Objective The main objective of CLRS is to introduce frameworks to design and analyze algorithms.
Terminology Loop Invariant Something used to prove the correctness of an algorithm, with three properties:
Initialization It is true prior to the first iteration of the loop Maintenance If it is true before an iteration, it remains true after that iteration Termination It helps show that the algorithm is correct after the loop terminates Commonly used in combination with the termination condition Input Size, $n$ Depends on the problem, but commonly:</description></item><item><title/><link>https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/</guid><description>title: &amp;#34;Reinforcement Learning&amp;#34; date: 2022-04-08T23:59:00+08:00 draft: false math: true Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$</description></item></channel></rss>