<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AndreLimZS</title>
    <link>https://andrelimzs.github.io/</link>
    <description>Recent content on AndreLimZS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 Jun 2022 22:00:00 +0800</lastBuildDate><atom:link href="https://andrelimzs.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Unsupervised Learning</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/unsupervised-learning/</link>
      <pubDate>Sat, 25 Jun 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/unsupervised-learning/</guid>
      <description>Clustering &amp;amp; K-Means Expectation Maximization (EM) Algorithms Principal Component Analysis (PCA) Independent Component Analysis (ICA) Self-Supervised Learning </description>
    </item>
    
    <item>
      <title>Regularization and Model Selection</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/regularization/</link>
      <pubDate>Fri, 24 Jun 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/regularization/</guid>
      <description>Regularization There is a tradeoff between bias (underfitting) and variance (overfitting). The optimal tradeoff requires computing the correct model complexity.
Model Complexity : Can be a function of the parameters ($l_2$ norm) and not just the number of parameters
Regularization : Allows us to:
Control model complexity Prevent overfitting Regularizer Function A regularizer $R(\theta)$, is a function which measures model complexity. It is usually nonnegative.
In classical methods : $R(\theta)$ depends only on parameters $\theta$</description>
    </item>
    
    <item>
      <title>Generalization</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/generalisation/</link>
      <pubDate>Thu, 23 Jun 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/generalisation/</guid>
      <description>Generalisation The ultimate goal of machine learning is to create a predictive model which performs well on unseen examples (it generalises well).
Generalization is a model&amp;rsquo;s performance on unseen test data, measured by test error (loss on unseen test data).
Test Error Loss/error on test examples $(x,y)$ sampled from a test distribution $\mathcal{D}$ $$ L(\theta) = \mathbb{E}_{(x,y)\sim\mathcal{D}} [ (y-h_\theta(x))^2] $$ The expectation $\mathbb E$ can be approximated by averaging many samples</description>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/</link>
      <pubDate>Mon, 11 Apr 2022 23:59:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/reinforcement-learning/</guid>
      <description>Reinforcement Learning Many sequential decision making and control problems are hard to provide explicit supervision for.
Instead provide a reward function and let the learning algorithm figure out how to choose actions over time.
Successful in applications such as helicopter flight, legged locomotion, network routing, marketing strategy selection, etc
Markov Decision Processes (MDP) Provide formalism for many RL problems
Terminology States : $s$ Actions : $a$ State Transition Probabilities : $P_{sa}$ Discount Factor : $\gamma \in [0,1)$ Reward Function : $R : S \times A \mapsto \mathbb{R}$ Dynamics of MDPs Start in some state $s_0$ Choose some action $a_0 \in A$ MDP randomly transitions to successor state $s_1 \sim P_{s_0a_0}$ Pick another action Repeat Represent as $$ s_0 \overset{a_0}{\rightarrow} s_1 \overset{a_1}{\rightarrow} s_2 \overset{a_2}{\rightarrow} s_3 \overset{a_3}{\rightarrow} \dots $$ Total Payoff $$ R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots $$ Goal : Maximise expected value of total payoff $$ E \left[ R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \right] $$</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/deep-learning/</link>
      <pubDate>Sun, 10 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/deep-learning/</guid>
      <description>Deep Learning Supervised with Nonlinear Models Supervised learning is
Predict $y$ from input $x$ Suppose model/hypothesis is $h_\theta(x)$ Previous methods have considered
Linear regression : $h_\theta(x) = \theta^Tx$ Kernel method : $h_\theta(x) = \theta^T \phi(x)$ Both are linear in $\theta$
Now consider models that are nonlinear in both
Parameters : $\theta$ Inputs : $x$ The most common of which is the neural network
Cost/Loss Function Define least-squares cost for one sample $$ J^{(i)}(\theta) = \frac{1}{2} ( h_\theta(x^{(i)}) - y^{(i)})^2 $$ and mean-square cost for the dataset $$ J(\theta) = \frac{1}{n} \sum_{i=1}^{n} J^{(i)}(\theta) $$</description>
    </item>
    
    <item>
      <title>Support Vector Machine (SVM)</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/support-vector-machines/</link>
      <pubDate>Sat, 09 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/support-vector-machines/</guid>
      <description>Support Vector Machines For the linear classifier $$ h_{w,b}(x) = g(w^T x + b) $$ with $y \in {-1, 1}$
Margins Margins represent the idea of how confident and correct a prediction is.
Functional Margin, $\hat \gamma$ The functional margin is defined as $$ \hat{\gamma}^{(i)} = y^{(i)} (w^T x^{(i)} + b) $$
And it requires some sort of normalization condition.
Otherwise the functional margin can be made arbitrarily large by scaling $w$ and $b$ without changing the decision boundary.</description>
    </item>
    
    <item>
      <title>Kernel Methods</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/kernel-methods/</link>
      <pubDate>Fri, 08 Apr 2022 22:30:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/kernel-methods/</guid>
      <description>Kernel Methods Kernel methods are an efficient way to perform nonlinear regression or classification, by calculating the dot product instead of the entire feature map.
Feature Maps A feature map $\phi$ is a function that maps input attributes to some new (nonlinear) feature variables.
LMS with Features First lets define:
Input : $x \in \mathbb R ^ d$ Features : $\phi(x) \in \mathbb R^{p}$ Weights : $\theta \in \mathbb R^d$ Modify gradient descent for ordinary le2ast squares problem $$ \theta := \theta + \alpha \sum_{i=1}^{n}{(y^{(i)} - \theta^T x^{(i)})\ x^{(i)}} $$ With a feature map $\phi : \mathbb R^d \rightarrow \mathbb R^p$ that maps $x$ to $\phi(x)$ $$ \theta := \theta + \alpha \sum_{i=1}^{n}{(y^{(i)} - \theta^T \underbrace{\phi(x^{(i)})} )\ \underbrace{\phi(x^{(i)}})} $$ Which will have SGD update rule $$ \theta := \theta + \alpha (y^{(i)} - \theta^T \phi(x^{(i)}))\ \phi(x^{(i)}) $$</description>
    </item>
    
    <item>
      <title>Generative Learning</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/generative-learning/</link>
      <pubDate>Thu, 07 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/generative-learning/</guid>
      <description>Generative Learning Generative learning is a different approach to learning as opposed to discriminative learning. It tries to model $p(x|y)$ and $p(y)$ instead of learning $p(y|x)$ directly.
$p(x|y)$ : Distribution of the target&amp;rsquo;s features $p(y)$ : Class priors It uses Bayes rule to calculate posterior distribution $p(y|x)$ $$ p(y|x) = \frac{p(x|y) p(y)}{p(x)} $$ And can be simplified when using for prediction because the denominator is constant and doesn&amp;rsquo;t matter $$ \arg \max_y P(y|x) = \arg \max_y p(x|y) p(y) $$</description>
    </item>
    
    <item>
      <title>Generalised Linear Models (GLM)</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/generalised-linear-models/</link>
      <pubDate>Wed, 06 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/generalised-linear-models/</guid>
      <description>Generalised Linear Models (GLM) Generalised Linear Models (GLMs) are a family of models which include many common distributions such as Gaussian, Bernoulli and Multinomial.
The Exponential Family The exponential family serves as a starting point for GLMs.
The exponential family is defined as $$ p(y; \eta) = b(y) \ \exp{\left( \eta^T\ T(y) - a(\eta) \right)} $$
Natural (Canonical) parameter : $\eta$ Sufficient statistic : $T(y)$ Log Partition function : $a(\eta)$ $T(y)$ is often chosen to be $T(y) = y$</description>
    </item>
    
    <item>
      <title>Supervised Learning - Classification</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/supervised-learning-classification/</link>
      <pubDate>Wed, 06 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/supervised-learning-classification/</guid>
      <description>Classification Classification is similar to regression, except output $y$ only takes on a small number of discrete values, or classes.
Logistic Regression Ignoring the fact that $y$ is discrete will often result in very poor performance. $h_\theta(x)$ should also be constrained to $y \in { 0, 1 }$
One approach is to modify the hypothesis function to use the logistic / sigmoid function $$ h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}} $$ where $g(z) = \frac{1}{1 + e^{-z}}$ is the logistic/sigmoid function</description>
    </item>
    
    <item>
      <title>Supervised Learning - Regression</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/supervised-learning-regression/</link>
      <pubDate>Wed, 06 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/supervised-learning-regression/</guid>
      <description>Supervised Learning Supervised learning is the task of learning a function mapping from input to output $$ y = f(x) $$ The relationship can be linear/nonlinear or convex/nonconvex. The approach learns from labeled data.
Terminology Input (Features) : $x^{(i)}$
Output (Target) : $y^{(i)}$
Training example : $(x^{(i)}, y^{(i)})$
Hypothesis : $h(x)$
Parameters / Weights : $\theta$
Types Regression : Continuous values
Classification : Discrete values
Linear Regression Objective
Learn parameters $\theta$ for a given hypthesis function $h$ to best predict output $y$ from input $x$</description>
    </item>
    
    <item>
      <title>Probability Theory</title>
      <link>https://andrelimzs.github.io/posts/machine-learning/cs229-0-probability-theory/</link>
      <pubDate>Tue, 05 Apr 2022 22:00:00 +0800</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/machine-learning/cs229-0-probability-theory/</guid>
      <description>Review of Probability Theory Overview Probability is an important aspect of machine learning, and forms a useful basis for concepts. It is the study of uncertainty
Elements of Probability Defining probability on a set requires the following:
Sample Space : $\Omega$
The set of all possible outcomes of a random experiment Each outcome $\omega \in \Omega$ is the complete description of the state at the end of the experiment Event Space (Set of events) : $\mathcal{F}$</description>
    </item>
    
    <item>
      <title>CNC Machine Conclusion</title>
      <link>https://andrelimzs.github.io/cnc-machine-conclusion/</link>
      <pubDate>Sun, 27 Jun 2021 05:14:37 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/cnc-machine-conclusion/</guid>
      <description>I&amp;rsquo;ve finished up the machine, and sold it. On to other projects.</description>
    </item>
    
    <item>
      <title>First Cuts</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-12-14-first-cuts/</link>
      <pubDate>Mon, 14 Dec 2020 08:46:50 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-12-14-first-cuts/</guid>
      <description>1) 2F, 4mm DOC, 1.6mm WOC, 0.02 FPT, 150 m/min, 90N Too much DOC/WOC and not enough feed-per-tooth? Significant chatter
2) 1F, 2mm DOC, 0.6mm WOC, 0.08 FPT, 150m/min, 45N Dropping DOC &amp;amp; WOC and increasing FPT seems to reduce chatter
3) 1F, 1.2mm DOC, 0.4mm WOC, 0.16 FPT, 75m/min, 54N Even less chatter
4) Non-adaptive climb milling 1F, 0.8mm DOC, 0.4mm WOC, 0.16 FPT, 150m/min, 72N Looks good</description>
    </item>
    
    <item>
      <title>Assembling the machine</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-08-09-assembling-the-machine/</link>
      <pubDate>Sun, 09 Aug 2020 08:06:57 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-08-09-assembling-the-machine/</guid>
      <description>Frame This was the basic frame, made up of only extrusions and joints. I added the linear modules and custom plates, spindle and fixture plate Finally the enclosure, motors and limit switches Electronics I can&amp;rsquo;t be sure of the quality of these power supplies, so I&amp;rsquo;m using one for the steppers and another for the spindle. I hope this prevents noise from the spindle affecting the steppers.</description>
    </item>
    
    <item>
      <title>G2Core Motion Controller</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-06-19-g2core-motion-controller/</link>
      <pubDate>Fri, 19 Jun 2020 16:21:33 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-06-19-g2core-motion-controller/</guid>
      <description>G2Core is a motion control system, similar to grbl, mach3/4, linuxCNC, etc. It runs on a 32-bit ARM Cortex M3 Arduino Due, which means plenty of power for additional features.
Due Pinout
Configuration in G2core Term Meaning machine BOARD + SETTINGS_FILE CONFIG Specifies default BOARD/SETTINGS_FILE for a machine BOARD Specify board type and revision BASE_BOARD Specify underlying hardware platform SETTINGS_FILE Specify default settings Enable Motors and Axes Create a new SETTINGS_FILE, settings_defaultdue.</description>
    </item>
    
    <item>
      <title>Principles of Rapid Machine Design – Bamberg</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-06-18-principles-of-rapid-machine-design-bamberg/</link>
      <pubDate>Thu, 18 Jun 2020 20:16:50 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-06-18-principles-of-rapid-machine-design-bamberg/</guid>
      <description>I stumbled on an excellent resource, Principles of Rapid Machine Design Bamberg, E. (2000). Principles of rapid machine design. It covers design and manufacturing principles, calculations and simulations, and experimental data.
Design Principles Stiffness Budget &amp;ldquo;One of the most important criteria is the effective stiffness of the tool/work piece interface, measured in force per unit deflection $N/\mu m$.&amp;rdquo; Bamberg states that values of 10 to 25 are desirable for machining.</description>
    </item>
    
    <item>
      <title>CNC Mill V3</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-06-15-cnc-mill-v3/</link>
      <pubDate>Mon, 15 Jun 2020 07:05:57 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-06-15-cnc-mill-v3/</guid>
      <description>The parts are going to take a while to arrive, so in the meantime I&amp;rsquo;m refining my design again. I went from a XY-Z configuration to Y-XZ. This frees up space in both the x and z axes and should make it easier to put additional reinforcement as well as a future 4th axis.
The y-axis table also extends out of the frame, which will make it easier to load/unload the machine.</description>
    </item>
    
    <item>
      <title>Improving Rigidity</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-06-10-improving-rigidity/</link>
      <pubDate>Wed, 10 Jun 2020 07:21:37 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-06-10-improving-rigidity/</guid>
      <description>When we last left off we managed to get 20 μm, I&amp;rsquo;m going to try to improve on it.
Spindle Brace Adding
Stainless steel sheet metal (5 mm) brace between the spindle and rear column Aluminum sheet (4 mm) at the bottom reduces deflection (at 110 N) from 20 μm to 16 μm.
Column Brace Adding a 60 x 200 x 5 mm stainless steel plate reduces deflection from 16 μm to 14.</description>
    </item>
    
    <item>
      <title>CNC Mill V2</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-06-09-cnc-mill-v2/</link>
      <pubDate>Tue, 09 Jun 2020 04:25:39 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-06-09-cnc-mill-v2/</guid>
      <description>Design Goal (Quantified) After a few more weeks of research and reading up I have decided to use deflection at the spindle as my primary design goal. I plan to use depth of cut (DOC) + width of cut (WOC) to estimate the acceptable deflection.
Depth of Cut DOC in aluminum ranges from 0.254mm/0.76mm/1.6mm (Shapeoko Wiki) to 2.54mm (Vince Fab) and 6.35mm (Shapeoko Forum).
Recommendations for the Tormach PCNC1100 is also 6.</description>
    </item>
    
    <item>
      <title>Back to Serial</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-06-08-back-to-serial/</link>
      <pubDate>Mon, 08 Jun 2020 17:24:57 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-06-08-back-to-serial/</guid>
      <description>Tripteron The Tripteron is a 3 DoF parallel cartesian robot. It is decoupled, which means each axis can be controlled independently and isotropic, which means it&amp;rsquo;s dexterity is unchanged throughout the workspace.
Practical Limitations But there are some serious obstacles to building a parallel CNC as a first machine. It needs many precise, high strength joints. Precise because any backlash gets magnified by the length of the chain. Strong because the length also results in a large moment.</description>
    </item>
    
    <item>
      <title>Inverse Kinematics, Velocity &amp; Acceleration Analysis</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-05-24-inverse-kinematics-velocity-acceleration-analysis/</link>
      <pubDate>Sun, 24 May 2020 16:27:03 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-05-24-inverse-kinematics-velocity-acceleration-analysis/</guid>
      <description>Overview Notation $$ \begin{aligned} &amp;amp;\Theta &amp;amp;&amp;amp;: \text{Joint velocities} \cr &amp;amp;\Theta_{a/p} &amp;amp;&amp;amp;: \text{Active/Passive} \cr &amp;amp;W &amp;amp;&amp;amp;: \text{Twist} \cr &amp;amp;V &amp;amp;&amp;amp;: \text{Velocity of end-effector} \cr &amp;amp; \Omega &amp;amp;&amp;amp;: \text{Angular Velocity} \cr &amp;amp; X &amp;amp;&amp;amp;: \text{Generalized coordinates} \end{aligned} $$
Outline Inverse Kinematics : Relates position
Inverse Jacobian : Relates velocities (in generalized coordinates)
Inverse Kinematic Jacobian : Relates velocities (in cartesian coordinates)
Inverse Kinematics Inverse kinematics are a way of getting joint coordinates from a desired end-effector pose.</description>
    </item>
    
    <item>
      <title>Parallel Manipulators</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-05-23-parallel-manipulators/</link>
      <pubDate>Sat, 23 May 2020 20:01:30 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-05-23-parallel-manipulators/</guid>
      <description>Background Parallel manipulators are closed-loop kinematic chain mechanisms linked by several independent chains (Merlet, J. P. 2006). They have several advantages over serial manipulators such as accuracy and stiffness.
Because the chains are independent, each link does not have to support all subsequent links and can be lighter and smaller. The lower load and shorter link length reduce the (unmeasured) deformation in the system. The shorter kinematic chains also reduce the effect of deformation, actuator error and measurement error on the final end-effector position.</description>
    </item>
    
    <item>
      <title>Designing a CNC Mill</title>
      <link>https://andrelimzs.github.io/posts/cnc-mill/2020-05-23-designing-a-cnc-mill/</link>
      <pubDate>Sat, 23 May 2020 13:03:54 +0000</pubDate>
      
      <guid>https://andrelimzs.github.io/posts/cnc-mill/2020-05-23-designing-a-cnc-mill/</guid>
      <description>Overview I’ve been looking at getting a CNC mill to machine small, strong, metal parts. Initially I looked at the chinese mills such as the 3018 ($400). But they have a reputation of not lasting very long. Then I moved on to the Shapeoko 3 ($1550) and OpenBuilds C-Beam ($1200), but they’re too big to fit comfortably in an apartment. The Carbide3D Nomad ($2750) would be perfect. It is rigid, compact and easily enclosed.</description>
    </item>
    
    
  </channel>
</rss>
